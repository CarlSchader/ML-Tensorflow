{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "reber_grammar = {\n",
    "  0: ((1, 'B'), ),\n",
    "  1: ((2, 'T'), (5, 'P'), ),\n",
    "  2: ((2, 'S'), (3, 'X'), ),\n",
    "  3: ((4, 'S'), (5, 'X'), ),\n",
    "  4: ((7, 'E'), ),\n",
    "  5: ((5, 'T'), (6, 'V'), ),\n",
    "  6: ((3, 'P'), (4, 'V'), ),\n",
    "  7: (),\n",
    "}\n",
    "\n",
    "nested_grammar = {\n",
    "  0: ((1, 'B'), ),\n",
    "  1: ((2, 'T'), (3, 'P'), ),\n",
    "  2: ((4, 'B'), ),\n",
    "  3: ((5, 'B'), ),\n",
    "  4: ((6, 'T'), (7, 'P')),\n",
    "  5: ((8, 'T'), (9, 'P')),\n",
    "  6: ((6, 'S'), (10, 'X')),\n",
    "  7: ((7, 'T'), (11, 'V')),\n",
    "  8: ((8, 'S'), (12, 'X')),\n",
    "  9: ((9, 'T'), (13, 'V')),\n",
    "  10: ((7, 'X'), (14, 'S')),\n",
    "  11: ((10, 'P'), (14, 'V')),\n",
    "  12: ((9, 'X'), (15, 'S')),\n",
    "  13: ((12, 'P'), (15, 'V')),\n",
    "  14: ((16, 'E'), ),\n",
    "  15: ((17, 'E'), ),\n",
    "  16: ((18, 'T'), ),\n",
    "  17: ((18, 'P'), ),\n",
    "  18: ((19, 'E'), ),\n",
    "  19: (),\n",
    "}\n",
    "\n",
    "incorrect_grammar = {\n",
    "  0: ((1, 'B'), ),\n",
    "  1: ((2, 'T'), (3, 'P'), ),\n",
    "  2: ((4, 'B'), ),\n",
    "  3: ((5, 'B'), ),\n",
    "  4: ((6, 'P'), (7, 'T')),\n",
    "  5: ((8, 'P'), (9, 'T')),\n",
    "  6: ((6, 'S'), (10, 'X')),\n",
    "  7: ((7, 'T'), (11, 'V')),\n",
    "  8: ((8, 'S'), (12, 'X')),\n",
    "  9: ((9, 'T'), (13, 'V')),\n",
    "  10: ((7, 'X'), (14, 'S')),\n",
    "  11: ((10, 'P'), (14, 'V')),\n",
    "  12: ((9, 'X'), (15, 'S')),\n",
    "  13: ((12, 'P'), (15, 'V')),\n",
    "  14: ((16, 'E'), ),\n",
    "  15: ((17, 'E'), ),\n",
    "  16: ((18, 'T'), ),\n",
    "  17: ((18, 'P'), ),\n",
    "  18: ((19, 'E'), ),\n",
    "  19: (),\n",
    "}\n",
    "\n",
    "symbols = ('B', 'T', 'P', 'S', 'X', 'V', 'E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_grammar(grammar):\n",
    "  sentence = ''\n",
    "  i = 0\n",
    "  while len(grammar[i]) > 0:\n",
    "    (i, char) = grammar[i][random.randint(0, len(grammar[i]) - 1)]\n",
    "    sentence += char\n",
    "  return sentence\n",
    "\n",
    "def generate_random_sequence(symbols, length):\n",
    "  sentence = 'B'\n",
    "  for _ in range(length - 2):\n",
    "    sentence += symbols[random.randint(0, len(symbols) - 1)]\n",
    "  return sentence + 'E'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 40 12.0114 11.47587003999999 3.38760535481924\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "\n",
    "used_grammar = nested_grammar\n",
    "\n",
    "sentences = [generate_grammar(used_grammar) for _ in range(n)]\n",
    "\n",
    "lengths = [len(x) for x in sentences]\n",
    "lengths_squared = [x*x for x in lengths]\n",
    "avg = sum(lengths) / n\n",
    "var = (sum(lengths_squared) / n) - (avg*avg)\n",
    "max_length = max(lengths)\n",
    "print(min(lengths), max_length, sum(lengths) / n, var, var**(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 35 11.9944 11.274368639999977 3.357732663569269\n"
     ]
    }
   ],
   "source": [
    "# invalid_sentences = [generate_random_sequence(symbols, len(sentence)) for sentence in sentences]\n",
    "invalid_sentences = [generate_grammar(incorrect_grammar) for _ in range(n)]\n",
    "\n",
    "\n",
    "invalid_lengths = [len(x) for x in invalid_sentences]\n",
    "invalid_lengths_squared = [x*x for x in invalid_lengths]\n",
    "invalid_avg = sum(invalid_lengths) / n\n",
    "invalid_var = (sum(invalid_lengths_squared) / n) - (invalid_avg*invalid_avg)\n",
    "print(min(invalid_lengths), max(invalid_lengths), sum(invalid_lengths) / n, invalid_var, invalid_var**(0.5))\n",
    "\n",
    "max_length = max(max_length, max(invalid_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1], [3], [2], [3], [2], [1], [1], [1], [5], [5], [4], [2], [4], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def dsprint(ds):\n",
    "  for item in ds.take(1):\n",
    "    print(item)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "def preprocess(x):\n",
    "  return [[-1]] + tokenizer.texts_to_sequences(x) + ([[-2]] * (max_length - len(x)))\n",
    "\n",
    "tokenized_sentences = [preprocess(x) for x in sentences]\n",
    "tokenized_invalid_sentences = [preprocess(x) for x in invalid_sentences]\n",
    "\n",
    "print(tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 41, 1), dtype=int32, numpy=\n",
      "array([[[-1],\n",
      "        [ 3],\n",
      "        [ 2],\n",
      "        ...,\n",
      "        [-2],\n",
      "        [-2],\n",
      "        [-2]],\n",
      "\n",
      "       [[-1],\n",
      "        [ 3],\n",
      "        [ 2],\n",
      "        ...,\n",
      "        [-2],\n",
      "        [-2],\n",
      "        [-2]],\n",
      "\n",
      "       [[-1],\n",
      "        [ 3],\n",
      "        [ 1],\n",
      "        ...,\n",
      "        [-2],\n",
      "        [-2],\n",
      "        [-2]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-1],\n",
      "        [ 3],\n",
      "        [ 2],\n",
      "        ...,\n",
      "        [-2],\n",
      "        [-2],\n",
      "        [-2]],\n",
      "\n",
      "       [[-1],\n",
      "        [ 3],\n",
      "        [ 2],\n",
      "        ...,\n",
      "        [-2],\n",
      "        [-2],\n",
      "        [-2]],\n",
      "\n",
      "       [[-1],\n",
      "        [ 3],\n",
      "        [ 2],\n",
      "        ...,\n",
      "        [-2],\n",
      "        [-2],\n",
      "        [-2]]], dtype=int32)>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,\n",
      "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "correct_ds = tf.data.Dataset.from_tensor_slices(tokenized_sentences)\n",
    "correct_ds = correct_ds.map(lambda x: (x, 1))\n",
    "\n",
    "incorrect_ds = tf.data.Dataset.from_tensor_slices(tokenized_invalid_sentences)\n",
    "incorrect_ds = incorrect_ds.map(lambda x: (x, 0))\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices([correct_ds, incorrect_ds])\n",
    "ds = ds.interleave(lambda x: x, 2, 1)\n",
    "ds = ds.shuffle(2*n).batch(32)\n",
    "\n",
    "dsprint(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(n * 0.8)\n",
    "val_size = (n - train_size) // 2\n",
    "\n",
    "train = ds.take(train_size)\n",
    "val = ds.skip(train_size).take(val_size)\n",
    "test = ds.skip(train_size + val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(max_length + 1, 1)),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Conv1D(filters=1 * 10, kernel_size=2, padding='causal', activation='relu', dilation_rate=1),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Conv1D(filters=2 * 10, kernel_size=2, padding='causal', activation='relu', dilation_rate=2),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Conv1D(filters=4 * 10, kernel_size=2, padding='causal', activation='relu', dilation_rate=4),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Conv1D(filters=8 * 10, kernel_size=2, padding='causal', activation='relu', dilation_rate=8),\n",
    "  tf.keras.layers.BatchNormalization(),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
    "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=False)),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss=[tf.keras.losses.binary_crossentropy], metrics=[tf.keras.metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 71s 113ms/step - loss: 0.0831 - binary_accuracy: 0.9615\n",
      "Epoch 2/5\n",
      "128/625 [=====>........................] - ETA: 54s - loss: 1.8862e-04 - binary_accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[206], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(train, validation_data\u001b[39m=\u001b[39;49mval, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train, validation_data=val, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTBTXSETE [[-1], [3], [1], [3], [1], [6], [7], [4], [1], [4], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2]]\n",
      "BPBPXSEPE [[-1], [3], [2], [3], [2], [6], [7], [4], [2], [4], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2], [-2]]\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.998388e-01],\n",
       "       [1.254029e-04]], dtype=float32)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = generate_grammar(nested_grammar)\n",
    "tokens = preprocess(test)\n",
    "random_test = generate_grammar(incorrect_grammar)\n",
    "# random_test = generate_random_sequence(symbols, len(test))\n",
    "# random_test = 'BTSSPXSE'\n",
    "# random_test = 'BTXXVVSE'\n",
    "random_tokens = preprocess(random_test)\n",
    "print(test, tokens)\n",
    "print(random_test, random_tokens)\n",
    "model.predict([tokens, random_tokens])\n",
    "\n",
    "# for item in ds.take(1):\n",
    "#   print(model.predict(item[0]))\n",
    "#   print(item[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9120ca18d9a6352a9f63d878cd22df8740658f703b5d617e89c0216f7e3927ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
