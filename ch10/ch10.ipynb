{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/q7k30xcj51x3tc06z506z3n00000gn/T/ipykernel_38010/1796832042.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = iris.target.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "# X = iris.data[:, (2, 3)]\n",
    "# y = (iris.target == 0).astype(np.int)\n",
    "X = iris.data\n",
    "y = iris.target.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Perceptron()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Perceptron</label><div class=\"sk-toggleable__content\"><pre>Perceptron()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0]\n",
      " [ 1 10  1]\n",
      " [ 0  1 10]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, per_clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(X_train_full, y_train_full), (X_test_full, y_test_full) = fashion_mnist.load_data()\n",
    "print(X_train_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_val, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "class_names = [\n",
    "  'T-shirt/top',\n",
    "  'Trouser',\n",
    "  'Pullover',\n",
    "  'Dress',\n",
    "  'Coat',\n",
    "  'Sandal',\n",
    "  'Shirt',\n",
    "  'Sneaker',\n",
    "  'Bag',\n",
    "  'Ankle, boot',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential()\n",
    "# model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "# model.add(keras.layers.Dense(300, activation='relu'))\n",
    "# model.add(keras.layers.Dense(100, activation='relu'))\n",
    "# model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# or\n",
    "\n",
    "model = keras.Sequential([\n",
    "  keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  keras.layers.Dense(100, activation='relu'),\n",
    "  keras.layers.Dense(50, activation='relu'),\n",
    "  keras.layers.Dense(20, activation='relu'),\n",
    "  keras.layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_5 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 20)                1020      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                210       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84,780\n",
      "Trainable params: 84,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[<keras.layers.reshaping.flatten.Flatten object at 0x304fe6c20>, <keras.layers.core.dense.Dense object at 0x304fe44c0>, <keras.layers.core.dense.Dense object at 0x2d185dcc0>, <keras.layers.core.dense.Dense object at 0x2d185d840>, <keras.layers.core.dense.Dense object at 0x2d185ded0>]\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n",
    "print(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50) (50,)\n"
     ]
    }
   ],
   "source": [
    "weights, biases = model.layers[2].get_weights()\n",
    "print(weights.shape, biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  loss=keras.losses.sparse_categorical_crossentropy,\n",
    "  optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "1719/1719 [==============================] - 1s 734us/step - loss: 0.8355 - accuracy: 0.7116 - val_loss: 0.5354 - val_accuracy: 0.8130\n",
      "Epoch 2/64\n",
      "1719/1719 [==============================] - 1s 655us/step - loss: 0.5073 - accuracy: 0.8222 - val_loss: 0.4972 - val_accuracy: 0.8232\n",
      "Epoch 3/64\n",
      "1719/1719 [==============================] - 1s 753us/step - loss: 0.4550 - accuracy: 0.8401 - val_loss: 0.4745 - val_accuracy: 0.8360\n",
      "Epoch 4/64\n",
      "1719/1719 [==============================] - 1s 683us/step - loss: 0.4256 - accuracy: 0.8499 - val_loss: 0.4131 - val_accuracy: 0.8522\n",
      "Epoch 5/64\n",
      "1719/1719 [==============================] - 1s 704us/step - loss: 0.4037 - accuracy: 0.8563 - val_loss: 0.4098 - val_accuracy: 0.8512\n",
      "Epoch 6/64\n",
      "1719/1719 [==============================] - 1s 684us/step - loss: 0.3856 - accuracy: 0.8629 - val_loss: 0.3720 - val_accuracy: 0.8688\n",
      "Epoch 7/64\n",
      "1719/1719 [==============================] - 1s 692us/step - loss: 0.3709 - accuracy: 0.8676 - val_loss: 0.3649 - val_accuracy: 0.8686\n",
      "Epoch 8/64\n",
      "1719/1719 [==============================] - 1s 649us/step - loss: 0.3580 - accuracy: 0.8717 - val_loss: 0.3872 - val_accuracy: 0.8638\n",
      "Epoch 9/64\n",
      "1719/1719 [==============================] - 1s 662us/step - loss: 0.3457 - accuracy: 0.8763 - val_loss: 0.3597 - val_accuracy: 0.8694\n",
      "Epoch 10/64\n",
      "1719/1719 [==============================] - 1s 656us/step - loss: 0.3373 - accuracy: 0.8789 - val_loss: 0.3654 - val_accuracy: 0.8680\n",
      "Epoch 11/64\n",
      "1719/1719 [==============================] - 1s 757us/step - loss: 0.3277 - accuracy: 0.8813 - val_loss: 0.3609 - val_accuracy: 0.8712\n",
      "Epoch 12/64\n",
      "1719/1719 [==============================] - 1s 677us/step - loss: 0.3198 - accuracy: 0.8844 - val_loss: 0.3403 - val_accuracy: 0.8772\n",
      "Epoch 13/64\n",
      "1719/1719 [==============================] - 1s 739us/step - loss: 0.3118 - accuracy: 0.8869 - val_loss: 0.3529 - val_accuracy: 0.8714\n",
      "Epoch 14/64\n",
      "1719/1719 [==============================] - 1s 692us/step - loss: 0.3066 - accuracy: 0.8889 - val_loss: 0.3239 - val_accuracy: 0.8834\n",
      "Epoch 15/64\n",
      "1719/1719 [==============================] - 1s 666us/step - loss: 0.2989 - accuracy: 0.8913 - val_loss: 0.3235 - val_accuracy: 0.8844\n",
      "Epoch 16/64\n",
      "1719/1719 [==============================] - 1s 714us/step - loss: 0.2938 - accuracy: 0.8922 - val_loss: 0.3543 - val_accuracy: 0.8718\n",
      "Epoch 17/64\n",
      "1719/1719 [==============================] - 1s 680us/step - loss: 0.2894 - accuracy: 0.8940 - val_loss: 0.3457 - val_accuracy: 0.8756\n",
      "Epoch 18/64\n",
      "1719/1719 [==============================] - 1s 700us/step - loss: 0.2830 - accuracy: 0.8965 - val_loss: 0.3200 - val_accuracy: 0.8880\n",
      "Epoch 19/64\n",
      "1719/1719 [==============================] - 1s 650us/step - loss: 0.2790 - accuracy: 0.8975 - val_loss: 0.3371 - val_accuracy: 0.8808\n",
      "Epoch 20/64\n",
      "1719/1719 [==============================] - 1s 652us/step - loss: 0.2739 - accuracy: 0.8995 - val_loss: 0.3223 - val_accuracy: 0.8816\n",
      "Epoch 21/64\n",
      "1719/1719 [==============================] - 1s 664us/step - loss: 0.2694 - accuracy: 0.9024 - val_loss: 0.3171 - val_accuracy: 0.8854\n",
      "Epoch 22/64\n",
      "1719/1719 [==============================] - 1s 672us/step - loss: 0.2652 - accuracy: 0.9028 - val_loss: 0.3193 - val_accuracy: 0.8838\n",
      "Epoch 23/64\n",
      "1719/1719 [==============================] - 1s 646us/step - loss: 0.2614 - accuracy: 0.9047 - val_loss: 0.3353 - val_accuracy: 0.8776\n",
      "Epoch 24/64\n",
      "1719/1719 [==============================] - 1s 661us/step - loss: 0.2574 - accuracy: 0.9056 - val_loss: 0.3383 - val_accuracy: 0.8816\n",
      "Epoch 25/64\n",
      "1719/1719 [==============================] - 1s 656us/step - loss: 0.2532 - accuracy: 0.9067 - val_loss: 0.3257 - val_accuracy: 0.8794\n",
      "Epoch 26/64\n",
      "1719/1719 [==============================] - 1s 641us/step - loss: 0.2489 - accuracy: 0.9087 - val_loss: 0.3240 - val_accuracy: 0.8818\n",
      "Epoch 27/64\n",
      "1719/1719 [==============================] - 1s 665us/step - loss: 0.2456 - accuracy: 0.9101 - val_loss: 0.3188 - val_accuracy: 0.8874\n",
      "Epoch 28/64\n",
      "1719/1719 [==============================] - 1s 665us/step - loss: 0.2421 - accuracy: 0.9109 - val_loss: 0.3233 - val_accuracy: 0.8860\n",
      "Epoch 29/64\n",
      "1719/1719 [==============================] - 1s 663us/step - loss: 0.2390 - accuracy: 0.9127 - val_loss: 0.3276 - val_accuracy: 0.8858\n",
      "Epoch 30/64\n",
      "1719/1719 [==============================] - 1s 645us/step - loss: 0.2354 - accuracy: 0.9136 - val_loss: 0.3275 - val_accuracy: 0.8816\n",
      "Epoch 31/64\n",
      "1719/1719 [==============================] - 1s 647us/step - loss: 0.2318 - accuracy: 0.9147 - val_loss: 0.3134 - val_accuracy: 0.8916\n",
      "Epoch 32/64\n",
      "1719/1719 [==============================] - 1s 650us/step - loss: 0.2290 - accuracy: 0.9158 - val_loss: 0.3090 - val_accuracy: 0.8912\n",
      "Epoch 33/64\n",
      "1719/1719 [==============================] - 1s 672us/step - loss: 0.2262 - accuracy: 0.9173 - val_loss: 0.3386 - val_accuracy: 0.8842\n",
      "Epoch 34/64\n",
      "1719/1719 [==============================] - 1s 660us/step - loss: 0.2223 - accuracy: 0.9184 - val_loss: 0.3261 - val_accuracy: 0.8872\n",
      "Epoch 35/64\n",
      "1719/1719 [==============================] - 1s 642us/step - loss: 0.2193 - accuracy: 0.9205 - val_loss: 0.3096 - val_accuracy: 0.8900\n",
      "Epoch 36/64\n",
      "1719/1719 [==============================] - 1s 665us/step - loss: 0.2163 - accuracy: 0.9207 - val_loss: 0.3425 - val_accuracy: 0.8738\n",
      "Epoch 37/64\n",
      "1719/1719 [==============================] - 1s 666us/step - loss: 0.2137 - accuracy: 0.9221 - val_loss: 0.3159 - val_accuracy: 0.8942\n",
      "Epoch 38/64\n",
      "1719/1719 [==============================] - 1s 646us/step - loss: 0.2115 - accuracy: 0.9227 - val_loss: 0.3285 - val_accuracy: 0.8910\n",
      "Epoch 39/64\n",
      "1719/1719 [==============================] - 1s 669us/step - loss: 0.2082 - accuracy: 0.9233 - val_loss: 0.3370 - val_accuracy: 0.8812\n",
      "Epoch 40/64\n",
      "1719/1719 [==============================] - 1s 665us/step - loss: 0.2052 - accuracy: 0.9248 - val_loss: 0.3154 - val_accuracy: 0.8912\n",
      "Epoch 41/64\n",
      "1719/1719 [==============================] - 1s 661us/step - loss: 0.2039 - accuracy: 0.9245 - val_loss: 0.3150 - val_accuracy: 0.8946\n",
      "Epoch 42/64\n",
      "1719/1719 [==============================] - 1s 665us/step - loss: 0.2014 - accuracy: 0.9256 - val_loss: 0.3249 - val_accuracy: 0.8880\n",
      "Epoch 43/64\n",
      "1719/1719 [==============================] - 1s 647us/step - loss: 0.1982 - accuracy: 0.9275 - val_loss: 0.3167 - val_accuracy: 0.8914\n",
      "Epoch 44/64\n",
      "1719/1719 [==============================] - 1s 663us/step - loss: 0.1966 - accuracy: 0.9282 - val_loss: 0.3111 - val_accuracy: 0.8970\n",
      "Epoch 45/64\n",
      "1719/1719 [==============================] - 1s 651us/step - loss: 0.1927 - accuracy: 0.9284 - val_loss: 0.3207 - val_accuracy: 0.8882\n",
      "Epoch 46/64\n",
      "1719/1719 [==============================] - 1s 663us/step - loss: 0.1897 - accuracy: 0.9308 - val_loss: 0.3106 - val_accuracy: 0.8938\n",
      "Epoch 47/64\n",
      "1719/1719 [==============================] - 1s 642us/step - loss: 0.1889 - accuracy: 0.9311 - val_loss: 0.3444 - val_accuracy: 0.8878\n",
      "Epoch 48/64\n",
      "1719/1719 [==============================] - 1s 641us/step - loss: 0.1858 - accuracy: 0.9329 - val_loss: 0.3164 - val_accuracy: 0.8932\n",
      "Epoch 49/64\n",
      "1719/1719 [==============================] - 1s 645us/step - loss: 0.1824 - accuracy: 0.9326 - val_loss: 0.3268 - val_accuracy: 0.8934\n",
      "Epoch 50/64\n",
      "1719/1719 [==============================] - 1s 646us/step - loss: 0.1805 - accuracy: 0.9330 - val_loss: 0.3226 - val_accuracy: 0.8926\n",
      "Epoch 51/64\n",
      "1719/1719 [==============================] - 1s 650us/step - loss: 0.1788 - accuracy: 0.9339 - val_loss: 0.3242 - val_accuracy: 0.8924\n",
      "Epoch 52/64\n",
      "1719/1719 [==============================] - 1s 664us/step - loss: 0.1757 - accuracy: 0.9358 - val_loss: 0.3233 - val_accuracy: 0.8946\n",
      "Epoch 53/64\n",
      "1719/1719 [==============================] - 1s 665us/step - loss: 0.1738 - accuracy: 0.9354 - val_loss: 0.3618 - val_accuracy: 0.8792\n",
      "Epoch 54/64\n",
      "1719/1719 [==============================] - 1s 674us/step - loss: 0.1729 - accuracy: 0.9361 - val_loss: 0.3381 - val_accuracy: 0.8858\n",
      "Epoch 55/64\n",
      "1719/1719 [==============================] - 1s 666us/step - loss: 0.1698 - accuracy: 0.9371 - val_loss: 0.3356 - val_accuracy: 0.8908\n",
      "Epoch 56/64\n",
      "1719/1719 [==============================] - 1s 648us/step - loss: 0.1671 - accuracy: 0.9381 - val_loss: 0.3675 - val_accuracy: 0.8762\n",
      "Epoch 57/64\n",
      "1719/1719 [==============================] - 1s 648us/step - loss: 0.1647 - accuracy: 0.9392 - val_loss: 0.3654 - val_accuracy: 0.8888\n",
      "Epoch 58/64\n",
      "1719/1719 [==============================] - 1s 640us/step - loss: 0.1639 - accuracy: 0.9398 - val_loss: 0.3655 - val_accuracy: 0.8860\n",
      "Epoch 59/64\n",
      "1719/1719 [==============================] - 1s 645us/step - loss: 0.1615 - accuracy: 0.9407 - val_loss: 0.3451 - val_accuracy: 0.8902\n",
      "Epoch 60/64\n",
      "1719/1719 [==============================] - 1s 647us/step - loss: 0.1597 - accuracy: 0.9406 - val_loss: 0.3569 - val_accuracy: 0.8904\n",
      "Epoch 61/64\n",
      "1719/1719 [==============================] - 1s 664us/step - loss: 0.1577 - accuracy: 0.9413 - val_loss: 0.3449 - val_accuracy: 0.8932\n",
      "Epoch 62/64\n",
      "1719/1719 [==============================] - 1s 663us/step - loss: 0.1559 - accuracy: 0.9419 - val_loss: 0.3305 - val_accuracy: 0.8924\n",
      "Epoch 63/64\n",
      "1719/1719 [==============================] - 1s 657us/step - loss: 0.1553 - accuracy: 0.9422 - val_loss: 0.3327 - val_accuracy: 0.8902\n",
      "Epoch 64/64\n",
      "1719/1719 [==============================] - 1s 645us/step - loss: 0.1513 - accuracy: 0.9445 - val_loss: 0.3591 - val_accuracy: 0.8904\n",
      "{'verbose': 1, 'epochs': 64, 'steps': 1719}\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=64, validation_data=(X_val, y_val))\n",
    "print(history.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 405us/step\n",
      "[[858   6   9  16   4   1  79   0  27   0]\n",
      " [  0 978   1  13   1   0   3   0   4   0]\n",
      " [ 31   8 634   4 241   1  55   0  26   0]\n",
      " [ 31  22  21 804  74   2  28   0  17   1]\n",
      " [  1   1  24  13 924   0  22   0  15   0]\n",
      " [  2   0   0   0   1 938   0  14   6  39]\n",
      " [165   4  54  20 216   0 501   1  39   0]\n",
      " [  1   0   0   0   0  54   0 894   9  42]\n",
      " [  5   0   4   1   0   2   2   2 983   1]\n",
      " [  1   0   0   0   0   6   0  24   0 969]]\n",
      "0.8483\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "pred = np.argmax(model.predict(X_test_full), axis=1)\n",
    "print(confusion_matrix(y_test_full, pred))\n",
    "print(accuracy_score(y_test_full, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 416us/step\n",
      "Sandal\n",
      "Sneaker\n",
      "Pullover\n",
      "Coat\n",
      "Sandal\n",
      "Ankle, boot\n",
      "Shirt\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Bag\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "Trouser\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Sandal\n",
      "Dress\n",
      "Pullover\n",
      "Sandal\n",
      "Sneaker\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Dress\n",
      "Shirt\n",
      "Dress\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Bag\n",
      "Shirt\n",
      "Sandal\n",
      "Pullover\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Shirt\n",
      "Coat\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Bag\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Trouser\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Bag\n",
      "Trouser\n",
      "T-shirt/top\n",
      "Trouser\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Trouser\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Dress\n",
      "Sandal\n",
      "Sneaker\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Bag\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Bag\n",
      "Sandal\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Trouser\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Sneaker\n",
      "Sandal\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Trouser\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Bag\n",
      "Coat\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Bag\n",
      "T-shirt/top\n",
      "Dress\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Sneaker\n",
      "Sandal\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Coat\n",
      "Dress\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Sneaker\n",
      "Sandal\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "Coat\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Bag\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Sandal\n",
      "Bag\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Dress\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Dress\n",
      "T-shirt/top\n",
      "Dress\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "T-shirt/top\n",
      "Bag\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "Coat\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Bag\n",
      "Coat\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Bag\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sandal\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Shirt\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "Coat\n",
      "Bag\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Sandal\n",
      "Sneaker\n",
      "Trouser\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Dress\n",
      "T-shirt/top\n",
      "Bag\n",
      "Bag\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Dress\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Shirt\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Dress\n",
      "Trouser\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Trouser\n",
      "Dress\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Sandal\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Sandal\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Pullover\n",
      "Dress\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "Coat\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Shirt\n",
      "Dress\n",
      "Sandal\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Sandal\n",
      "T-shirt/top\n",
      "Sandal\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Sandal\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Dress\n",
      "T-shirt/top\n",
      "Pullover\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Bag\n",
      "Dress\n",
      "Pullover\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Bag\n",
      "Shirt\n",
      "Bag\n",
      "Sneaker\n",
      "Shirt\n",
      "Pullover\n",
      "Dress\n",
      "Trouser\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Trouser\n",
      "Shirt\n",
      "Coat\n",
      "Trouser\n",
      "Dress\n",
      "Pullover\n",
      "Shirt\n",
      "Dress\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Trouser\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Bag\n",
      "T-shirt/top\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "Pullover\n",
      "Dress\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Trouser\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Pullover\n",
      "Dress\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "Dress\n",
      "Dress\n",
      "Coat\n",
      "Bag\n",
      "Sneaker\n",
      "Pullover\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Dress\n",
      "Shirt\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Dress\n",
      "Shirt\n",
      "Trouser\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "Coat\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Bag\n",
      "Dress\n",
      "Coat\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Trouser\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "T-shirt/top\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Bag\n",
      "Trouser\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Trouser\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "T-shirt/top\n",
      "Dress\n",
      "Bag\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "Pullover\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Bag\n",
      "Coat\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Dress\n",
      "Coat\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "T-shirt/top\n",
      "Sandal\n",
      "Sneaker\n",
      "Bag\n",
      "Dress\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Bag\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "Shirt\n",
      "Coat\n",
      "Bag\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Trouser\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Ankle, boot\n",
      "Bag\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Dress\n",
      "Shirt\n",
      "Pullover\n",
      "Trouser\n",
      "Dress\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "T-shirt/top\n",
      "Bag\n",
      "T-shirt/top\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Sandal\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Trouser\n",
      "Shirt\n",
      "Bag\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "T-shirt/top\n",
      "Trouser\n",
      "Dress\n",
      "Bag\n",
      "T-shirt/top\n",
      "Ankle, boot\n",
      "Sandal\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Dress\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Sandal\n",
      "Sneaker\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "T-shirt/top\n",
      "Dress\n",
      "Bag\n",
      "Shirt\n",
      "Bag\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Sneaker\n",
      "Sandal\n",
      "Bag\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Sandal\n",
      "Sneaker\n",
      "Trouser\n",
      "Dress\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Bag\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Trouser\n",
      "Shirt\n",
      "Dress\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Coat\n",
      "Dress\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Bag\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Trouser\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Sandal\n",
      "Sneaker\n",
      "Bag\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "Trouser\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Bag\n",
      "Sneaker\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Sandal\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Pullover\n",
      "Bag\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Trouser\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Dress\n",
      "Bag\n",
      "Sandal\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Trouser\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Shirt\n",
      "Coat\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "T-shirt/top\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Pullover\n",
      "Dress\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Bag\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Sneaker\n",
      "Sandal\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Dress\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Bag\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Sandal\n",
      "Sneaker\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "T-shirt/top\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "Coat\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sandal\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Dress\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Shirt\n",
      "Dress\n",
      "Trouser\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Bag\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Sandal\n",
      "Ankle, boot\n",
      "T-shirt/top\n",
      "Bag\n",
      "Coat\n",
      "Dress\n",
      "Bag\n",
      "Shirt\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Dress\n",
      "Sandal\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Sandal\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Bag\n",
      "Sandal\n",
      "Sneaker\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Dress\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Dress\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Shirt\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "T-shirt/top\n",
      "Trouser\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Bag\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Coat\n",
      "Dress\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Sandal\n",
      "Ankle, boot\n",
      "Bag\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Dress\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Dress\n",
      "Bag\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Trouser\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "Dress\n",
      "Sandal\n",
      "Sneaker\n",
      "Bag\n",
      "Sandal\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "Shirt\n",
      "Dress\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Sandal\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Dress\n",
      "Dress\n",
      "T-shirt/top\n",
      "Bag\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Trouser\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Pullover\n",
      "Bag\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Dress\n",
      "Pullover\n",
      "Shirt\n",
      "Bag\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "T-shirt/top\n",
      "Ankle, boot\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Dress\n",
      "Trouser\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Pullover\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Dress\n",
      "Pullover\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Dress\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Sneaker\n",
      "Sandal\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Sandal\n",
      "Sneaker\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Dress\n",
      "Trouser\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Trouser\n",
      "Trouser\n",
      "Dress\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Trouser\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Sandal\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Sandal\n",
      "Ankle, boot\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Bag\n",
      "T-shirt/top\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "Shirt\n",
      "Bag\n",
      "Shirt\n",
      "Shirt\n",
      "Coat\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Dress\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Bag\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "T-shirt/top\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Pullover\n",
      "Bag\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Bag\n",
      "Trouser\n",
      "Trouser\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Shirt\n",
      "Coat\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Dress\n",
      "Shirt\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Trouser\n",
      "Dress\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "Pullover\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "Coat\n",
      "Bag\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Dress\n",
      "Sandal\n",
      "Ankle, boot\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Trouser\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Coat\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Bag\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "Dress\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Sneaker\n",
      "Sandal\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Shirt\n",
      "Dress\n",
      "Sneaker\n",
      "Sandal\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Bag\n",
      "Sandal\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Shirt\n",
      "Coat\n",
      "Coat\n",
      "Dress\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Shirt\n",
      "Trouser\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Coat\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Trouser\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Sandal\n",
      "Sneaker\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Coat\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Pullover\n",
      "Shirt\n",
      "Bag\n",
      "Dress\n",
      "Sneaker\n",
      "Sandal\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Dress\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "Shirt\n",
      "Dress\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Bag\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Trouser\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Bag\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Bag\n",
      "Coat\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Dress\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Bag\n",
      "Shirt\n",
      "Bag\n",
      "Coat\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "T-shirt/top\n",
      "Sneaker\n",
      "Pullover\n",
      "Dress\n",
      "Bag\n",
      "Dress\n",
      "Dress\n",
      "Trouser\n",
      "Coat\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Dress\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Trouser\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Dress\n",
      "Shirt\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Trouser\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Dress\n",
      "Bag\n",
      "Sneaker\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "Sneaker\n",
      "Coat\n",
      "Trouser\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Dress\n",
      "Bag\n",
      "Coat\n",
      "Coat\n",
      "Dress\n",
      "Bag\n",
      "Coat\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Sandal\n",
      "Pullover\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Dress\n",
      "Trouser\n",
      "Coat\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Shirt\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Dress\n",
      "Bag\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Bag\n",
      "Trouser\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "Dress\n",
      "Bag\n",
      "Shirt\n",
      "Bag\n",
      "Pullover\n",
      "Shirt\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Bag\n",
      "Sandal\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Coat\n",
      "Dress\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Dress\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Dress\n",
      "Coat\n",
      "Bag\n",
      "Sneaker\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "T-shirt/top\n",
      "Bag\n",
      "Coat\n",
      "Sneaker\n",
      "Sandal\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Sandal\n",
      "Ankle, boot\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Shirt\n",
      "Coat\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Trouser\n",
      "Coat\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Trouser\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Bag\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Trouser\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Bag\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Dress\n",
      "Shirt\n",
      "Trouser\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Bag\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Coat\n",
      "Shirt\n",
      "Sandal\n",
      "Sneaker\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "Coat\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Pullover\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Coat\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Pullover\n",
      "Coat\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Dress\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Trouser\n",
      "Dress\n",
      "Shirt\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Trouser\n",
      "T-shirt/top\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Bag\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Coat\n",
      "Coat\n",
      "Dress\n",
      "Bag\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Trouser\n",
      "Dress\n",
      "Pullover\n",
      "Shirt\n",
      "Bag\n",
      "Pullover\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Dress\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Dress\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "T-shirt/top\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Dress\n",
      "Pullover\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Pullover\n",
      "Coat\n",
      "Sandal\n",
      "Sneaker\n",
      "Pullover\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "Dress\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Pullover\n",
      "Bag\n",
      "Shirt\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Bag\n",
      "Trouser\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Dress\n",
      "Sandal\n",
      "Bag\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Bag\n",
      "Sneaker\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Dress\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "T-shirt/top\n",
      "Dress\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Bag\n",
      "T-shirt/top\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "Trouser\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Shirt\n",
      "Coat\n",
      "Coat\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Shirt\n",
      "T-shirt/top\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "Coat\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Shirt\n",
      "Pullover\n",
      "Coat\n",
      "Pullover\n",
      "Pullover\n",
      "Dress\n",
      "Sandal\n",
      "Sneaker\n",
      "Coat\n",
      "Pullover\n",
      "Sneaker\n",
      "Ankle, boot\n",
      "Coat\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Sneaker\n",
      "Sandal\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Trouser\n",
      "T-shirt/top\n",
      "Bag\n",
      "Trouser\n",
      "Coat\n",
      "Shirt\n",
      "Ankle, boot\n",
      "Sneaker\n",
      "Bag\n",
      "T-shirt/top\n",
      "Ankle, boot\n",
      "Sandal\n",
      "Bag\n",
      "Pullover\n",
      "Coat\n",
      "Shirt\n",
      "Pullover\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Coat\n",
      "Shirt\n",
      "T-shirt/top\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n",
      "Trouser\n",
      "Pullover\n",
      "T-shirt/top\n",
      "Shirt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pred = np.argmax(model.predict(X_test_full), axis=1)\n",
    "\n",
    "for i in range(len(X_test_full)):\n",
    "  if pred[i] != y_test_full[i]:\n",
    "    print(class_names[pred[i]])\n",
    "    print(class_names[y_test_full[i]])\n",
    "    plt.imshow(X_test_full[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.8489 - val_loss: 0.5731\n",
      "Epoch 2/64\n",
      "363/363 [==============================] - 0s 467us/step - loss: 1.3695 - val_loss: 0.4378\n",
      "Epoch 3/64\n",
      "363/363 [==============================] - 0s 507us/step - loss: 0.4995 - val_loss: 0.4010\n",
      "Epoch 4/64\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.4248 - val_loss: 0.3853\n",
      "Epoch 5/64\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.4064 - val_loss: 0.3698\n",
      "Epoch 6/64\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.3956 - val_loss: 0.3695\n",
      "Epoch 7/64\n",
      "363/363 [==============================] - 0s 542us/step - loss: 0.3894 - val_loss: 0.3587\n",
      "Epoch 8/64\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.3858 - val_loss: 0.3558\n",
      "Epoch 9/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3784 - val_loss: 0.3520\n",
      "Epoch 10/64\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.3751 - val_loss: 0.3601\n",
      "Epoch 11/64\n",
      "363/363 [==============================] - 0s 542us/step - loss: 0.3759 - val_loss: 0.3446\n",
      "Epoch 12/64\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3715 - val_loss: 0.3494\n",
      "Epoch 13/64\n",
      "363/363 [==============================] - 0s 469us/step - loss: 0.3692 - val_loss: 0.3442\n",
      "Epoch 14/64\n",
      "363/363 [==============================] - 0s 468us/step - loss: 0.3669 - val_loss: 0.3395\n",
      "Epoch 15/64\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.3653 - val_loss: 0.3388\n",
      "Epoch 16/64\n",
      "363/363 [==============================] - 0s 542us/step - loss: 0.3625 - val_loss: 0.3472\n",
      "Epoch 17/64\n",
      "363/363 [==============================] - 0s 467us/step - loss: 0.3608 - val_loss: 0.3343\n",
      "Epoch 18/64\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.3586 - val_loss: 0.3364\n",
      "Epoch 19/64\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3565 - val_loss: 0.3361\n",
      "Epoch 20/64\n",
      "363/363 [==============================] - 0s 532us/step - loss: 0.3558 - val_loss: 0.3353\n",
      "Epoch 21/64\n",
      "363/363 [==============================] - 0s 468us/step - loss: 0.3548 - val_loss: 0.3321\n",
      "Epoch 22/64\n",
      "363/363 [==============================] - 0s 465us/step - loss: 0.3534 - val_loss: 0.3305\n",
      "Epoch 23/64\n",
      "363/363 [==============================] - 0s 471us/step - loss: 0.3508 - val_loss: 0.3278\n",
      "Epoch 24/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3500 - val_loss: 0.3396\n",
      "Epoch 25/64\n",
      "363/363 [==============================] - 0s 468us/step - loss: 0.3482 - val_loss: 0.3259\n",
      "Epoch 26/64\n",
      "363/363 [==============================] - 0s 463us/step - loss: 0.3460 - val_loss: 0.3258\n",
      "Epoch 27/64\n",
      "363/363 [==============================] - 0s 469us/step - loss: 0.3454 - val_loss: 0.3251\n",
      "Epoch 28/64\n",
      "363/363 [==============================] - 0s 467us/step - loss: 0.3451 - val_loss: 0.3331\n",
      "Epoch 29/64\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.3423 - val_loss: 0.3286\n",
      "Epoch 30/64\n",
      "363/363 [==============================] - 0s 469us/step - loss: 0.3421 - val_loss: 0.3226\n",
      "Epoch 31/64\n",
      "363/363 [==============================] - 0s 470us/step - loss: 0.3406 - val_loss: 0.3254\n",
      "Epoch 32/64\n",
      "363/363 [==============================] - 0s 465us/step - loss: 0.3399 - val_loss: 0.3177\n",
      "Epoch 33/64\n",
      "363/363 [==============================] - 0s 462us/step - loss: 0.3378 - val_loss: 0.3199\n",
      "Epoch 34/64\n",
      "363/363 [==============================] - 0s 467us/step - loss: 0.3374 - val_loss: 0.3193\n",
      "Epoch 35/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3367 - val_loss: 0.3152\n",
      "Epoch 36/64\n",
      "363/363 [==============================] - 0s 468us/step - loss: 0.3355 - val_loss: 0.3193\n",
      "Epoch 37/64\n",
      "363/363 [==============================] - 0s 506us/step - loss: 0.3356 - val_loss: 0.3140\n",
      "Epoch 38/64\n",
      "363/363 [==============================] - 0s 488us/step - loss: 0.3343 - val_loss: 0.3167\n",
      "Epoch 39/64\n",
      "363/363 [==============================] - 0s 473us/step - loss: 0.3349 - val_loss: 0.3157\n",
      "Epoch 40/64\n",
      "363/363 [==============================] - 0s 469us/step - loss: 0.3325 - val_loss: 0.3176\n",
      "Epoch 41/64\n",
      "363/363 [==============================] - 0s 469us/step - loss: 0.3320 - val_loss: 0.3166\n",
      "Epoch 42/64\n",
      "363/363 [==============================] - 0s 469us/step - loss: 0.3325 - val_loss: 0.3179\n",
      "Epoch 43/64\n",
      "363/363 [==============================] - 0s 470us/step - loss: 0.3317 - val_loss: 0.3178\n",
      "Epoch 44/64\n",
      "363/363 [==============================] - 0s 467us/step - loss: 0.3297 - val_loss: 0.3092\n",
      "Epoch 45/64\n",
      "363/363 [==============================] - 0s 467us/step - loss: 0.3294 - val_loss: 0.3112\n",
      "Epoch 46/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3274 - val_loss: 0.3106\n",
      "Epoch 47/64\n",
      "363/363 [==============================] - 0s 468us/step - loss: 0.3264 - val_loss: 0.3096\n",
      "Epoch 48/64\n",
      "363/363 [==============================] - 0s 469us/step - loss: 0.3295 - val_loss: 0.3092\n",
      "Epoch 49/64\n",
      "363/363 [==============================] - 0s 467us/step - loss: 0.3332 - val_loss: 0.3132\n",
      "Epoch 50/64\n",
      "363/363 [==============================] - 0s 468us/step - loss: 0.3268 - val_loss: 0.3112\n",
      "Epoch 51/64\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.3243 - val_loss: 0.3093\n",
      "Epoch 52/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3242 - val_loss: 0.3099\n",
      "Epoch 53/64\n",
      "363/363 [==============================] - 0s 468us/step - loss: 0.3237 - val_loss: 0.3080\n",
      "Epoch 54/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3223 - val_loss: 0.3053\n",
      "Epoch 55/64\n",
      "363/363 [==============================] - 0s 465us/step - loss: 0.3222 - val_loss: 0.3049\n",
      "Epoch 56/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3204 - val_loss: 0.3032\n",
      "Epoch 57/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3193 - val_loss: 0.3056\n",
      "Epoch 58/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3186 - val_loss: 0.3099\n",
      "Epoch 59/64\n",
      "363/363 [==============================] - 0s 469us/step - loss: 0.3186 - val_loss: 0.3033\n",
      "Epoch 60/64\n",
      "363/363 [==============================] - 0s 470us/step - loss: 0.3176 - val_loss: 0.3006\n",
      "Epoch 61/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3176 - val_loss: 0.3041\n",
      "Epoch 62/64\n",
      "363/363 [==============================] - 0s 469us/step - loss: 0.3176 - val_loss: 0.3010\n",
      "Epoch 63/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3161 - val_loss: 0.3057\n",
      "Epoch 64/64\n",
      "363/363 [==============================] - 0s 466us/step - loss: 0.3142 - val_loss: 0.3056\n",
      "162/162 [==============================] - 0s 333us/step - loss: 0.3320\n",
      "0.3320491909980774\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "  keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "  keras.layers.Dense(1),\n",
    "])\n",
    "model.compile(loss=keras.losses.mean_squared_error, optimizer=keras.optimizers.SGD(0.01))\n",
    "history = model.fit(X_train, y_train, epochs=64, validation_data=(X_val, y_val))\n",
    "print(model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 304us/step\n",
      "[3.964 1.869 1.676 ... 1.087 1.232 2.725] [[3.817144 ]\n",
      " [3.3610525]\n",
      " [1.9695238]\n",
      " ...\n",
      " [1.0668554]\n",
      " [1.3867844]\n",
      " [2.4804306]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 8)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 60)           540         ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, 30)           1830        ['dense_50[0][0]']               \n",
      "                                                                                                  \n",
      " dense_52 (Dense)               (None, 10)           310         ['dense_51[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 18)           0           ['input_8[0][0]',                \n",
      "                                                                  'dense_52[0][0]']               \n",
      "                                                                                                  \n",
      " dense_53 (Dense)               (None, 1)            19          ['concatenate_7[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,699\n",
      "Trainable params: 2,699\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Wide and deep network\n",
    "input_ = keras.layers.Input(shape=X_train_full.shape[1:])\n",
    "hidden1 = keras.layers.Dense(60, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "hidden3 = keras.layers.Dense(10, activation='relu')(hidden2)\n",
    "concat = keras.layers.Concatenate()([input_, hidden3])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "model.compile(loss=keras.losses.mean_squared_error, optimizer=keras.optimizers.SGD(0.005))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.2885 - val_loss: 0.2904\n",
      "Epoch 2/128\n",
      "363/363 [==============================] - 0s 527us/step - loss: 0.2910 - val_loss: 0.2839\n",
      "Epoch 3/128\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.2913 - val_loss: 0.2807\n",
      "Epoch 4/128\n",
      "363/363 [==============================] - 0s 527us/step - loss: 0.2859 - val_loss: 0.2804\n",
      "Epoch 5/128\n",
      "363/363 [==============================] - 0s 504us/step - loss: 0.2842 - val_loss: 0.2853\n",
      "Epoch 6/128\n",
      "363/363 [==============================] - 0s 503us/step - loss: 0.2874 - val_loss: 0.2833\n",
      "Epoch 7/128\n",
      "363/363 [==============================] - 0s 500us/step - loss: 0.2884 - val_loss: 0.2800\n",
      "Epoch 8/128\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.2860 - val_loss: 0.2990\n",
      "Epoch 9/128\n",
      "363/363 [==============================] - 0s 525us/step - loss: 0.2865 - val_loss: 0.2869\n",
      "Epoch 10/128\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.2872 - val_loss: 0.2776\n",
      "Epoch 11/128\n",
      "363/363 [==============================] - 0s 505us/step - loss: 0.2906 - val_loss: 0.2825\n",
      "Epoch 12/128\n",
      "363/363 [==============================] - 0s 502us/step - loss: 0.2955 - val_loss: 0.2772\n",
      "Epoch 13/128\n",
      "363/363 [==============================] - 0s 503us/step - loss: 0.2845 - val_loss: 0.2804\n",
      "Epoch 14/128\n",
      "363/363 [==============================] - 0s 504us/step - loss: 0.2871 - val_loss: 0.2990\n",
      "Epoch 15/128\n",
      "363/363 [==============================] - 0s 504us/step - loss: 0.2805 - val_loss: 0.2760\n",
      "Epoch 16/128\n",
      "363/363 [==============================] - 0s 502us/step - loss: 0.2833 - val_loss: 0.2849\n",
      "Epoch 17/128\n",
      "363/363 [==============================] - 0s 505us/step - loss: 0.2835 - val_loss: 0.2770\n",
      "Epoch 18/128\n",
      "363/363 [==============================] - 0s 501us/step - loss: 0.2821 - val_loss: 0.2759\n",
      "Epoch 19/128\n",
      "363/363 [==============================] - 0s 509us/step - loss: 0.2782 - val_loss: 0.2797\n",
      "Epoch 20/128\n",
      "363/363 [==============================] - 0s 530us/step - loss: 0.2788 - val_loss: 0.2795\n",
      "Epoch 21/128\n",
      "363/363 [==============================] - 0s 502us/step - loss: 0.2804 - val_loss: 0.2775\n",
      "Epoch 22/128\n",
      "363/363 [==============================] - 0s 501us/step - loss: 0.2806 - val_loss: 0.2825\n",
      "Epoch 23/128\n",
      "363/363 [==============================] - 0s 499us/step - loss: 0.2779 - val_loss: 0.2788\n",
      "Epoch 24/128\n",
      "363/363 [==============================] - 0s 501us/step - loss: 0.2809 - val_loss: 0.2932\n",
      "Epoch 25/128\n",
      "363/363 [==============================] - 0s 503us/step - loss: 0.2797 - val_loss: 0.3392\n",
      "Epoch 26/128\n",
      "363/363 [==============================] - 0s 499us/step - loss: 0.2776 - val_loss: 0.2750\n",
      "Epoch 27/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2754 - val_loss: 0.2919\n",
      "Epoch 28/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2762 - val_loss: 0.2728\n",
      "Epoch 29/128\n",
      "363/363 [==============================] - 0s 508us/step - loss: 0.2761 - val_loss: 0.2825\n",
      "Epoch 30/128\n",
      "363/363 [==============================] - 0s 501us/step - loss: 0.2751 - val_loss: 0.2795\n",
      "Epoch 31/128\n",
      "363/363 [==============================] - 0s 499us/step - loss: 0.2757 - val_loss: 0.2739\n",
      "Epoch 32/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2768 - val_loss: 0.2780\n",
      "Epoch 33/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2760 - val_loss: 0.2791\n",
      "Epoch 34/128\n",
      "363/363 [==============================] - 0s 494us/step - loss: 0.2833 - val_loss: 0.2858\n",
      "Epoch 35/128\n",
      "363/363 [==============================] - 0s 495us/step - loss: 0.3076 - val_loss: 0.2856\n",
      "Epoch 36/128\n",
      "363/363 [==============================] - 0s 501us/step - loss: 0.2754 - val_loss: 0.2762\n",
      "Epoch 37/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2737 - val_loss: 0.2790\n",
      "Epoch 38/128\n",
      "363/363 [==============================] - 0s 495us/step - loss: 0.2758 - val_loss: 0.2798\n",
      "Epoch 39/128\n",
      "363/363 [==============================] - 0s 500us/step - loss: 0.2742 - val_loss: 0.2823\n",
      "Epoch 40/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2727 - val_loss: 0.2790\n",
      "Epoch 41/128\n",
      "363/363 [==============================] - 0s 499us/step - loss: 0.2748 - val_loss: 0.2751\n",
      "Epoch 42/128\n",
      "363/363 [==============================] - 0s 505us/step - loss: 0.2756 - val_loss: 0.2847\n",
      "Epoch 43/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2775 - val_loss: 0.2727\n",
      "Epoch 44/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2744 - val_loss: 0.2752\n",
      "Epoch 45/128\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.2734 - val_loss: 0.2717\n",
      "Epoch 46/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2739 - val_loss: 0.2778\n",
      "Epoch 47/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2710 - val_loss: 0.2782\n",
      "Epoch 48/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2721 - val_loss: 0.2719\n",
      "Epoch 49/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2742 - val_loss: 0.2875\n",
      "Epoch 50/128\n",
      "363/363 [==============================] - 0s 495us/step - loss: 0.2674 - val_loss: 0.2740\n",
      "Epoch 51/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2692 - val_loss: 0.2887\n",
      "Epoch 52/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2669 - val_loss: 0.2781\n",
      "Epoch 53/128\n",
      "363/363 [==============================] - 0s 507us/step - loss: 0.2681 - val_loss: 0.2714\n",
      "Epoch 54/128\n",
      "363/363 [==============================] - 0s 499us/step - loss: 0.2693 - val_loss: 0.2764\n",
      "Epoch 55/128\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.2678 - val_loss: 0.2776\n",
      "Epoch 56/128\n",
      "363/363 [==============================] - 0s 499us/step - loss: 0.2679 - val_loss: 0.2742\n",
      "Epoch 57/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2655 - val_loss: 0.2724\n",
      "Epoch 58/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2649 - val_loss: 0.2860\n",
      "Epoch 59/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2662 - val_loss: 0.3051\n",
      "Epoch 60/128\n",
      "363/363 [==============================] - 0s 500us/step - loss: 0.2644 - val_loss: 0.2746\n",
      "Epoch 61/128\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.2660 - val_loss: 0.2729\n",
      "Epoch 62/128\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.2675 - val_loss: 0.2735\n",
      "Epoch 63/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2643 - val_loss: 0.2726\n",
      "Epoch 64/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2648 - val_loss: 0.2730\n",
      "Epoch 65/128\n",
      "363/363 [==============================] - 0s 499us/step - loss: 0.2657 - val_loss: 0.2726\n",
      "Epoch 66/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2684 - val_loss: 0.2810\n",
      "Epoch 67/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2635 - val_loss: 0.2767\n",
      "Epoch 68/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2628 - val_loss: 0.2777\n",
      "Epoch 69/128\n",
      "363/363 [==============================] - 0s 494us/step - loss: 0.2611 - val_loss: 0.2803\n",
      "Epoch 70/128\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.2650 - val_loss: 0.2830\n",
      "Epoch 71/128\n",
      "363/363 [==============================] - 0s 494us/step - loss: 0.2637 - val_loss: 0.2743\n",
      "Epoch 72/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2641 - val_loss: 0.2731\n",
      "Epoch 73/128\n",
      "363/363 [==============================] - 0s 495us/step - loss: 0.2638 - val_loss: 0.2743\n",
      "Epoch 74/128\n",
      "363/363 [==============================] - 0s 494us/step - loss: 0.2617 - val_loss: 0.2812\n",
      "Epoch 75/128\n",
      "363/363 [==============================] - 0s 495us/step - loss: 0.2612 - val_loss: 0.2712\n",
      "Epoch 76/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2597 - val_loss: 0.2698\n",
      "Epoch 77/128\n",
      "363/363 [==============================] - 0s 504us/step - loss: 0.2607 - val_loss: 0.2712\n",
      "Epoch 78/128\n",
      "363/363 [==============================] - 0s 494us/step - loss: 0.2636 - val_loss: 0.2734\n",
      "Epoch 79/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2606 - val_loss: 0.2708\n",
      "Epoch 80/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2646 - val_loss: 0.2747\n",
      "Epoch 81/128\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.2630 - val_loss: 0.2832\n",
      "Epoch 82/128\n",
      "363/363 [==============================] - 0s 491us/step - loss: 0.2583 - val_loss: 0.2699\n",
      "Epoch 83/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2586 - val_loss: 0.2750\n",
      "Epoch 84/128\n",
      "363/363 [==============================] - 0s 490us/step - loss: 0.2597 - val_loss: 0.2729\n",
      "Epoch 85/128\n",
      "363/363 [==============================] - 0s 495us/step - loss: 0.2611 - val_loss: 0.2710\n",
      "Epoch 86/128\n",
      "363/363 [==============================] - 0s 495us/step - loss: 0.2592 - val_loss: 0.2744\n",
      "Epoch 87/128\n",
      "363/363 [==============================] - 0s 495us/step - loss: 0.2620 - val_loss: 0.2728\n",
      "Epoch 88/128\n",
      "363/363 [==============================] - 0s 507us/step - loss: 0.2580 - val_loss: 0.2761\n",
      "Epoch 89/128\n",
      "363/363 [==============================] - 0s 489us/step - loss: 0.2570 - val_loss: 0.2707\n",
      "Epoch 90/128\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.2600 - val_loss: 0.2747\n",
      "Epoch 91/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2605 - val_loss: 0.2668\n",
      "Epoch 92/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2571 - val_loss: 0.2769\n",
      "Epoch 93/128\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.2562 - val_loss: 0.2656\n",
      "Epoch 94/128\n",
      "363/363 [==============================] - 0s 494us/step - loss: 0.2550 - val_loss: 0.2781\n",
      "Epoch 95/128\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.2558 - val_loss: 0.2774\n",
      "Epoch 96/128\n",
      "363/363 [==============================] - 0s 495us/step - loss: 0.2610 - val_loss: 0.2743\n",
      "Epoch 97/128\n",
      "363/363 [==============================] - 0s 500us/step - loss: 0.2617 - val_loss: 0.2675\n",
      "Epoch 98/128\n",
      "363/363 [==============================] - 0s 491us/step - loss: 0.2551 - val_loss: 0.2735\n",
      "Epoch 99/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2537 - val_loss: 0.2691\n",
      "Epoch 100/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2546 - val_loss: 0.2814\n",
      "Epoch 101/128\n",
      "363/363 [==============================] - 0s 501us/step - loss: 0.2569 - val_loss: 0.2768\n",
      "Epoch 102/128\n",
      "363/363 [==============================] - 0s 494us/step - loss: 0.2529 - val_loss: 0.3279\n",
      "Epoch 103/128\n",
      "363/363 [==============================] - 0s 530us/step - loss: 0.2544 - val_loss: 0.2745\n",
      "Epoch 104/128\n",
      "363/363 [==============================] - 0s 499us/step - loss: 0.2536 - val_loss: 0.3172\n",
      "Epoch 105/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2567 - val_loss: 0.2700\n",
      "Epoch 106/128\n",
      "363/363 [==============================] - 0s 502us/step - loss: 0.2566 - val_loss: 0.2716\n",
      "Epoch 107/128\n",
      "363/363 [==============================] - 0s 499us/step - loss: 0.2538 - val_loss: 0.2797\n",
      "Epoch 108/128\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.2530 - val_loss: 0.2782\n",
      "Epoch 109/128\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.2509 - val_loss: 0.2735\n",
      "Epoch 110/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2510 - val_loss: 0.2703\n",
      "Epoch 111/128\n",
      "363/363 [==============================] - 0s 511us/step - loss: 0.2508 - val_loss: 0.2808\n",
      "Epoch 112/128\n",
      "363/363 [==============================] - 0s 501us/step - loss: 0.2510 - val_loss: 0.2758\n",
      "Epoch 113/128\n",
      "363/363 [==============================] - 0s 495us/step - loss: 0.2523 - val_loss: 0.2673\n",
      "Epoch 114/128\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.2516 - val_loss: 0.2718\n",
      "Epoch 115/128\n",
      "363/363 [==============================] - 0s 489us/step - loss: 0.2510 - val_loss: 0.2651\n",
      "Epoch 116/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2522 - val_loss: 0.2693\n",
      "Epoch 117/128\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.2504 - val_loss: 0.2739\n",
      "Epoch 118/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2524 - val_loss: 0.2682\n",
      "Epoch 119/128\n",
      "363/363 [==============================] - 0s 489us/step - loss: 0.2509 - val_loss: 0.2787\n",
      "Epoch 120/128\n",
      "363/363 [==============================] - 0s 493us/step - loss: 0.2508 - val_loss: 0.2705\n",
      "Epoch 121/128\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.2515 - val_loss: 0.2761\n",
      "Epoch 122/128\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.2493 - val_loss: 0.2709\n",
      "Epoch 123/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2519 - val_loss: 0.2660\n",
      "Epoch 124/128\n",
      "363/363 [==============================] - 0s 505us/step - loss: 0.2485 - val_loss: 0.2664\n",
      "Epoch 125/128\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2502 - val_loss: 0.2731\n",
      "Epoch 126/128\n",
      "363/363 [==============================] - 0s 491us/step - loss: 0.2482 - val_loss: 0.2779\n",
      "Epoch 127/128\n",
      "363/363 [==============================] - 0s 494us/step - loss: 0.2480 - val_loss: 0.2692\n",
      "Epoch 128/128\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2511 - val_loss: 0.2707\n",
      "162/162 [==============================] - 0s 356us/step - loss: 0.2893\n",
      "0.28929993510246277\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=128, validation_data=(X_val, y_val))\n",
    "print(model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 372us/step - loss: 0.2893\n",
      "0.28929993510246277\n",
      "162/162 [==============================] - 0s 336us/step\n",
      "[[3.989099 ]\n",
      " [3.182486 ]\n",
      " [2.1702776]\n",
      " ...\n",
      " [1.1236055]\n",
      " [1.7190018]\n",
      " [2.9968128]] [3.964 1.869 1.676 ... 1.087 1.232 2.725]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(X_test, y_test))\n",
    "print(model.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_deep (InputLayer)        [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_75 (Dense)               (None, 30)           150         ['input_deep[0][0]']             \n",
      "                                                                                                  \n",
      " input_wide (InputLayer)        [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_76 (Dense)               (None, 30)           930         ['dense_75[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_16 (Concatenate)   (None, 34)           0           ['input_wide[0][0]',             \n",
      "                                                                  'dense_76[0][0]']               \n",
      "                                                                                                  \n",
      " output1 (Dense)                (None, 1)            35          ['concatenate_16[0][0]']         \n",
      "                                                                                                  \n",
      " output2 (Dense)                (None, 1)            31          ['dense_76[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,146\n",
      "Trainable params: 1,146\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_wide = keras.layers.Input(shape=(4), name='input_wide')\n",
    "input_deep = keras.layers.Input(shape=(4), name='input_deep')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_deep)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_wide, hidden2])\n",
    "output1 = keras.layers.Dense(1, name='output1')(concat)\n",
    "output2 = keras.layers.Dense(1, name='output2')(hidden2)\n",
    "model = keras.Model(inputs=[input_wide, input_deep], outputs=[output1, output2])\n",
    "\n",
    "model.compile(\n",
    "  loss=[keras.losses.mean_squared_error, keras.losses.mean_squared_error],\n",
    "  loss_weights=(0.5, 0.5),\n",
    "  optimizer=keras.optimizers.SGD(0.01),\n",
    "  )\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.1840 - output1_loss: 0.9193 - output2_loss: 1.4487 - val_loss: 0.6747 - val_output1_loss: 0.4625 - val_output2_loss: 0.8869\n",
      "Epoch 2/128\n",
      "363/363 [==============================] - 0s 611us/step - loss: 0.7132 - output1_loss: 0.5042 - output2_loss: 0.9222 - val_loss: 0.6330 - val_output1_loss: 0.4251 - val_output2_loss: 0.8409\n",
      "Epoch 3/128\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.6708 - output1_loss: 0.4644 - output2_loss: 0.8772 - val_loss: 0.6069 - val_output1_loss: 0.4040 - val_output2_loss: 0.8097\n",
      "Epoch 4/128\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.6673 - output1_loss: 0.4593 - output2_loss: 0.8754 - val_loss: 0.6033 - val_output1_loss: 0.4003 - val_output2_loss: 0.8062\n",
      "Epoch 5/128\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.6485 - output1_loss: 0.4426 - output2_loss: 0.8544 - val_loss: 0.5932 - val_output1_loss: 0.3911 - val_output2_loss: 0.7952\n",
      "Epoch 6/128\n",
      "363/363 [==============================] - 0s 582us/step - loss: 0.6348 - output1_loss: 0.4335 - output2_loss: 0.8362 - val_loss: 0.5862 - val_output1_loss: 0.3866 - val_output2_loss: 0.7858\n",
      "Epoch 7/128\n",
      "363/363 [==============================] - 0s 667us/step - loss: 0.6284 - output1_loss: 0.4288 - output2_loss: 0.8280 - val_loss: 0.5822 - val_output1_loss: 0.3839 - val_output2_loss: 0.7805\n",
      "Epoch 8/128\n",
      "363/363 [==============================] - 0s 600us/step - loss: 0.6228 - output1_loss: 0.4266 - output2_loss: 0.8190 - val_loss: 0.5852 - val_output1_loss: 0.3869 - val_output2_loss: 0.7835\n",
      "Epoch 9/128\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.6166 - output1_loss: 0.4184 - output2_loss: 0.8148 - val_loss: 0.5711 - val_output1_loss: 0.3736 - val_output2_loss: 0.7687\n",
      "Epoch 10/128\n",
      "363/363 [==============================] - 0s 585us/step - loss: 0.6109 - output1_loss: 0.4140 - output2_loss: 0.8078 - val_loss: 0.5670 - val_output1_loss: 0.3717 - val_output2_loss: 0.7623\n",
      "Epoch 11/128\n",
      "363/363 [==============================] - 0s 625us/step - loss: 0.6063 - output1_loss: 0.4116 - output2_loss: 0.8009 - val_loss: 0.5662 - val_output1_loss: 0.3708 - val_output2_loss: 0.7616\n",
      "Epoch 12/128\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.6004 - output1_loss: 0.4059 - output2_loss: 0.7948 - val_loss: 0.5587 - val_output1_loss: 0.3639 - val_output2_loss: 0.7536\n",
      "Epoch 13/128\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.5987 - output1_loss: 0.4051 - output2_loss: 0.7923 - val_loss: 0.5571 - val_output1_loss: 0.3639 - val_output2_loss: 0.7503\n",
      "Epoch 14/128\n",
      "363/363 [==============================] - 0s 661us/step - loss: 0.5920 - output1_loss: 0.4006 - output2_loss: 0.7835 - val_loss: 0.5554 - val_output1_loss: 0.3640 - val_output2_loss: 0.7468\n",
      "Epoch 15/128\n",
      "363/363 [==============================] - 0s 602us/step - loss: 0.5878 - output1_loss: 0.3963 - output2_loss: 0.7794 - val_loss: 0.5589 - val_output1_loss: 0.3670 - val_output2_loss: 0.7507\n",
      "Epoch 16/128\n",
      "363/363 [==============================] - 0s 635us/step - loss: 0.5863 - output1_loss: 0.3942 - output2_loss: 0.7784 - val_loss: 0.5450 - val_output1_loss: 0.3544 - val_output2_loss: 0.7357\n",
      "Epoch 17/128\n",
      "363/363 [==============================] - 0s 618us/step - loss: 0.5827 - output1_loss: 0.3904 - output2_loss: 0.7749 - val_loss: 0.5437 - val_output1_loss: 0.3547 - val_output2_loss: 0.7327\n",
      "Epoch 18/128\n",
      "363/363 [==============================] - 0s 653us/step - loss: 0.5787 - output1_loss: 0.3904 - output2_loss: 0.7670 - val_loss: 0.5459 - val_output1_loss: 0.3562 - val_output2_loss: 0.7356\n",
      "Epoch 19/128\n",
      "363/363 [==============================] - 0s 593us/step - loss: 0.5763 - output1_loss: 0.3880 - output2_loss: 0.7646 - val_loss: 0.5394 - val_output1_loss: 0.3507 - val_output2_loss: 0.7280\n",
      "Epoch 20/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5739 - output1_loss: 0.3867 - output2_loss: 0.7610 - val_loss: 0.5514 - val_output1_loss: 0.3610 - val_output2_loss: 0.7417\n",
      "Epoch 21/128\n",
      "363/363 [==============================] - 0s 601us/step - loss: 0.5714 - output1_loss: 0.3836 - output2_loss: 0.7591 - val_loss: 0.5498 - val_output1_loss: 0.3575 - val_output2_loss: 0.7421\n",
      "Epoch 22/128\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.5690 - output1_loss: 0.3824 - output2_loss: 0.7555 - val_loss: 0.5356 - val_output1_loss: 0.3489 - val_output2_loss: 0.7223\n",
      "Epoch 23/128\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.5693 - output1_loss: 0.3840 - output2_loss: 0.7546 - val_loss: 0.5443 - val_output1_loss: 0.3566 - val_output2_loss: 0.7320\n",
      "Epoch 24/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5665 - output1_loss: 0.3805 - output2_loss: 0.7525 - val_loss: 0.5443 - val_output1_loss: 0.3552 - val_output2_loss: 0.7334\n",
      "Epoch 25/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.5663 - output1_loss: 0.3807 - output2_loss: 0.7520 - val_loss: 0.5432 - val_output1_loss: 0.3526 - val_output2_loss: 0.7338\n",
      "Epoch 26/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.5640 - output1_loss: 0.3801 - output2_loss: 0.7479 - val_loss: 0.5340 - val_output1_loss: 0.3478 - val_output2_loss: 0.7201\n",
      "Epoch 27/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5624 - output1_loss: 0.3790 - output2_loss: 0.7459 - val_loss: 0.5480 - val_output1_loss: 0.3586 - val_output2_loss: 0.7373\n",
      "Epoch 28/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5632 - output1_loss: 0.3786 - output2_loss: 0.7478 - val_loss: 0.5364 - val_output1_loss: 0.3504 - val_output2_loss: 0.7224\n",
      "Epoch 29/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5607 - output1_loss: 0.3771 - output2_loss: 0.7443 - val_loss: 0.5403 - val_output1_loss: 0.3541 - val_output2_loss: 0.7264\n",
      "Epoch 30/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5599 - output1_loss: 0.3764 - output2_loss: 0.7433 - val_loss: 0.5520 - val_output1_loss: 0.3644 - val_output2_loss: 0.7396\n",
      "Epoch 31/128\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.5591 - output1_loss: 0.3755 - output2_loss: 0.7427 - val_loss: 0.5338 - val_output1_loss: 0.3479 - val_output2_loss: 0.7197\n",
      "Epoch 32/128\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.5575 - output1_loss: 0.3753 - output2_loss: 0.7397 - val_loss: 0.5327 - val_output1_loss: 0.3482 - val_output2_loss: 0.7172\n",
      "Epoch 33/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5578 - output1_loss: 0.3757 - output2_loss: 0.7399 - val_loss: 0.5362 - val_output1_loss: 0.3493 - val_output2_loss: 0.7232\n",
      "Epoch 34/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5571 - output1_loss: 0.3737 - output2_loss: 0.7405 - val_loss: 0.5293 - val_output1_loss: 0.3459 - val_output2_loss: 0.7128\n",
      "Epoch 35/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5558 - output1_loss: 0.3738 - output2_loss: 0.7378 - val_loss: 0.5286 - val_output1_loss: 0.3438 - val_output2_loss: 0.7134\n",
      "Epoch 36/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5553 - output1_loss: 0.3735 - output2_loss: 0.7371 - val_loss: 0.5339 - val_output1_loss: 0.3483 - val_output2_loss: 0.7194\n",
      "Epoch 37/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5559 - output1_loss: 0.3743 - output2_loss: 0.7375 - val_loss: 0.5320 - val_output1_loss: 0.3481 - val_output2_loss: 0.7159\n",
      "Epoch 38/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5539 - output1_loss: 0.3723 - output2_loss: 0.7356 - val_loss: 0.5398 - val_output1_loss: 0.3496 - val_output2_loss: 0.7300\n",
      "Epoch 39/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5526 - output1_loss: 0.3718 - output2_loss: 0.7335 - val_loss: 0.5938 - val_output1_loss: 0.3907 - val_output2_loss: 0.7969\n",
      "Epoch 40/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5534 - output1_loss: 0.3716 - output2_loss: 0.7352 - val_loss: 0.5245 - val_output1_loss: 0.3406 - val_output2_loss: 0.7084\n",
      "Epoch 41/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5526 - output1_loss: 0.3725 - output2_loss: 0.7328 - val_loss: 0.5331 - val_output1_loss: 0.3462 - val_output2_loss: 0.7199\n",
      "Epoch 42/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.5514 - output1_loss: 0.3709 - output2_loss: 0.7319 - val_loss: 0.5252 - val_output1_loss: 0.3421 - val_output2_loss: 0.7083\n",
      "Epoch 43/128\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.5511 - output1_loss: 0.3712 - output2_loss: 0.7310 - val_loss: 0.5285 - val_output1_loss: 0.3456 - val_output2_loss: 0.7114\n",
      "Epoch 44/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.5507 - output1_loss: 0.3703 - output2_loss: 0.7312 - val_loss: 0.5251 - val_output1_loss: 0.3427 - val_output2_loss: 0.7075\n",
      "Epoch 45/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5491 - output1_loss: 0.3698 - output2_loss: 0.7284 - val_loss: 0.5332 - val_output1_loss: 0.3492 - val_output2_loss: 0.7172\n",
      "Epoch 46/128\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.5499 - output1_loss: 0.3697 - output2_loss: 0.7301 - val_loss: 0.5287 - val_output1_loss: 0.3465 - val_output2_loss: 0.7108\n",
      "Epoch 47/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5485 - output1_loss: 0.3690 - output2_loss: 0.7279 - val_loss: 0.5320 - val_output1_loss: 0.3504 - val_output2_loss: 0.7136\n",
      "Epoch 48/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5472 - output1_loss: 0.3674 - output2_loss: 0.7270 - val_loss: 0.5366 - val_output1_loss: 0.3517 - val_output2_loss: 0.7214\n",
      "Epoch 49/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5461 - output1_loss: 0.3674 - output2_loss: 0.7247 - val_loss: 0.5220 - val_output1_loss: 0.3409 - val_output2_loss: 0.7031\n",
      "Epoch 50/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5451 - output1_loss: 0.3661 - output2_loss: 0.7240 - val_loss: 0.5321 - val_output1_loss: 0.3503 - val_output2_loss: 0.7139\n",
      "Epoch 51/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5441 - output1_loss: 0.3662 - output2_loss: 0.7220 - val_loss: 0.5188 - val_output1_loss: 0.3393 - val_output2_loss: 0.6983\n",
      "Epoch 52/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5435 - output1_loss: 0.3660 - output2_loss: 0.7210 - val_loss: 0.5183 - val_output1_loss: 0.3382 - val_output2_loss: 0.6985\n",
      "Epoch 53/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.5426 - output1_loss: 0.3648 - output2_loss: 0.7204 - val_loss: 0.5205 - val_output1_loss: 0.3393 - val_output2_loss: 0.7017\n",
      "Epoch 54/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5421 - output1_loss: 0.3652 - output2_loss: 0.7190 - val_loss: 0.5385 - val_output1_loss: 0.3553 - val_output2_loss: 0.7217\n",
      "Epoch 55/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5429 - output1_loss: 0.3654 - output2_loss: 0.7205 - val_loss: 0.5301 - val_output1_loss: 0.3476 - val_output2_loss: 0.7126\n",
      "Epoch 56/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5421 - output1_loss: 0.3655 - output2_loss: 0.7187 - val_loss: 0.5206 - val_output1_loss: 0.3400 - val_output2_loss: 0.7013\n",
      "Epoch 57/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5394 - output1_loss: 0.3626 - output2_loss: 0.7163 - val_loss: 0.5446 - val_output1_loss: 0.3592 - val_output2_loss: 0.7300\n",
      "Epoch 58/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5383 - output1_loss: 0.3626 - output2_loss: 0.7140 - val_loss: 0.5336 - val_output1_loss: 0.3488 - val_output2_loss: 0.7185\n",
      "Epoch 59/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5379 - output1_loss: 0.3622 - output2_loss: 0.7136 - val_loss: 0.5129 - val_output1_loss: 0.3350 - val_output2_loss: 0.6907\n",
      "Epoch 60/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.5374 - output1_loss: 0.3617 - output2_loss: 0.7132 - val_loss: 0.5213 - val_output1_loss: 0.3421 - val_output2_loss: 0.7006\n",
      "Epoch 61/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5373 - output1_loss: 0.3624 - output2_loss: 0.7123 - val_loss: 0.5236 - val_output1_loss: 0.3437 - val_output2_loss: 0.7035\n",
      "Epoch 62/128\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.5380 - output1_loss: 0.3623 - output2_loss: 0.7138 - val_loss: 0.5178 - val_output1_loss: 0.3369 - val_output2_loss: 0.6987\n",
      "Epoch 63/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5368 - output1_loss: 0.3625 - output2_loss: 0.7111 - val_loss: 0.5150 - val_output1_loss: 0.3361 - val_output2_loss: 0.6939\n",
      "Epoch 64/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5354 - output1_loss: 0.3613 - output2_loss: 0.7094 - val_loss: 0.5142 - val_output1_loss: 0.3353 - val_output2_loss: 0.6930\n",
      "Epoch 65/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5327 - output1_loss: 0.3588 - output2_loss: 0.7065 - val_loss: 0.5183 - val_output1_loss: 0.3403 - val_output2_loss: 0.6963\n",
      "Epoch 66/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5347 - output1_loss: 0.3604 - output2_loss: 0.7091 - val_loss: 0.5161 - val_output1_loss: 0.3376 - val_output2_loss: 0.6945\n",
      "Epoch 67/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.5333 - output1_loss: 0.3594 - output2_loss: 0.7072 - val_loss: 0.5224 - val_output1_loss: 0.3429 - val_output2_loss: 0.7020\n",
      "Epoch 68/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5331 - output1_loss: 0.3588 - output2_loss: 0.7073 - val_loss: 0.5100 - val_output1_loss: 0.3321 - val_output2_loss: 0.6880\n",
      "Epoch 69/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5316 - output1_loss: 0.3586 - output2_loss: 0.7047 - val_loss: 0.5091 - val_output1_loss: 0.3326 - val_output2_loss: 0.6856\n",
      "Epoch 70/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5332 - output1_loss: 0.3588 - output2_loss: 0.7076 - val_loss: 0.5202 - val_output1_loss: 0.3400 - val_output2_loss: 0.7005\n",
      "Epoch 71/128\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.5334 - output1_loss: 0.3588 - output2_loss: 0.7080 - val_loss: 0.5126 - val_output1_loss: 0.3353 - val_output2_loss: 0.6899\n",
      "Epoch 72/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.5301 - output1_loss: 0.3568 - output2_loss: 0.7033 - val_loss: 0.5482 - val_output1_loss: 0.3580 - val_output2_loss: 0.7383\n",
      "Epoch 73/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5320 - output1_loss: 0.3578 - output2_loss: 0.7062 - val_loss: 0.5071 - val_output1_loss: 0.3307 - val_output2_loss: 0.6835\n",
      "Epoch 74/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5313 - output1_loss: 0.3565 - output2_loss: 0.7061 - val_loss: 0.5087 - val_output1_loss: 0.3313 - val_output2_loss: 0.6860\n",
      "Epoch 75/128\n",
      "363/363 [==============================] - 0s 561us/step - loss: 0.5298 - output1_loss: 0.3564 - output2_loss: 0.7031 - val_loss: 0.5114 - val_output1_loss: 0.3344 - val_output2_loss: 0.6883\n",
      "Epoch 76/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5284 - output1_loss: 0.3557 - output2_loss: 0.7012 - val_loss: 0.5136 - val_output1_loss: 0.3386 - val_output2_loss: 0.6885\n",
      "Epoch 77/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5271 - output1_loss: 0.3550 - output2_loss: 0.6991 - val_loss: 0.5356 - val_output1_loss: 0.3506 - val_output2_loss: 0.7206\n",
      "Epoch 78/128\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.5278 - output1_loss: 0.3545 - output2_loss: 0.7011 - val_loss: 0.5168 - val_output1_loss: 0.3375 - val_output2_loss: 0.6962\n",
      "Epoch 79/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.5278 - output1_loss: 0.3551 - output2_loss: 0.7005 - val_loss: 0.5083 - val_output1_loss: 0.3310 - val_output2_loss: 0.6857\n",
      "Epoch 80/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5275 - output1_loss: 0.3550 - output2_loss: 0.7000 - val_loss: 0.5495 - val_output1_loss: 0.3584 - val_output2_loss: 0.7406\n",
      "Epoch 81/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5279 - output1_loss: 0.3552 - output2_loss: 0.7005 - val_loss: 0.5123 - val_output1_loss: 0.3342 - val_output2_loss: 0.6904\n",
      "Epoch 82/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5267 - output1_loss: 0.3546 - output2_loss: 0.6988 - val_loss: 0.5271 - val_output1_loss: 0.3465 - val_output2_loss: 0.7077\n",
      "Epoch 83/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5249 - output1_loss: 0.3535 - output2_loss: 0.6964 - val_loss: 0.5128 - val_output1_loss: 0.3365 - val_output2_loss: 0.6891\n",
      "Epoch 84/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5252 - output1_loss: 0.3548 - output2_loss: 0.6956 - val_loss: 0.5136 - val_output1_loss: 0.3356 - val_output2_loss: 0.6915\n",
      "Epoch 85/128\n",
      "363/363 [==============================] - 0s 559us/step - loss: 0.5258 - output1_loss: 0.3533 - output2_loss: 0.6983 - val_loss: 0.5098 - val_output1_loss: 0.3305 - val_output2_loss: 0.6890\n",
      "Epoch 86/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.5248 - output1_loss: 0.3530 - output2_loss: 0.6966 - val_loss: 0.5097 - val_output1_loss: 0.3306 - val_output2_loss: 0.6889\n",
      "Epoch 87/128\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.5264 - output1_loss: 0.3546 - output2_loss: 0.6982 - val_loss: 0.5174 - val_output1_loss: 0.3389 - val_output2_loss: 0.6960\n",
      "Epoch 88/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.5255 - output1_loss: 0.3538 - output2_loss: 0.6971 - val_loss: 0.5061 - val_output1_loss: 0.3296 - val_output2_loss: 0.6825\n",
      "Epoch 89/128\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.5241 - output1_loss: 0.3532 - output2_loss: 0.6949 - val_loss: 0.5209 - val_output1_loss: 0.3428 - val_output2_loss: 0.6989\n",
      "Epoch 90/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.5229 - output1_loss: 0.3529 - output2_loss: 0.6928 - val_loss: 0.5035 - val_output1_loss: 0.3272 - val_output2_loss: 0.6799\n",
      "Epoch 91/128\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.5215 - output1_loss: 0.3505 - output2_loss: 0.6925 - val_loss: 0.5054 - val_output1_loss: 0.3289 - val_output2_loss: 0.6819\n",
      "Epoch 92/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5234 - output1_loss: 0.3532 - output2_loss: 0.6935 - val_loss: 0.5067 - val_output1_loss: 0.3298 - val_output2_loss: 0.6837\n",
      "Epoch 93/128\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.5241 - output1_loss: 0.3533 - output2_loss: 0.6949 - val_loss: 0.5537 - val_output1_loss: 0.3669 - val_output2_loss: 0.7405\n",
      "Epoch 94/128\n",
      "363/363 [==============================] - 0s 561us/step - loss: 0.5222 - output1_loss: 0.3511 - output2_loss: 0.6933 - val_loss: 0.5027 - val_output1_loss: 0.3279 - val_output2_loss: 0.6775\n",
      "Epoch 95/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5231 - output1_loss: 0.3514 - output2_loss: 0.6947 - val_loss: 0.5104 - val_output1_loss: 0.3322 - val_output2_loss: 0.6886\n",
      "Epoch 96/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5216 - output1_loss: 0.3514 - output2_loss: 0.6918 - val_loss: 0.5107 - val_output1_loss: 0.3350 - val_output2_loss: 0.6863\n",
      "Epoch 97/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5214 - output1_loss: 0.3505 - output2_loss: 0.6923 - val_loss: 0.5025 - val_output1_loss: 0.3280 - val_output2_loss: 0.6770\n",
      "Epoch 98/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.5197 - output1_loss: 0.3498 - output2_loss: 0.6895 - val_loss: 0.5128 - val_output1_loss: 0.3350 - val_output2_loss: 0.6907\n",
      "Epoch 99/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5201 - output1_loss: 0.3509 - output2_loss: 0.6894 - val_loss: 0.5004 - val_output1_loss: 0.3260 - val_output2_loss: 0.6747\n",
      "Epoch 100/128\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.5204 - output1_loss: 0.3507 - output2_loss: 0.6900 - val_loss: 0.5059 - val_output1_loss: 0.3293 - val_output2_loss: 0.6826\n",
      "Epoch 101/128\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.5196 - output1_loss: 0.3498 - output2_loss: 0.6893 - val_loss: 0.5019 - val_output1_loss: 0.3267 - val_output2_loss: 0.6771\n",
      "Epoch 102/128\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.5178 - output1_loss: 0.3491 - output2_loss: 0.6866 - val_loss: 0.5025 - val_output1_loss: 0.3271 - val_output2_loss: 0.6779\n",
      "Epoch 103/128\n",
      "363/363 [==============================] - 0s 561us/step - loss: 0.5185 - output1_loss: 0.3492 - output2_loss: 0.6878 - val_loss: 0.4990 - val_output1_loss: 0.3240 - val_output2_loss: 0.6739\n",
      "Epoch 104/128\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.5166 - output1_loss: 0.3489 - output2_loss: 0.6843 - val_loss: 0.4978 - val_output1_loss: 0.3246 - val_output2_loss: 0.6711\n",
      "Epoch 105/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.5184 - output1_loss: 0.3498 - output2_loss: 0.6869 - val_loss: 0.4992 - val_output1_loss: 0.3260 - val_output2_loss: 0.6724\n",
      "Epoch 106/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5167 - output1_loss: 0.3491 - output2_loss: 0.6844 - val_loss: 0.5065 - val_output1_loss: 0.3304 - val_output2_loss: 0.6826\n",
      "Epoch 107/128\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.5171 - output1_loss: 0.3488 - output2_loss: 0.6854 - val_loss: 0.5056 - val_output1_loss: 0.3309 - val_output2_loss: 0.6804\n",
      "Epoch 108/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5183 - output1_loss: 0.3494 - output2_loss: 0.6871 - val_loss: 0.5141 - val_output1_loss: 0.3334 - val_output2_loss: 0.6949\n",
      "Epoch 109/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5173 - output1_loss: 0.3489 - output2_loss: 0.6857 - val_loss: 0.5007 - val_output1_loss: 0.3277 - val_output2_loss: 0.6736\n",
      "Epoch 110/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5159 - output1_loss: 0.3481 - output2_loss: 0.6836 - val_loss: 0.5206 - val_output1_loss: 0.3428 - val_output2_loss: 0.6983\n",
      "Epoch 111/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5137 - output1_loss: 0.3480 - output2_loss: 0.6794 - val_loss: 0.5013 - val_output1_loss: 0.3287 - val_output2_loss: 0.6740\n",
      "Epoch 112/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5153 - output1_loss: 0.3480 - output2_loss: 0.6827 - val_loss: 0.4982 - val_output1_loss: 0.3240 - val_output2_loss: 0.6724\n",
      "Epoch 113/128\n",
      "363/363 [==============================] - 0s 561us/step - loss: 0.5163 - output1_loss: 0.3477 - output2_loss: 0.6849 - val_loss: 0.5063 - val_output1_loss: 0.3311 - val_output2_loss: 0.6815\n",
      "Epoch 114/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5134 - output1_loss: 0.3475 - output2_loss: 0.6794 - val_loss: 0.4990 - val_output1_loss: 0.3262 - val_output2_loss: 0.6718\n",
      "Epoch 115/128\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.5130 - output1_loss: 0.3473 - output2_loss: 0.6787 - val_loss: 0.5085 - val_output1_loss: 0.3302 - val_output2_loss: 0.6867\n",
      "Epoch 116/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5146 - output1_loss: 0.3470 - output2_loss: 0.6822 - val_loss: 0.5160 - val_output1_loss: 0.3370 - val_output2_loss: 0.6950\n",
      "Epoch 117/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5136 - output1_loss: 0.3476 - output2_loss: 0.6795 - val_loss: 0.4966 - val_output1_loss: 0.3236 - val_output2_loss: 0.6697\n",
      "Epoch 118/128\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.5124 - output1_loss: 0.3463 - output2_loss: 0.6786 - val_loss: 0.5065 - val_output1_loss: 0.3314 - val_output2_loss: 0.6815\n",
      "Epoch 119/128\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.5121 - output1_loss: 0.3462 - output2_loss: 0.6781 - val_loss: 0.5078 - val_output1_loss: 0.3316 - val_output2_loss: 0.6840\n",
      "Epoch 120/128\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.5145 - output1_loss: 0.3471 - output2_loss: 0.6819 - val_loss: 0.5095 - val_output1_loss: 0.3343 - val_output2_loss: 0.6847\n",
      "Epoch 121/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.5127 - output1_loss: 0.3464 - output2_loss: 0.6789 - val_loss: 0.5007 - val_output1_loss: 0.3245 - val_output2_loss: 0.6769\n",
      "Epoch 122/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5121 - output1_loss: 0.3462 - output2_loss: 0.6779 - val_loss: 0.5030 - val_output1_loss: 0.3276 - val_output2_loss: 0.6784\n",
      "Epoch 123/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5132 - output1_loss: 0.3470 - output2_loss: 0.6795 - val_loss: 0.4964 - val_output1_loss: 0.3234 - val_output2_loss: 0.6693\n",
      "Epoch 124/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5125 - output1_loss: 0.3459 - output2_loss: 0.6792 - val_loss: 0.4960 - val_output1_loss: 0.3240 - val_output2_loss: 0.6680\n",
      "Epoch 125/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5118 - output1_loss: 0.3467 - output2_loss: 0.6769 - val_loss: 0.4979 - val_output1_loss: 0.3266 - val_output2_loss: 0.6692\n",
      "Epoch 126/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5111 - output1_loss: 0.3462 - output2_loss: 0.6761 - val_loss: 0.4971 - val_output1_loss: 0.3250 - val_output2_loss: 0.6692\n",
      "Epoch 127/128\n",
      "363/363 [==============================] - 0s 558us/step - loss: 0.5088 - output1_loss: 0.3453 - output2_loss: 0.6723 - val_loss: 0.5069 - val_output1_loss: 0.3313 - val_output2_loss: 0.6825\n",
      "Epoch 128/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5100 - output1_loss: 0.3461 - output2_loss: 0.6739 - val_loss: 0.5013 - val_output1_loss: 0.3278 - val_output2_loss: 0.6749\n",
      "162/162 [==============================] - 0s 439us/step - loss: 0.5135 - output1_loss: 0.3432 - output2_loss: 0.6838\n",
      "[0.513494074344635, 0.3431648910045624, 0.68382328748703]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train[:, :4], X_train[:, 4:]), (y_train, y_train), epochs=128, validation_data=((X_val[:, :4], X_val[:, 4:]), y_val))\n",
    "print(model.evaluate((X_test[:, :4], X_test[:, 4:], ), (y_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 394us/step\n",
      "[3.6428246 2.7640448 1.921495  ... 0.9335916 1.1911012 2.4535055]\n",
      "[3.5213852 2.4069636 2.1003957 ... 1.6615747 1.3980312 2.7683866]\n",
      "[3.964 1.869 1.676 ... 1.087 1.232 2.725]\n"
     ]
    }
   ],
   "source": [
    "pred1, pred2 = model.predict((X_test[:, :4], X_test[:, 4:]))\n",
    "print(pred1.reshape(1, -1).ravel())\n",
    "print(pred2.reshape(1, -1).ravel())\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/Users/carlschader/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/carlschader/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/carlschader/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/carlschader/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/carlschader/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/sw/q7k30xcj51x3tc06z506z3n00000gn/T/__autograph_generated_filet77404tk.py\", line 13, in tf__call\n        concat = ag__.converted_call(ag__.ld(keras).layers.Concatenate, (ag__.ld(input1), ag__.ld(hidden2)), None, fscope)\n\n    TypeError: Exception encountered when calling layer \"wide_and_deep_7\" \"                 f\"(type WideAndDeep).\n    \n    in user code:\n    \n        File \"/var/folders/sw/q7k30xcj51x3tc06z506z3n00000gn/T/ipykernel_42341/644067800.py\", line 13, in call  *\n            concat = keras.layers.Concatenate(input1, hidden2)\n    \n        TypeError: Concatenate.__init__() takes from 1 to 2 positional arguments but 3 were given\n    \n    \n    Call arguments received by layer \"wide_and_deep_7\" \"                 f\"(type WideAndDeep):\n      • inputs=('tf.Tensor(shape=(None, 4), dtype=float32)', 'tf.Tensor(shape=(None, 4), dtype=float32)')\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model \u001b[39m=\u001b[39m WideAndDeep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD(\u001b[39m0.01\u001b[39m), loss\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m), loss_weights\u001b[39m=\u001b[39m(\u001b[39m0.75\u001b[39m, \u001b[39m0.25\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit((X_train[:, :\u001b[39m4\u001b[39;49m], X_train[:, \u001b[39m4\u001b[39;49m:]), (y_train, y_train), epochs\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m((X_val[:, :\u001b[39m4\u001b[39;49m], X_val[:, \u001b[39m4\u001b[39;49m:]), y_val))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mevaluate((X_test[:, :\u001b[39m4\u001b[39m], X_test[:, \u001b[39m4\u001b[39m:], ), (y_test, y_test)))\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/sw/q7k30xcj51x3tc06z506z3n00000gn/T/__autograph_generated_filevb_z_ilj.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/sw/q7k30xcj51x3tc06z506z3n00000gn/T/__autograph_generated_filet77404tk.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m hidden1 \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mhidden1, (ag__\u001b[39m.\u001b[39mld(input2),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     12\u001b[0m hidden2 \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mhidden2, (ag__\u001b[39m.\u001b[39mld(hidden1),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 13\u001b[0m concat \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(keras)\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mConcatenate, (ag__\u001b[39m.\u001b[39;49mld(input1), ag__\u001b[39m.\u001b[39;49mld(hidden2)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     14\u001b[0m output1 \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(keras)\u001b[39m.\u001b[39mDense, (\u001b[39m1\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope), (ag__\u001b[39m.\u001b[39mld(concat),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     15\u001b[0m output2 \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(keras)\u001b[39m.\u001b[39mDense, (\u001b[39m1\u001b[39m,), \u001b[39mNone\u001b[39;00m, fscope), (ag__\u001b[39m.\u001b[39mld(hidden2),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/Users/carlschader/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/carlschader/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/carlschader/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/carlschader/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/carlschader/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/sw/q7k30xcj51x3tc06z506z3n00000gn/T/__autograph_generated_filet77404tk.py\", line 13, in tf__call\n        concat = ag__.converted_call(ag__.ld(keras).layers.Concatenate, (ag__.ld(input1), ag__.ld(hidden2)), None, fscope)\n\n    TypeError: Exception encountered when calling layer \"wide_and_deep_7\" \"                 f\"(type WideAndDeep).\n    \n    in user code:\n    \n        File \"/var/folders/sw/q7k30xcj51x3tc06z506z3n00000gn/T/ipykernel_42341/644067800.py\", line 13, in call  *\n            concat = keras.layers.Concatenate(input1, hidden2)\n    \n        TypeError: Concatenate.__init__() takes from 1 to 2 positional arguments but 3 were given\n    \n    \n    Call arguments received by layer \"wide_and_deep_7\" \"                 f\"(type WideAndDeep):\n      • inputs=('tf.Tensor(shape=(None, 4), dtype=float32)', 'tf.Tensor(shape=(None, 4), dtype=float32)')\n"
     ]
    }
   ],
   "source": [
    "# Not sure why this doesn't work\n",
    "\n",
    "class WideAndDeep(keras.Model):\n",
    "  def __init__(self, units=30, activation='relu', **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "    self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "    self.output1 = keras.layers.Dense(1, name='output1')\n",
    "    self.output2 = keras.layers.Dense(1, name='output2')\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    input1, input2 = inputs\n",
    "    hidden1 = self.hidden1(input2)\n",
    "    hidden2 = self.hidden2(hidden1)\n",
    "    concat = keras.layers.Concatenate(input1, hidden2)\n",
    "    output1 = keras.Dense(1)(concat)\n",
    "    output2 = keras.Dense(1)(hidden2)\n",
    "    return output1, output2\n",
    "\n",
    "model = WideAndDeep()\n",
    "model.compile(optimizer=keras.optimizers.SGD(0.01), loss=('mse', 'mse'), loss_weights=(0.75, 0.25))\n",
    "\n",
    "history = model.fit((X_train[:, :4], X_train[:, 4:]), (y_train, y_train), epochs=128, validation_data=((X_val[:, :4], X_val[:, 4:]), y_val))\n",
    "print(model.evaluate((X_test[:, :4], X_test[:, 4:], ), (y_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "363/363 [==============================] - 1s 859us/step - loss: 1.4756 - output1_loss: 1.2391 - output2_loss: 1.7121 - val_loss: 0.8177 - val_output1_loss: 0.5835 - val_output2_loss: 1.0518\n",
      "Epoch 2/128\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.7962 - output1_loss: 0.5720 - output2_loss: 1.0203 - val_loss: 0.6638 - val_output1_loss: 0.4546 - val_output2_loss: 0.8730\n",
      "Epoch 3/128\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.6816 - output1_loss: 0.4750 - output2_loss: 0.8883 - val_loss: 0.6199 - val_output1_loss: 0.4125 - val_output2_loss: 0.8273\n",
      "Epoch 4/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.6527 - output1_loss: 0.4488 - output2_loss: 0.8566 - val_loss: 0.6200 - val_output1_loss: 0.4067 - val_output2_loss: 0.8333\n",
      "Epoch 5/128\n",
      "363/363 [==============================] - 0s 580us/step - loss: 0.6413 - output1_loss: 0.4380 - output2_loss: 0.8447 - val_loss: 0.6028 - val_output1_loss: 0.3955 - val_output2_loss: 0.8101\n",
      "Epoch 6/128\n",
      "363/363 [==============================] - 0s 580us/step - loss: 0.6375 - output1_loss: 0.4370 - output2_loss: 0.8380 - val_loss: 0.5932 - val_output1_loss: 0.3906 - val_output2_loss: 0.7958\n",
      "Epoch 7/128\n",
      "363/363 [==============================] - 0s 585us/step - loss: 0.6249 - output1_loss: 0.4259 - output2_loss: 0.8239 - val_loss: 0.5803 - val_output1_loss: 0.3814 - val_output2_loss: 0.7792\n",
      "Epoch 8/128\n",
      "363/363 [==============================] - 0s 712us/step - loss: 0.6181 - output1_loss: 0.4207 - output2_loss: 0.8154 - val_loss: 0.5757 - val_output1_loss: 0.3788 - val_output2_loss: 0.7725\n",
      "Epoch 9/128\n",
      "363/363 [==============================] - 0s 602us/step - loss: 0.6116 - output1_loss: 0.4164 - output2_loss: 0.8068 - val_loss: 0.5717 - val_output1_loss: 0.3761 - val_output2_loss: 0.7673\n",
      "Epoch 10/128\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.6088 - output1_loss: 0.4156 - output2_loss: 0.8021 - val_loss: 0.5642 - val_output1_loss: 0.3697 - val_output2_loss: 0.7586\n",
      "Epoch 11/128\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.6031 - output1_loss: 0.4117 - output2_loss: 0.7944 - val_loss: 0.5709 - val_output1_loss: 0.3753 - val_output2_loss: 0.7665\n",
      "Epoch 12/128\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.5965 - output1_loss: 0.4064 - output2_loss: 0.7866 - val_loss: 0.5621 - val_output1_loss: 0.3671 - val_output2_loss: 0.7570\n",
      "Epoch 13/128\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.5940 - output1_loss: 0.4048 - output2_loss: 0.7831 - val_loss: 0.5559 - val_output1_loss: 0.3647 - val_output2_loss: 0.7471\n",
      "Epoch 14/128\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.5920 - output1_loss: 0.4035 - output2_loss: 0.7806 - val_loss: 0.5509 - val_output1_loss: 0.3608 - val_output2_loss: 0.7409\n",
      "Epoch 15/128\n",
      "363/363 [==============================] - 0s 627us/step - loss: 0.5883 - output1_loss: 0.4010 - output2_loss: 0.7757 - val_loss: 0.5491 - val_output1_loss: 0.3596 - val_output2_loss: 0.7386\n",
      "Epoch 16/128\n",
      "363/363 [==============================] - 0s 604us/step - loss: 0.5836 - output1_loss: 0.3978 - output2_loss: 0.7695 - val_loss: 0.5491 - val_output1_loss: 0.3587 - val_output2_loss: 0.7396\n",
      "Epoch 17/128\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.5781 - output1_loss: 0.3933 - output2_loss: 0.7629 - val_loss: 0.5622 - val_output1_loss: 0.3766 - val_output2_loss: 0.7479\n",
      "Epoch 18/128\n",
      "363/363 [==============================] - 0s 588us/step - loss: 0.5745 - output1_loss: 0.3909 - output2_loss: 0.7580 - val_loss: 0.5468 - val_output1_loss: 0.3591 - val_output2_loss: 0.7345\n",
      "Epoch 19/128\n",
      "363/363 [==============================] - 0s 583us/step - loss: 0.5755 - output1_loss: 0.3914 - output2_loss: 0.7596 - val_loss: 0.5393 - val_output1_loss: 0.3532 - val_output2_loss: 0.7254\n",
      "Epoch 20/128\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.5705 - output1_loss: 0.3878 - output2_loss: 0.7532 - val_loss: 0.5407 - val_output1_loss: 0.3545 - val_output2_loss: 0.7270\n",
      "Epoch 21/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5695 - output1_loss: 0.3871 - output2_loss: 0.7518 - val_loss: 0.5494 - val_output1_loss: 0.3571 - val_output2_loss: 0.7417\n",
      "Epoch 22/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5652 - output1_loss: 0.3838 - output2_loss: 0.7466 - val_loss: 0.5435 - val_output1_loss: 0.3569 - val_output2_loss: 0.7302\n",
      "Epoch 23/128\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.5670 - output1_loss: 0.3852 - output2_loss: 0.7488 - val_loss: 0.5349 - val_output1_loss: 0.3483 - val_output2_loss: 0.7214\n",
      "Epoch 24/128\n",
      "363/363 [==============================] - 0s 582us/step - loss: 0.5611 - output1_loss: 0.3800 - output2_loss: 0.7422 - val_loss: 0.5300 - val_output1_loss: 0.3447 - val_output2_loss: 0.7153\n",
      "Epoch 25/128\n",
      "363/363 [==============================] - 0s 606us/step - loss: 0.5598 - output1_loss: 0.3794 - output2_loss: 0.7403 - val_loss: 0.5587 - val_output1_loss: 0.3692 - val_output2_loss: 0.7482\n",
      "Epoch 26/128\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.5602 - output1_loss: 0.3796 - output2_loss: 0.7407 - val_loss: 0.5294 - val_output1_loss: 0.3452 - val_output2_loss: 0.7137\n",
      "Epoch 27/128\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.5551 - output1_loss: 0.3759 - output2_loss: 0.7344 - val_loss: 0.5275 - val_output1_loss: 0.3416 - val_output2_loss: 0.7133\n",
      "Epoch 28/128\n",
      "363/363 [==============================] - 0s 593us/step - loss: 0.5571 - output1_loss: 0.3768 - output2_loss: 0.7374 - val_loss: 0.5272 - val_output1_loss: 0.3433 - val_output2_loss: 0.7111\n",
      "Epoch 29/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.5552 - output1_loss: 0.3760 - output2_loss: 0.7345 - val_loss: 0.5487 - val_output1_loss: 0.3590 - val_output2_loss: 0.7383\n",
      "Epoch 30/128\n",
      "363/363 [==============================] - 0s 583us/step - loss: 0.5541 - output1_loss: 0.3748 - output2_loss: 0.7333 - val_loss: 0.5220 - val_output1_loss: 0.3398 - val_output2_loss: 0.7042\n",
      "Epoch 31/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.5527 - output1_loss: 0.3741 - output2_loss: 0.7314 - val_loss: 0.5363 - val_output1_loss: 0.3454 - val_output2_loss: 0.7271\n",
      "Epoch 32/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5489 - output1_loss: 0.3713 - output2_loss: 0.7265 - val_loss: 0.5403 - val_output1_loss: 0.3493 - val_output2_loss: 0.7313\n",
      "Epoch 33/128\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.5487 - output1_loss: 0.3712 - output2_loss: 0.7263 - val_loss: 0.5333 - val_output1_loss: 0.3448 - val_output2_loss: 0.7218\n",
      "Epoch 34/128\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.5482 - output1_loss: 0.3704 - output2_loss: 0.7260 - val_loss: 0.5180 - val_output1_loss: 0.3362 - val_output2_loss: 0.6998\n",
      "Epoch 35/128\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.5472 - output1_loss: 0.3694 - output2_loss: 0.7250 - val_loss: 0.5330 - val_output1_loss: 0.3452 - val_output2_loss: 0.7208\n",
      "Epoch 36/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.5458 - output1_loss: 0.3688 - output2_loss: 0.7228 - val_loss: 0.5318 - val_output1_loss: 0.3444 - val_output2_loss: 0.7191\n",
      "Epoch 37/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5472 - output1_loss: 0.3699 - output2_loss: 0.7245 - val_loss: 0.5202 - val_output1_loss: 0.3385 - val_output2_loss: 0.7019\n",
      "Epoch 38/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5444 - output1_loss: 0.3679 - output2_loss: 0.7209 - val_loss: 0.5194 - val_output1_loss: 0.3366 - val_output2_loss: 0.7022\n",
      "Epoch 39/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.5438 - output1_loss: 0.3669 - output2_loss: 0.7207 - val_loss: 0.5200 - val_output1_loss: 0.3360 - val_output2_loss: 0.7039\n",
      "Epoch 40/128\n",
      "363/363 [==============================] - 0s 600us/step - loss: 0.5428 - output1_loss: 0.3655 - output2_loss: 0.7200 - val_loss: 0.5134 - val_output1_loss: 0.3321 - val_output2_loss: 0.6947\n",
      "Epoch 41/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.5410 - output1_loss: 0.3639 - output2_loss: 0.7181 - val_loss: 0.5438 - val_output1_loss: 0.3553 - val_output2_loss: 0.7323\n",
      "Epoch 42/128\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.5427 - output1_loss: 0.3656 - output2_loss: 0.7198 - val_loss: 0.5124 - val_output1_loss: 0.3312 - val_output2_loss: 0.6937\n",
      "Epoch 43/128\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.5397 - output1_loss: 0.3627 - output2_loss: 0.7166 - val_loss: 0.5197 - val_output1_loss: 0.3361 - val_output2_loss: 0.7032\n",
      "Epoch 44/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5414 - output1_loss: 0.3658 - output2_loss: 0.7171 - val_loss: 0.5194 - val_output1_loss: 0.3368 - val_output2_loss: 0.7021\n",
      "Epoch 45/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.5400 - output1_loss: 0.3636 - output2_loss: 0.7164 - val_loss: 0.5190 - val_output1_loss: 0.3363 - val_output2_loss: 0.7018\n",
      "Epoch 46/128\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.5397 - output1_loss: 0.3637 - output2_loss: 0.7158 - val_loss: 0.5111 - val_output1_loss: 0.3320 - val_output2_loss: 0.6903\n",
      "Epoch 47/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5385 - output1_loss: 0.3624 - output2_loss: 0.7146 - val_loss: 0.5127 - val_output1_loss: 0.3332 - val_output2_loss: 0.6922\n",
      "Epoch 48/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5372 - output1_loss: 0.3624 - output2_loss: 0.7119 - val_loss: 0.5112 - val_output1_loss: 0.3294 - val_output2_loss: 0.6929\n",
      "Epoch 49/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5364 - output1_loss: 0.3614 - output2_loss: 0.7115 - val_loss: 0.5212 - val_output1_loss: 0.3379 - val_output2_loss: 0.7046\n",
      "Epoch 50/128\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.5361 - output1_loss: 0.3625 - output2_loss: 0.7096 - val_loss: 0.5121 - val_output1_loss: 0.3315 - val_output2_loss: 0.6927\n",
      "Epoch 51/128\n",
      "363/363 [==============================] - 0s 580us/step - loss: 0.5358 - output1_loss: 0.3601 - output2_loss: 0.7115 - val_loss: 0.5076 - val_output1_loss: 0.3279 - val_output2_loss: 0.6872\n",
      "Epoch 52/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5359 - output1_loss: 0.3608 - output2_loss: 0.7111 - val_loss: 0.5197 - val_output1_loss: 0.3377 - val_output2_loss: 0.7018\n",
      "Epoch 53/128\n",
      "363/363 [==============================] - 0s 583us/step - loss: 0.5339 - output1_loss: 0.3589 - output2_loss: 0.7090 - val_loss: 0.5316 - val_output1_loss: 0.3459 - val_output2_loss: 0.7173\n",
      "Epoch 54/128\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.5349 - output1_loss: 0.3615 - output2_loss: 0.7084 - val_loss: 0.5264 - val_output1_loss: 0.3419 - val_output2_loss: 0.7109\n",
      "Epoch 55/128\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.5347 - output1_loss: 0.3612 - output2_loss: 0.7082 - val_loss: 0.5133 - val_output1_loss: 0.3353 - val_output2_loss: 0.6912\n",
      "Epoch 56/128\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.5333 - output1_loss: 0.3595 - output2_loss: 0.7072 - val_loss: 0.5075 - val_output1_loss: 0.3301 - val_output2_loss: 0.6849\n",
      "Epoch 57/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5329 - output1_loss: 0.3593 - output2_loss: 0.7066 - val_loss: 0.5104 - val_output1_loss: 0.3321 - val_output2_loss: 0.6887\n",
      "Epoch 58/128\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.5315 - output1_loss: 0.3584 - output2_loss: 0.7046 - val_loss: 0.5052 - val_output1_loss: 0.3272 - val_output2_loss: 0.6832\n",
      "Epoch 59/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5327 - output1_loss: 0.3598 - output2_loss: 0.7057 - val_loss: 0.5114 - val_output1_loss: 0.3311 - val_output2_loss: 0.6917\n",
      "Epoch 60/128\n",
      "363/363 [==============================] - 0s 560us/step - loss: 0.5333 - output1_loss: 0.3603 - output2_loss: 0.7062 - val_loss: 0.5060 - val_output1_loss: 0.3288 - val_output2_loss: 0.6832\n",
      "Epoch 61/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5307 - output1_loss: 0.3561 - output2_loss: 0.7052 - val_loss: 0.5073 - val_output1_loss: 0.3315 - val_output2_loss: 0.6830\n",
      "Epoch 62/128\n",
      "363/363 [==============================] - 0s 580us/step - loss: 0.5305 - output1_loss: 0.3581 - output2_loss: 0.7030 - val_loss: 0.5142 - val_output1_loss: 0.3329 - val_output2_loss: 0.6956\n",
      "Epoch 63/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5302 - output1_loss: 0.3573 - output2_loss: 0.7032 - val_loss: 0.5169 - val_output1_loss: 0.3388 - val_output2_loss: 0.6949\n",
      "Epoch 64/128\n",
      "363/363 [==============================] - 0s 587us/step - loss: 0.5292 - output1_loss: 0.3563 - output2_loss: 0.7020 - val_loss: 0.5212 - val_output1_loss: 0.3422 - val_output2_loss: 0.7002\n",
      "Epoch 65/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5274 - output1_loss: 0.3560 - output2_loss: 0.6989 - val_loss: 0.5102 - val_output1_loss: 0.3319 - val_output2_loss: 0.6884\n",
      "Epoch 66/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.5277 - output1_loss: 0.3557 - output2_loss: 0.6997 - val_loss: 0.5108 - val_output1_loss: 0.3317 - val_output2_loss: 0.6900\n",
      "Epoch 67/128\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.5262 - output1_loss: 0.3551 - output2_loss: 0.6972 - val_loss: 0.5038 - val_output1_loss: 0.3279 - val_output2_loss: 0.6797\n",
      "Epoch 68/128\n",
      "363/363 [==============================] - 0s 627us/step - loss: 0.5269 - output1_loss: 0.3551 - output2_loss: 0.6988 - val_loss: 0.4999 - val_output1_loss: 0.3255 - val_output2_loss: 0.6743\n",
      "Epoch 69/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5271 - output1_loss: 0.3554 - output2_loss: 0.6988 - val_loss: 0.5087 - val_output1_loss: 0.3320 - val_output2_loss: 0.6854\n",
      "Epoch 70/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5247 - output1_loss: 0.3539 - output2_loss: 0.6955 - val_loss: 0.5051 - val_output1_loss: 0.3293 - val_output2_loss: 0.6809\n",
      "Epoch 71/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.5242 - output1_loss: 0.3542 - output2_loss: 0.6942 - val_loss: 0.5021 - val_output1_loss: 0.3272 - val_output2_loss: 0.6771\n",
      "Epoch 72/128\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.5233 - output1_loss: 0.3540 - output2_loss: 0.6925 - val_loss: 0.4984 - val_output1_loss: 0.3250 - val_output2_loss: 0.6718\n",
      "Epoch 73/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5242 - output1_loss: 0.3542 - output2_loss: 0.6942 - val_loss: 0.5040 - val_output1_loss: 0.3286 - val_output2_loss: 0.6794\n",
      "Epoch 74/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5236 - output1_loss: 0.3533 - output2_loss: 0.6940 - val_loss: 0.5029 - val_output1_loss: 0.3294 - val_output2_loss: 0.6764\n",
      "Epoch 75/128\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.5217 - output1_loss: 0.3521 - output2_loss: 0.6913 - val_loss: 0.4955 - val_output1_loss: 0.3234 - val_output2_loss: 0.6676\n",
      "Epoch 76/128\n",
      "363/363 [==============================] - 0s 560us/step - loss: 0.5211 - output1_loss: 0.3517 - output2_loss: 0.6906 - val_loss: 0.4992 - val_output1_loss: 0.3252 - val_output2_loss: 0.6733\n",
      "Epoch 77/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5206 - output1_loss: 0.3506 - output2_loss: 0.6906 - val_loss: 0.5087 - val_output1_loss: 0.3360 - val_output2_loss: 0.6814\n",
      "Epoch 78/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.5199 - output1_loss: 0.3506 - output2_loss: 0.6892 - val_loss: 0.5083 - val_output1_loss: 0.3323 - val_output2_loss: 0.6843\n",
      "Epoch 79/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5199 - output1_loss: 0.3508 - output2_loss: 0.6890 - val_loss: 0.4960 - val_output1_loss: 0.3243 - val_output2_loss: 0.6676\n",
      "Epoch 80/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.5184 - output1_loss: 0.3503 - output2_loss: 0.6865 - val_loss: 0.5241 - val_output1_loss: 0.3497 - val_output2_loss: 0.6984\n",
      "Epoch 81/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.5193 - output1_loss: 0.3498 - output2_loss: 0.6888 - val_loss: 0.5270 - val_output1_loss: 0.3443 - val_output2_loss: 0.7097\n",
      "Epoch 82/128\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.5188 - output1_loss: 0.3498 - output2_loss: 0.6877 - val_loss: 0.5080 - val_output1_loss: 0.3297 - val_output2_loss: 0.6864\n",
      "Epoch 83/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5185 - output1_loss: 0.3497 - output2_loss: 0.6872 - val_loss: 0.4963 - val_output1_loss: 0.3234 - val_output2_loss: 0.6692\n",
      "Epoch 84/128\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.5171 - output1_loss: 0.3482 - output2_loss: 0.6861 - val_loss: 0.4914 - val_output1_loss: 0.3203 - val_output2_loss: 0.6625\n",
      "Epoch 85/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5157 - output1_loss: 0.3480 - output2_loss: 0.6834 - val_loss: 0.4943 - val_output1_loss: 0.3235 - val_output2_loss: 0.6652\n",
      "Epoch 86/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.5179 - output1_loss: 0.3506 - output2_loss: 0.6852 - val_loss: 0.5022 - val_output1_loss: 0.3247 - val_output2_loss: 0.6798\n",
      "Epoch 87/128\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.5168 - output1_loss: 0.3481 - output2_loss: 0.6855 - val_loss: 0.4992 - val_output1_loss: 0.3244 - val_output2_loss: 0.6740\n",
      "Epoch 88/128\n",
      "363/363 [==============================] - 0s 627us/step - loss: 0.5155 - output1_loss: 0.3480 - output2_loss: 0.6831 - val_loss: 0.4932 - val_output1_loss: 0.3219 - val_output2_loss: 0.6645\n",
      "Epoch 89/128\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.5142 - output1_loss: 0.3470 - output2_loss: 0.6813 - val_loss: 0.5075 - val_output1_loss: 0.3345 - val_output2_loss: 0.6806\n",
      "Epoch 90/128\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.5138 - output1_loss: 0.3468 - output2_loss: 0.6808 - val_loss: 0.5011 - val_output1_loss: 0.3286 - val_output2_loss: 0.6736\n",
      "Epoch 91/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.5132 - output1_loss: 0.3472 - output2_loss: 0.6792 - val_loss: 0.4912 - val_output1_loss: 0.3212 - val_output2_loss: 0.6612\n",
      "Epoch 92/128\n",
      "363/363 [==============================] - 0s 593us/step - loss: 0.5135 - output1_loss: 0.3469 - output2_loss: 0.6802 - val_loss: 0.4918 - val_output1_loss: 0.3220 - val_output2_loss: 0.6615\n",
      "Epoch 93/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5124 - output1_loss: 0.3460 - output2_loss: 0.6788 - val_loss: 0.5051 - val_output1_loss: 0.3313 - val_output2_loss: 0.6788\n",
      "Epoch 94/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5136 - output1_loss: 0.3474 - output2_loss: 0.6797 - val_loss: 0.4966 - val_output1_loss: 0.3277 - val_output2_loss: 0.6655\n",
      "Epoch 95/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5106 - output1_loss: 0.3448 - output2_loss: 0.6765 - val_loss: 0.4940 - val_output1_loss: 0.3200 - val_output2_loss: 0.6679\n",
      "Epoch 96/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5102 - output1_loss: 0.3445 - output2_loss: 0.6758 - val_loss: 0.4972 - val_output1_loss: 0.3243 - val_output2_loss: 0.6701\n",
      "Epoch 97/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5115 - output1_loss: 0.3452 - output2_loss: 0.6777 - val_loss: 0.4988 - val_output1_loss: 0.3279 - val_output2_loss: 0.6697\n",
      "Epoch 98/128\n",
      "363/363 [==============================] - 0s 585us/step - loss: 0.5098 - output1_loss: 0.3444 - output2_loss: 0.6752 - val_loss: 0.4894 - val_output1_loss: 0.3199 - val_output2_loss: 0.6590\n",
      "Epoch 99/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5093 - output1_loss: 0.3435 - output2_loss: 0.6750 - val_loss: 0.4900 - val_output1_loss: 0.3207 - val_output2_loss: 0.6592\n",
      "Epoch 100/128\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.5093 - output1_loss: 0.3436 - output2_loss: 0.6750 - val_loss: 0.4913 - val_output1_loss: 0.3223 - val_output2_loss: 0.6602\n",
      "Epoch 101/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5089 - output1_loss: 0.3432 - output2_loss: 0.6746 - val_loss: 0.4936 - val_output1_loss: 0.3228 - val_output2_loss: 0.6644\n",
      "Epoch 102/128\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.5099 - output1_loss: 0.3442 - output2_loss: 0.6756 - val_loss: 0.4856 - val_output1_loss: 0.3171 - val_output2_loss: 0.6541\n",
      "Epoch 103/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5091 - output1_loss: 0.3436 - output2_loss: 0.6747 - val_loss: 0.4966 - val_output1_loss: 0.3273 - val_output2_loss: 0.6658\n",
      "Epoch 104/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5076 - output1_loss: 0.3425 - output2_loss: 0.6727 - val_loss: 0.4949 - val_output1_loss: 0.3214 - val_output2_loss: 0.6683\n",
      "Epoch 105/128\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.5081 - output1_loss: 0.3435 - output2_loss: 0.6726 - val_loss: 0.4977 - val_output1_loss: 0.3223 - val_output2_loss: 0.6730\n",
      "Epoch 106/128\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.5075 - output1_loss: 0.3432 - output2_loss: 0.6719 - val_loss: 0.4848 - val_output1_loss: 0.3173 - val_output2_loss: 0.6523\n",
      "Epoch 107/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.5064 - output1_loss: 0.3420 - output2_loss: 0.6708 - val_loss: 0.4991 - val_output1_loss: 0.3275 - val_output2_loss: 0.6707\n",
      "Epoch 108/128\n",
      "363/363 [==============================] - 0s 582us/step - loss: 0.5078 - output1_loss: 0.3432 - output2_loss: 0.6724 - val_loss: 0.4836 - val_output1_loss: 0.3164 - val_output2_loss: 0.6508\n",
      "Epoch 109/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5060 - output1_loss: 0.3411 - output2_loss: 0.6709 - val_loss: 0.4857 - val_output1_loss: 0.3183 - val_output2_loss: 0.6532\n",
      "Epoch 110/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.5053 - output1_loss: 0.3417 - output2_loss: 0.6690 - val_loss: 0.4859 - val_output1_loss: 0.3176 - val_output2_loss: 0.6543\n",
      "Epoch 111/128\n",
      "363/363 [==============================] - 0s 585us/step - loss: 0.5041 - output1_loss: 0.3406 - output2_loss: 0.6676 - val_loss: 0.4846 - val_output1_loss: 0.3179 - val_output2_loss: 0.6514\n",
      "Epoch 112/128\n",
      "363/363 [==============================] - 0s 599us/step - loss: 0.5049 - output1_loss: 0.3413 - output2_loss: 0.6685 - val_loss: 0.4910 - val_output1_loss: 0.3214 - val_output2_loss: 0.6607\n",
      "Epoch 113/128\n",
      "363/363 [==============================] - 0s 597us/step - loss: 0.5052 - output1_loss: 0.3414 - output2_loss: 0.6689 - val_loss: 0.4808 - val_output1_loss: 0.3146 - val_output2_loss: 0.6471\n",
      "Epoch 114/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.5031 - output1_loss: 0.3393 - output2_loss: 0.6669 - val_loss: 0.4883 - val_output1_loss: 0.3205 - val_output2_loss: 0.6562\n",
      "Epoch 115/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.5044 - output1_loss: 0.3405 - output2_loss: 0.6684 - val_loss: 0.4825 - val_output1_loss: 0.3181 - val_output2_loss: 0.6468\n",
      "Epoch 116/128\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.5040 - output1_loss: 0.3414 - output2_loss: 0.6666 - val_loss: 0.4860 - val_output1_loss: 0.3165 - val_output2_loss: 0.6555\n",
      "Epoch 117/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.5016 - output1_loss: 0.3392 - output2_loss: 0.6641 - val_loss: 0.4819 - val_output1_loss: 0.3150 - val_output2_loss: 0.6487\n",
      "Epoch 118/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.5025 - output1_loss: 0.3397 - output2_loss: 0.6653 - val_loss: 0.4808 - val_output1_loss: 0.3150 - val_output2_loss: 0.6467\n",
      "Epoch 119/128\n",
      "363/363 [==============================] - 0s 626us/step - loss: 0.5032 - output1_loss: 0.3398 - output2_loss: 0.6666 - val_loss: 0.4849 - val_output1_loss: 0.3194 - val_output2_loss: 0.6505\n",
      "Epoch 120/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.5020 - output1_loss: 0.3403 - output2_loss: 0.6637 - val_loss: 0.4822 - val_output1_loss: 0.3168 - val_output2_loss: 0.6477\n",
      "Epoch 121/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.5033 - output1_loss: 0.3411 - output2_loss: 0.6655 - val_loss: 0.5033 - val_output1_loss: 0.3348 - val_output2_loss: 0.6718\n",
      "Epoch 122/128\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.5021 - output1_loss: 0.3399 - output2_loss: 0.6644 - val_loss: 0.4778 - val_output1_loss: 0.3136 - val_output2_loss: 0.6419\n",
      "Epoch 123/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.5013 - output1_loss: 0.3399 - output2_loss: 0.6627 - val_loss: 0.4987 - val_output1_loss: 0.3272 - val_output2_loss: 0.6703\n",
      "Epoch 124/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.5018 - output1_loss: 0.3402 - output2_loss: 0.6635 - val_loss: 0.4816 - val_output1_loss: 0.3154 - val_output2_loss: 0.6478\n",
      "Epoch 125/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4998 - output1_loss: 0.3390 - output2_loss: 0.6606 - val_loss: 0.4775 - val_output1_loss: 0.3134 - val_output2_loss: 0.6416\n",
      "Epoch 126/128\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.4999 - output1_loss: 0.3391 - output2_loss: 0.6607 - val_loss: 0.4806 - val_output1_loss: 0.3162 - val_output2_loss: 0.6451\n",
      "Epoch 127/128\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.4989 - output1_loss: 0.3381 - output2_loss: 0.6597 - val_loss: 0.4769 - val_output1_loss: 0.3123 - val_output2_loss: 0.6415\n",
      "Epoch 128/128\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.4986 - output1_loss: 0.3378 - output2_loss: 0.6595 - val_loss: 0.4761 - val_output1_loss: 0.3130 - val_output2_loss: 0.6392\n",
      "162/162 [==============================] - 0s 442us/step - loss: 0.4989 - output1_loss: 0.3385 - output2_loss: 0.6594\n",
      "[0.4989345669746399, 0.3384840190410614, 0.6593848466873169]\n"
     ]
    }
   ],
   "source": [
    "input_wide = keras.layers.Input(shape=(4), name='input_wide')\n",
    "input_deep = keras.layers.Input(shape=(4), name='input_deep')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_deep)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_wide, hidden2])\n",
    "output1 = keras.layers.Dense(1, name='output1')(concat)\n",
    "output2 = keras.layers.Dense(1, name='output2')(hidden2)\n",
    "model = keras.Model(inputs=[input_wide, input_deep], outputs=[output1, output2])\n",
    "\n",
    "model.compile(\n",
    "  loss=[keras.losses.mean_squared_error, keras.losses.mean_squared_error],\n",
    "  loss_weights=(0.5, 0.5),\n",
    "  optimizer=keras.optimizers.SGD(0.01),\n",
    ")\n",
    "\n",
    "save_on_epoch_cb = keras.callbacks.ModelCheckpoint('wide_and_deep.h5', save_best_only=True)\n",
    "\n",
    "history = model.fit((X_train[:, :4], X_train[:, 4:]), (y_train, y_train), epochs=128, validation_data=((X_val[:, :4], X_val[:, 4:]), y_val), callbacks=(save_on_epoch_cb))\n",
    "print(model.evaluate((X_test[:, :4], X_test[:, 4:], ), (y_test, y_test)))\n",
    "\n",
    "best_model = keras.models.load_model('wide_and_deep.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "320/363 [=========================>....] - ETA: 0s - loss: 0.4668 - output1_loss: 0.3246 - output2_loss: 0.6090\n",
      "val/train: 0.974887\n",
      "363/363 [==============================] - 0s 683us/step - loss: 0.4620 - output1_loss: 0.3199 - output2_loss: 0.6041 - val_loss: 0.4504 - val_output1_loss: 0.3006 - val_output2_loss: 0.6003\n",
      "Epoch 2/256\n",
      "348/363 [===========================>..] - ETA: 0s - loss: 0.4651 - output1_loss: 0.3221 - output2_loss: 0.6082\n",
      "val/train: 0.968711\n",
      "363/363 [==============================] - 0s 585us/step - loss: 0.4634 - output1_loss: 0.3207 - output2_loss: 0.6061 - val_loss: 0.4489 - val_output1_loss: 0.3006 - val_output2_loss: 0.5971\n",
      "Epoch 3/256\n",
      "338/363 [==========================>...] - ETA: 0s - loss: 0.4613 - output1_loss: 0.3197 - output2_loss: 0.6029\n",
      "val/train: 0.959714\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.4616 - output1_loss: 0.3198 - output2_loss: 0.6033 - val_loss: 0.4430 - val_output1_loss: 0.2979 - val_output2_loss: 0.5881\n",
      "Epoch 4/256\n",
      "275/363 [=====================>........] - ETA: 0s - loss: 0.4563 - output1_loss: 0.3176 - output2_loss: 0.5950\n",
      "val/train: 1.037540\n",
      "363/363 [==============================] - 0s 680us/step - loss: 0.4603 - output1_loss: 0.3189 - output2_loss: 0.6016 - val_loss: 0.4775 - val_output1_loss: 0.3187 - val_output2_loss: 0.6364\n",
      "Epoch 5/256\n",
      "341/363 [===========================>..] - ETA: 0s - loss: 0.4623 - output1_loss: 0.3199 - output2_loss: 0.6046\n",
      "val/train: 0.972824\n",
      "363/363 [==============================] - 0s 596us/step - loss: 0.4618 - output1_loss: 0.3192 - output2_loss: 0.6043 - val_loss: 0.4492 - val_output1_loss: 0.3017 - val_output2_loss: 0.5967\n",
      "Epoch 6/256\n",
      "342/363 [===========================>..] - ETA: 0s - loss: 0.4581 - output1_loss: 0.3156 - output2_loss: 0.6005\n",
      "val/train: 0.981986\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.4596 - output1_loss: 0.3176 - output2_loss: 0.6016 - val_loss: 0.4514 - val_output1_loss: 0.3056 - val_output2_loss: 0.5971\n",
      "Epoch 7/256\n",
      "344/363 [===========================>..] - ETA: 0s - loss: 0.4628 - output1_loss: 0.3205 - output2_loss: 0.6050\n",
      "val/train: 0.972358\n",
      "363/363 [==============================] - 0s 588us/step - loss: 0.4601 - output1_loss: 0.3192 - output2_loss: 0.6011 - val_loss: 0.4474 - val_output1_loss: 0.2995 - val_output2_loss: 0.5954\n",
      "Epoch 8/256\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.4620 - output1_loss: 0.3203 - output2_loss: 0.6038\n",
      "val/train: 0.973997\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.4605 - output1_loss: 0.3181 - output2_loss: 0.6030 - val_loss: 0.4486 - val_output1_loss: 0.3030 - val_output2_loss: 0.5942\n",
      "Epoch 9/256\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.4594 - output1_loss: 0.3175 - output2_loss: 0.6013\n",
      "val/train: 0.953626\n",
      "363/363 [==============================] - 0s 597us/step - loss: 0.4614 - output1_loss: 0.3198 - output2_loss: 0.6029 - val_loss: 0.4400 - val_output1_loss: 0.2987 - val_output2_loss: 0.5812\n",
      "Epoch 10/256\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.4651 - output1_loss: 0.3230 - output2_loss: 0.6072\n",
      "val/train: 0.955046\n",
      "363/363 [==============================] - 0s 639us/step - loss: 0.4616 - output1_loss: 0.3197 - output2_loss: 0.6034 - val_loss: 0.4408 - val_output1_loss: 0.2993 - val_output2_loss: 0.5823\n",
      "Epoch 11/256\n",
      "345/363 [===========================>..] - ETA: 0s - loss: 0.4680 - output1_loss: 0.3249 - output2_loss: 0.6111\n",
      "val/train: 0.944556\n",
      "363/363 [==============================] - 0s 588us/step - loss: 0.4672 - output1_loss: 0.3245 - output2_loss: 0.6099 - val_loss: 0.4413 - val_output1_loss: 0.2968 - val_output2_loss: 0.5858\n",
      "Epoch 12/256\n",
      "311/363 [========================>.....] - ETA: 0s - loss: 0.4647 - output1_loss: 0.3210 - output2_loss: 0.6084\n",
      "val/train: 0.963130\n",
      "363/363 [==============================] - 0s 633us/step - loss: 0.4633 - output1_loss: 0.3225 - output2_loss: 0.6040 - val_loss: 0.4462 - val_output1_loss: 0.3040 - val_output2_loss: 0.5883\n",
      "Epoch 13/256\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.4618 - output1_loss: 0.3213 - output2_loss: 0.6023\n",
      "val/train: 0.954424\n",
      "363/363 [==============================] - 0s 611us/step - loss: 0.4603 - output1_loss: 0.3195 - output2_loss: 0.6010 - val_loss: 0.4393 - val_output1_loss: 0.2963 - val_output2_loss: 0.5823\n",
      "Epoch 14/256\n",
      "318/363 [=========================>....] - ETA: 0s - loss: 0.4648 - output1_loss: 0.3234 - output2_loss: 0.6063\n",
      "val/train: 0.965622\n",
      "363/363 [==============================] - 0s 629us/step - loss: 0.4611 - output1_loss: 0.3198 - output2_loss: 0.6024 - val_loss: 0.4453 - val_output1_loss: 0.3002 - val_output2_loss: 0.5903\n",
      "Epoch 15/256\n",
      "345/363 [===========================>..] - ETA: 0s - loss: 0.4584 - output1_loss: 0.3174 - output2_loss: 0.5995\n",
      "val/train: 0.988939\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.4589 - output1_loss: 0.3187 - output2_loss: 0.5991 - val_loss: 0.4538 - val_output1_loss: 0.3023 - val_output2_loss: 0.6053\n",
      "Epoch 16/256\n",
      "339/363 [===========================>..] - ETA: 0s - loss: 0.4574 - output1_loss: 0.3153 - output2_loss: 0.5995\n",
      "val/train: 0.959060\n",
      "363/363 [==============================] - 0s 614us/step - loss: 0.4590 - output1_loss: 0.3173 - output2_loss: 0.6008 - val_loss: 0.4403 - val_output1_loss: 0.2988 - val_output2_loss: 0.5817\n",
      "Epoch 17/256\n",
      "319/363 [=========================>....] - ETA: 0s - loss: 0.4587 - output1_loss: 0.3202 - output2_loss: 0.5972\n",
      "val/train: 0.970095\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.4575 - output1_loss: 0.3167 - output2_loss: 0.5983 - val_loss: 0.4438 - val_output1_loss: 0.3007 - val_output2_loss: 0.5869\n",
      "Epoch 18/256\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4567 - output1_loss: 0.3154 - output2_loss: 0.5981\n",
      "val/train: 0.977110\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.4576 - output1_loss: 0.3166 - output2_loss: 0.5985 - val_loss: 0.4471 - val_output1_loss: 0.3031 - val_output2_loss: 0.5911\n",
      "Epoch 19/256\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.4607 - output1_loss: 0.3203 - output2_loss: 0.6012\n",
      "val/train: 0.958523\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.4579 - output1_loss: 0.3178 - output2_loss: 0.5980 - val_loss: 0.4389 - val_output1_loss: 0.2968 - val_output2_loss: 0.5809\n",
      "Epoch 20/256\n",
      "288/363 [======================>.......] - ETA: 0s - loss: 0.4605 - output1_loss: 0.3175 - output2_loss: 0.6034\n",
      "val/train: 0.968654\n",
      "363/363 [==============================] - 0s 662us/step - loss: 0.4590 - output1_loss: 0.3179 - output2_loss: 0.6001 - val_loss: 0.4446 - val_output1_loss: 0.2987 - val_output2_loss: 0.5905\n",
      "Epoch 21/256\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4578 - output1_loss: 0.3180 - output2_loss: 0.5976\n",
      "val/train: 0.975902\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4578 - output1_loss: 0.3176 - output2_loss: 0.5981 - val_loss: 0.4468 - val_output1_loss: 0.2994 - val_output2_loss: 0.5942\n",
      "Epoch 22/256\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4563 - output1_loss: 0.3148 - output2_loss: 0.5978\n",
      "val/train: 0.992777\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.4589 - output1_loss: 0.3186 - output2_loss: 0.5993 - val_loss: 0.4556 - val_output1_loss: 0.3092 - val_output2_loss: 0.6020\n",
      "Epoch 23/256\n",
      "308/363 [========================>.....] - ETA: 0s - loss: 0.4531 - output1_loss: 0.3128 - output2_loss: 0.5934\n",
      "val/train: 0.984604\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.4579 - output1_loss: 0.3182 - output2_loss: 0.5975 - val_loss: 0.4508 - val_output1_loss: 0.3014 - val_output2_loss: 0.6002\n",
      "Epoch 24/256\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4601 - output1_loss: 0.3188 - output2_loss: 0.6014\n",
      "val/train: 0.958441\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.4594 - output1_loss: 0.3182 - output2_loss: 0.6006 - val_loss: 0.4403 - val_output1_loss: 0.2976 - val_output2_loss: 0.5831\n",
      "Epoch 25/256\n",
      "344/363 [===========================>..] - ETA: 0s - loss: 0.4562 - output1_loss: 0.3157 - output2_loss: 0.5968\n",
      "val/train: 0.975338\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.4581 - output1_loss: 0.3171 - output2_loss: 0.5991 - val_loss: 0.4468 - val_output1_loss: 0.3022 - val_output2_loss: 0.5913\n",
      "Epoch 26/256\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.4567 - output1_loss: 0.3153 - output2_loss: 0.5981\n",
      "val/train: 0.982120\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.4572 - output1_loss: 0.3161 - output2_loss: 0.5983 - val_loss: 0.4490 - val_output1_loss: 0.3019 - val_output2_loss: 0.5961\n",
      "Epoch 27/256\n",
      "337/363 [==========================>...] - ETA: 0s - loss: 0.4613 - output1_loss: 0.3219 - output2_loss: 0.6007\n",
      "val/train: 0.968051\n",
      "363/363 [==============================] - 0s 593us/step - loss: 0.4574 - output1_loss: 0.3183 - output2_loss: 0.5966 - val_loss: 0.4428 - val_output1_loss: 0.3002 - val_output2_loss: 0.5855\n",
      "Epoch 28/256\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.4559 - output1_loss: 0.3166 - output2_loss: 0.5953\n",
      "val/train: 0.969399\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.4556 - output1_loss: 0.3167 - output2_loss: 0.5946 - val_loss: 0.4417 - val_output1_loss: 0.2983 - val_output2_loss: 0.5851\n",
      "Epoch 29/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4573 - output1_loss: 0.3187 - output2_loss: 0.5959\n",
      "val/train: 0.964546\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4572 - output1_loss: 0.3177 - output2_loss: 0.5966 - val_loss: 0.4409 - val_output1_loss: 0.2972 - val_output2_loss: 0.5847\n",
      "Epoch 30/256\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.4536 - output1_loss: 0.3150 - output2_loss: 0.5923\n",
      "val/train: 0.979059\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.4543 - output1_loss: 0.3154 - output2_loss: 0.5932 - val_loss: 0.4448 - val_output1_loss: 0.2970 - val_output2_loss: 0.5926\n",
      "Epoch 31/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4561 - output1_loss: 0.3169 - output2_loss: 0.5952\n",
      "val/train: 0.997934\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.4545 - output1_loss: 0.3160 - output2_loss: 0.5930 - val_loss: 0.4535 - val_output1_loss: 0.3135 - val_output2_loss: 0.5936\n",
      "Epoch 32/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4575 - output1_loss: 0.3185 - output2_loss: 0.5966\n",
      "val/train: 0.971216\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4572 - output1_loss: 0.3179 - output2_loss: 0.5964 - val_loss: 0.4440 - val_output1_loss: 0.3003 - val_output2_loss: 0.5877\n",
      "Epoch 33/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4584 - output1_loss: 0.3182 - output2_loss: 0.5987\n",
      "val/train: 0.961878\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4568 - output1_loss: 0.3170 - output2_loss: 0.5966 - val_loss: 0.4394 - val_output1_loss: 0.2954 - val_output2_loss: 0.5834\n",
      "Epoch 34/256\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4563 - output1_loss: 0.3169 - output2_loss: 0.5957\n",
      "val/train: 0.983971\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.4550 - output1_loss: 0.3156 - output2_loss: 0.5944 - val_loss: 0.4477 - val_output1_loss: 0.3006 - val_output2_loss: 0.5949\n",
      "Epoch 35/256\n",
      "347/363 [===========================>..] - ETA: 0s - loss: 0.4534 - output1_loss: 0.3148 - output2_loss: 0.5919\n",
      "val/train: 0.970398\n",
      "363/363 [==============================] - 0s 583us/step - loss: 0.4567 - output1_loss: 0.3180 - output2_loss: 0.5953 - val_loss: 0.4431 - val_output1_loss: 0.2990 - val_output2_loss: 0.5873\n",
      "Epoch 36/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4571 - output1_loss: 0.3177 - output2_loss: 0.5965\n",
      "val/train: 0.987914\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.4567 - output1_loss: 0.3174 - output2_loss: 0.5960 - val_loss: 0.4512 - val_output1_loss: 0.3025 - val_output2_loss: 0.5998\n",
      "Epoch 37/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4530 - output1_loss: 0.3148 - output2_loss: 0.5912\n",
      "val/train: 0.991963\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4550 - output1_loss: 0.3161 - output2_loss: 0.5939 - val_loss: 0.4513 - val_output1_loss: 0.3031 - val_output2_loss: 0.5996\n",
      "Epoch 38/256\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.4544 - output1_loss: 0.3166 - output2_loss: 0.5921\n",
      "val/train: 0.962451\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.4557 - output1_loss: 0.3174 - output2_loss: 0.5940 - val_loss: 0.4386 - val_output1_loss: 0.2956 - val_output2_loss: 0.5816\n",
      "Epoch 39/256\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4549 - output1_loss: 0.3155 - output2_loss: 0.5942\n",
      "val/train: 0.982155\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.4550 - output1_loss: 0.3164 - output2_loss: 0.5937 - val_loss: 0.4469 - val_output1_loss: 0.3013 - val_output2_loss: 0.5925\n",
      "Epoch 40/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4507 - output1_loss: 0.3136 - output2_loss: 0.5879\n",
      "val/train: 0.990399\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.4531 - output1_loss: 0.3144 - output2_loss: 0.5917 - val_loss: 0.4487 - val_output1_loss: 0.2994 - val_output2_loss: 0.5980\n",
      "Epoch 41/256\n",
      "316/363 [=========================>....] - ETA: 0s - loss: 0.4585 - output1_loss: 0.3210 - output2_loss: 0.5959\n",
      "val/train: 0.974709\n",
      "363/363 [==============================] - 0s 622us/step - loss: 0.4550 - output1_loss: 0.3163 - output2_loss: 0.5936 - val_loss: 0.4435 - val_output1_loss: 0.2996 - val_output2_loss: 0.5874\n",
      "Epoch 42/256\n",
      "344/363 [===========================>..] - ETA: 0s - loss: 0.4576 - output1_loss: 0.3171 - output2_loss: 0.5981\n",
      "val/train: 0.979294\n",
      "363/363 [==============================] - 0s 588us/step - loss: 0.4540 - output1_loss: 0.3155 - output2_loss: 0.5926 - val_loss: 0.4446 - val_output1_loss: 0.3008 - val_output2_loss: 0.5885\n",
      "Epoch 43/256\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4534 - output1_loss: 0.3154 - output2_loss: 0.5914\n",
      "val/train: 0.987199\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.4549 - output1_loss: 0.3160 - output2_loss: 0.5938 - val_loss: 0.4491 - val_output1_loss: 0.3001 - val_output2_loss: 0.5980\n",
      "Epoch 44/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4535 - output1_loss: 0.3153 - output2_loss: 0.5916\n",
      "val/train: 0.980559\n",
      "363/363 [==============================] - 0s 591us/step - loss: 0.4533 - output1_loss: 0.3152 - output2_loss: 0.5913 - val_loss: 0.4444 - val_output1_loss: 0.3013 - val_output2_loss: 0.5876\n",
      "Epoch 45/256\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.4693 - output1_loss: 0.3252 - output2_loss: 0.6134\n",
      "val/train: 0.941851\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4659 - output1_loss: 0.3230 - output2_loss: 0.6087 - val_loss: 0.4388 - val_output1_loss: 0.2959 - val_output2_loss: 0.5816\n",
      "Epoch 46/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4555 - output1_loss: 0.3160 - output2_loss: 0.5950\n",
      "val/train: 0.975706\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4554 - output1_loss: 0.3155 - output2_loss: 0.5953 - val_loss: 0.4443 - val_output1_loss: 0.2996 - val_output2_loss: 0.5891\n",
      "Epoch 47/256\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.4528 - output1_loss: 0.3144 - output2_loss: 0.5913\n",
      "val/train: 0.991207\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.4533 - output1_loss: 0.3154 - output2_loss: 0.5913 - val_loss: 0.4493 - val_output1_loss: 0.3009 - val_output2_loss: 0.5978\n",
      "Epoch 48/256\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.4571 - output1_loss: 0.3176 - output2_loss: 0.5966\n",
      "val/train: 0.973981\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.4540 - output1_loss: 0.3151 - output2_loss: 0.5930 - val_loss: 0.4422 - val_output1_loss: 0.2978 - val_output2_loss: 0.5867\n",
      "Epoch 49/256\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4563 - output1_loss: 0.3170 - output2_loss: 0.5955\n",
      "val/train: 0.999538\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4555 - output1_loss: 0.3171 - output2_loss: 0.5938 - val_loss: 0.4552 - val_output1_loss: 0.3063 - val_output2_loss: 0.6042\n",
      "Epoch 50/256\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.4516 - output1_loss: 0.3138 - output2_loss: 0.5893\n",
      "val/train: 0.990242\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.4533 - output1_loss: 0.3156 - output2_loss: 0.5910 - val_loss: 0.4489 - val_output1_loss: 0.3008 - val_output2_loss: 0.5970\n",
      "Epoch 51/256\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4523 - output1_loss: 0.3147 - output2_loss: 0.5899\n",
      "val/train: 0.974436\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4526 - output1_loss: 0.3146 - output2_loss: 0.5907 - val_loss: 0.4411 - val_output1_loss: 0.2983 - val_output2_loss: 0.5838\n",
      "Epoch 52/256\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.4515 - output1_loss: 0.3145 - output2_loss: 0.5884\n",
      "val/train: 0.985865\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4518 - output1_loss: 0.3149 - output2_loss: 0.5888 - val_loss: 0.4455 - val_output1_loss: 0.3003 - val_output2_loss: 0.5907\n",
      "Epoch 53/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4532 - output1_loss: 0.3166 - output2_loss: 0.5898\n",
      "val/train: 0.978926\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4526 - output1_loss: 0.3157 - output2_loss: 0.5894 - val_loss: 0.4430 - val_output1_loss: 0.2990 - val_output2_loss: 0.5870\n",
      "Epoch 54/256\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.4514 - output1_loss: 0.3132 - output2_loss: 0.5895\n",
      "val/train: 0.980596\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4539 - output1_loss: 0.3157 - output2_loss: 0.5920 - val_loss: 0.4451 - val_output1_loss: 0.2973 - val_output2_loss: 0.5928\n",
      "Epoch 55/256\n",
      "349/363 [===========================>..] - ETA: 0s - loss: 0.4540 - output1_loss: 0.3163 - output2_loss: 0.5917\n",
      "val/train: 0.985547\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.4518 - output1_loss: 0.3151 - output2_loss: 0.5885 - val_loss: 0.4453 - val_output1_loss: 0.2990 - val_output2_loss: 0.5916\n",
      "Epoch 56/256\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4517 - output1_loss: 0.3151 - output2_loss: 0.5884\n",
      "val/train: 0.968906\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.4513 - output1_loss: 0.3139 - output2_loss: 0.5886 - val_loss: 0.4372 - val_output1_loss: 0.2963 - val_output2_loss: 0.5782\n",
      "Epoch 57/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4496 - output1_loss: 0.3136 - output2_loss: 0.5857\n",
      "val/train: 1.005210\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.4503 - output1_loss: 0.3144 - output2_loss: 0.5861 - val_loss: 0.4526 - val_output1_loss: 0.3038 - val_output2_loss: 0.6015\n",
      "Epoch 58/256\n",
      "358/363 [============================>.] - ETA: 0s - loss: 0.4532 - output1_loss: 0.3150 - output2_loss: 0.5914\n",
      "val/train: 0.970958\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.4519 - output1_loss: 0.3137 - output2_loss: 0.5901 - val_loss: 0.4388 - val_output1_loss: 0.2963 - val_output2_loss: 0.5812\n",
      "Epoch 59/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4523 - output1_loss: 0.3154 - output2_loss: 0.5892\n",
      "val/train: 0.979950\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4510 - output1_loss: 0.3142 - output2_loss: 0.5877 - val_loss: 0.4419 - val_output1_loss: 0.2967 - val_output2_loss: 0.5872\n",
      "Epoch 60/256\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4519 - output1_loss: 0.3147 - output2_loss: 0.5890\n",
      "val/train: 0.994361\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.4508 - output1_loss: 0.3137 - output2_loss: 0.5879 - val_loss: 0.4482 - val_output1_loss: 0.3013 - val_output2_loss: 0.5952\n",
      "Epoch 61/256\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.4522 - output1_loss: 0.3150 - output2_loss: 0.5895\n",
      "val/train: 0.986250\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.4515 - output1_loss: 0.3138 - output2_loss: 0.5891 - val_loss: 0.4452 - val_output1_loss: 0.3008 - val_output2_loss: 0.5897\n",
      "Epoch 62/256\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4514 - output1_loss: 0.3164 - output2_loss: 0.5863\n",
      "val/train: 1.001557\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4501 - output1_loss: 0.3145 - output2_loss: 0.5857 - val_loss: 0.4508 - val_output1_loss: 0.3038 - val_output2_loss: 0.5978\n",
      "Epoch 63/256\n",
      "350/363 [===========================>..] - ETA: 0s - loss: 0.4533 - output1_loss: 0.3163 - output2_loss: 0.5903\n",
      "val/train: 1.031284\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.4510 - output1_loss: 0.3151 - output2_loss: 0.5870 - val_loss: 0.4651 - val_output1_loss: 0.3119 - val_output2_loss: 0.6183\n",
      "Epoch 64/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4485 - output1_loss: 0.3136 - output2_loss: 0.5833\n",
      "val/train: 1.069623\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.4495 - output1_loss: 0.3139 - output2_loss: 0.5850 - val_loss: 0.4807 - val_output1_loss: 0.3187 - val_output2_loss: 0.6428\n",
      "Epoch 65/256\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.4516 - output1_loss: 0.3152 - output2_loss: 0.5880\n",
      "val/train: 0.975510\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.4510 - output1_loss: 0.3143 - output2_loss: 0.5877 - val_loss: 0.4399 - val_output1_loss: 0.2966 - val_output2_loss: 0.5833\n",
      "Epoch 66/256\n",
      "341/363 [===========================>..] - ETA: 0s - loss: 0.4515 - output1_loss: 0.3135 - output2_loss: 0.5895\n",
      "val/train: 0.984680\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.4511 - output1_loss: 0.3144 - output2_loss: 0.5878 - val_loss: 0.4442 - val_output1_loss: 0.3011 - val_output2_loss: 0.5874\n",
      "Epoch 67/256\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4513 - output1_loss: 0.3137 - output2_loss: 0.5889\n",
      "val/train: 1.006109\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.4497 - output1_loss: 0.3128 - output2_loss: 0.5866 - val_loss: 0.4524 - val_output1_loss: 0.3021 - val_output2_loss: 0.6028\n",
      "Epoch 68/256\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4490 - output1_loss: 0.3130 - output2_loss: 0.5850\n",
      "val/train: 0.970642\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.4496 - output1_loss: 0.3133 - output2_loss: 0.5860 - val_loss: 0.4364 - val_output1_loss: 0.2938 - val_output2_loss: 0.5791\n",
      "Epoch 69/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4476 - output1_loss: 0.3119 - output2_loss: 0.5834\n",
      "val/train: 0.984123\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.4499 - output1_loss: 0.3137 - output2_loss: 0.5861 - val_loss: 0.4428 - val_output1_loss: 0.2992 - val_output2_loss: 0.5864\n",
      "Epoch 70/256\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.4489 - output1_loss: 0.3136 - output2_loss: 0.5842\n",
      "val/train: 0.981412\n",
      "363/363 [==============================] - 0s 580us/step - loss: 0.4492 - output1_loss: 0.3136 - output2_loss: 0.5847 - val_loss: 0.4408 - val_output1_loss: 0.2952 - val_output2_loss: 0.5864\n",
      "Epoch 71/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4498 - output1_loss: 0.3135 - output2_loss: 0.5860\n",
      "val/train: 1.023889\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.4490 - output1_loss: 0.3127 - output2_loss: 0.5852 - val_loss: 0.4597 - val_output1_loss: 0.3033 - val_output2_loss: 0.6162\n",
      "Epoch 72/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4502 - output1_loss: 0.3133 - output2_loss: 0.5872\n",
      "val/train: 0.987849\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.4494 - output1_loss: 0.3131 - output2_loss: 0.5857 - val_loss: 0.4439 - val_output1_loss: 0.2982 - val_output2_loss: 0.5897\n",
      "Epoch 73/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4480 - output1_loss: 0.3123 - output2_loss: 0.5837\n",
      "val/train: 1.013739\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.4482 - output1_loss: 0.3133 - output2_loss: 0.5831 - val_loss: 0.4543 - val_output1_loss: 0.3061 - val_output2_loss: 0.6026\n",
      "Epoch 74/256\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4471 - output1_loss: 0.3118 - output2_loss: 0.5823\n",
      "val/train: 1.114172\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.4484 - output1_loss: 0.3124 - output2_loss: 0.5844 - val_loss: 0.4996 - val_output1_loss: 0.3271 - val_output2_loss: 0.6720\n",
      "Epoch 75/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4479 - output1_loss: 0.3137 - output2_loss: 0.5822\n",
      "val/train: 0.999739\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.4479 - output1_loss: 0.3128 - output2_loss: 0.5830 - val_loss: 0.4478 - val_output1_loss: 0.3007 - val_output2_loss: 0.5948\n",
      "Epoch 76/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4486 - output1_loss: 0.3124 - output2_loss: 0.5847\n",
      "val/train: 0.966893\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4485 - output1_loss: 0.3122 - output2_loss: 0.5848 - val_loss: 0.4337 - val_output1_loss: 0.2949 - val_output2_loss: 0.5724\n",
      "Epoch 77/256\n",
      "345/363 [===========================>..] - ETA: 0s - loss: 0.4492 - output1_loss: 0.3128 - output2_loss: 0.5856\n",
      "val/train: 1.022319\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.4488 - output1_loss: 0.3136 - output2_loss: 0.5841 - val_loss: 0.4588 - val_output1_loss: 0.3058 - val_output2_loss: 0.6119\n",
      "Epoch 78/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4470 - output1_loss: 0.3131 - output2_loss: 0.5810\n",
      "val/train: 0.985277\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.4470 - output1_loss: 0.3122 - output2_loss: 0.5818 - val_loss: 0.4404 - val_output1_loss: 0.2959 - val_output2_loss: 0.5850\n",
      "Epoch 79/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4453 - output1_loss: 0.3119 - output2_loss: 0.5787\n",
      "val/train: 1.010129\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.4493 - output1_loss: 0.3145 - output2_loss: 0.5841 - val_loss: 0.4539 - val_output1_loss: 0.3044 - val_output2_loss: 0.6033\n",
      "Epoch 80/256\n",
      "359/363 [============================>.] - ETA: 0s - loss: 0.4481 - output1_loss: 0.3134 - output2_loss: 0.5827\n",
      "val/train: 1.057052\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.4481 - output1_loss: 0.3129 - output2_loss: 0.5833 - val_loss: 0.4737 - val_output1_loss: 0.3103 - val_output2_loss: 0.6371\n",
      "Epoch 81/256\n",
      "358/363 [============================>.] - ETA: 0s - loss: 0.4443 - output1_loss: 0.3099 - output2_loss: 0.5787\n",
      "val/train: 0.981700\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.4450 - output1_loss: 0.3112 - output2_loss: 0.5788 - val_loss: 0.4369 - val_output1_loss: 0.2965 - val_output2_loss: 0.5772\n",
      "Epoch 82/256\n",
      "318/363 [=========================>....] - ETA: 0s - loss: 0.4512 - output1_loss: 0.3171 - output2_loss: 0.5852\n",
      "val/train: 1.015081\n",
      "363/363 [==============================] - 0s 615us/step - loss: 0.4488 - output1_loss: 0.3128 - output2_loss: 0.5848 - val_loss: 0.4556 - val_output1_loss: 0.3016 - val_output2_loss: 0.6096\n",
      "Epoch 83/256\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.4441 - output1_loss: 0.3100 - output2_loss: 0.5782\n",
      "val/train: 0.993876\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.4444 - output1_loss: 0.3105 - output2_loss: 0.5782 - val_loss: 0.4416 - val_output1_loss: 0.2978 - val_output2_loss: 0.5855\n",
      "Epoch 84/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4453 - output1_loss: 0.3132 - output2_loss: 0.5774\n",
      "val/train: 0.967853\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.4453 - output1_loss: 0.3124 - output2_loss: 0.5782 - val_loss: 0.4309 - val_output1_loss: 0.2933 - val_output2_loss: 0.5686\n",
      "Epoch 85/256\n",
      "361/363 [============================>.] - ETA: 0s - loss: 0.4462 - output1_loss: 0.3115 - output2_loss: 0.5809\n",
      "val/train: 1.003431\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.4472 - output1_loss: 0.3121 - output2_loss: 0.5824 - val_loss: 0.4488 - val_output1_loss: 0.3021 - val_output2_loss: 0.5955\n",
      "Epoch 86/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4450 - output1_loss: 0.3118 - output2_loss: 0.5782\n",
      "val/train: 0.976636\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4448 - output1_loss: 0.3112 - output2_loss: 0.5784 - val_loss: 0.4344 - val_output1_loss: 0.2940 - val_output2_loss: 0.5749\n",
      "Epoch 87/256\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.4443 - output1_loss: 0.3105 - output2_loss: 0.5780\n",
      "val/train: 0.987600\n",
      "363/363 [==============================] - 0s 583us/step - loss: 0.4460 - output1_loss: 0.3117 - output2_loss: 0.5802 - val_loss: 0.4404 - val_output1_loss: 0.2970 - val_output2_loss: 0.5838\n",
      "Epoch 88/256\n",
      "332/363 [==========================>...] - ETA: 0s - loss: 0.4445 - output1_loss: 0.3094 - output2_loss: 0.5796\n",
      "val/train: 0.961209\n",
      "363/363 [==============================] - 0s 635us/step - loss: 0.4471 - output1_loss: 0.3115 - output2_loss: 0.5827 - val_loss: 0.4297 - val_output1_loss: 0.2920 - val_output2_loss: 0.5675\n",
      "Epoch 89/256\n",
      "332/363 [==========================>...] - ETA: 0s - loss: 0.4448 - output1_loss: 0.3097 - output2_loss: 0.5798\n",
      "val/train: 0.996749\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.4459 - output1_loss: 0.3107 - output2_loss: 0.5811 - val_loss: 0.4445 - val_output1_loss: 0.2962 - val_output2_loss: 0.5927\n",
      "Epoch 90/256\n",
      "344/363 [===========================>..] - ETA: 0s - loss: 0.4436 - output1_loss: 0.3093 - output2_loss: 0.5779\n",
      "val/train: 0.969775\n",
      "363/363 [==============================] - 0s 585us/step - loss: 0.4449 - output1_loss: 0.3107 - output2_loss: 0.5792 - val_loss: 0.4315 - val_output1_loss: 0.2925 - val_output2_loss: 0.5705\n",
      "Epoch 91/256\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4419 - output1_loss: 0.3088 - output2_loss: 0.5751\n",
      "val/train: 1.034895\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.4431 - output1_loss: 0.3103 - output2_loss: 0.5759 - val_loss: 0.4585 - val_output1_loss: 0.3101 - val_output2_loss: 0.6070\n",
      "Epoch 92/256\n",
      "334/363 [==========================>...] - ETA: 0s - loss: 0.4418 - output1_loss: 0.3092 - output2_loss: 0.5743\n",
      "val/train: 0.963742\n",
      "363/363 [==============================] - 0s 600us/step - loss: 0.4467 - output1_loss: 0.3113 - output2_loss: 0.5821 - val_loss: 0.4305 - val_output1_loss: 0.2925 - val_output2_loss: 0.5686\n",
      "Epoch 93/256\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4451 - output1_loss: 0.3110 - output2_loss: 0.5792\n",
      "val/train: 0.996022\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4447 - output1_loss: 0.3105 - output2_loss: 0.5790 - val_loss: 0.4430 - val_output1_loss: 0.2961 - val_output2_loss: 0.5898\n",
      "Epoch 94/256\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.4468 - output1_loss: 0.3120 - output2_loss: 0.5816\n",
      "val/train: 1.001547\n",
      "363/363 [==============================] - 0s 588us/step - loss: 0.4454 - output1_loss: 0.3111 - output2_loss: 0.5796 - val_loss: 0.4460 - val_output1_loss: 0.3032 - val_output2_loss: 0.5889\n",
      "Epoch 95/256\n",
      "347/363 [===========================>..] - ETA: 0s - loss: 0.4476 - output1_loss: 0.3123 - output2_loss: 0.5829\n",
      "val/train: 0.976034\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4447 - output1_loss: 0.3098 - output2_loss: 0.5795 - val_loss: 0.4340 - val_output1_loss: 0.2933 - val_output2_loss: 0.5747\n",
      "Epoch 96/256\n",
      "347/363 [===========================>..] - ETA: 0s - loss: 0.4443 - output1_loss: 0.3115 - output2_loss: 0.5772\n",
      "val/train: 1.035200\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.4447 - output1_loss: 0.3107 - output2_loss: 0.5786 - val_loss: 0.4603 - val_output1_loss: 0.3131 - val_output2_loss: 0.6075\n",
      "Epoch 97/256\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4463 - output1_loss: 0.3119 - output2_loss: 0.5806\n",
      "val/train: 0.973862\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.4443 - output1_loss: 0.3113 - output2_loss: 0.5774 - val_loss: 0.4327 - val_output1_loss: 0.2935 - val_output2_loss: 0.5719\n",
      "Epoch 98/256\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4399 - output1_loss: 0.3082 - output2_loss: 0.5715\n",
      "val/train: 1.010990\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.4424 - output1_loss: 0.3094 - output2_loss: 0.5754 - val_loss: 0.4473 - val_output1_loss: 0.3036 - val_output2_loss: 0.5909\n",
      "Epoch 99/256\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.4455 - output1_loss: 0.3117 - output2_loss: 0.5792\n",
      "val/train: 1.017192\n",
      "363/363 [==============================] - 0s 587us/step - loss: 0.4445 - output1_loss: 0.3107 - output2_loss: 0.5783 - val_loss: 0.4522 - val_output1_loss: 0.3057 - val_output2_loss: 0.5986\n",
      "Epoch 100/256\n",
      "285/363 [======================>.......] - ETA: 0s - loss: 0.4475 - output1_loss: 0.3136 - output2_loss: 0.5814\n",
      "val/train: 0.962196\n",
      "363/363 [==============================] - 0s 666us/step - loss: 0.4460 - output1_loss: 0.3110 - output2_loss: 0.5810 - val_loss: 0.4291 - val_output1_loss: 0.2917 - val_output2_loss: 0.5666\n",
      "Epoch 101/256\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.4417 - output1_loss: 0.3067 - output2_loss: 0.5766\n",
      "val/train: 1.045348\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4461 - output1_loss: 0.3108 - output2_loss: 0.5815 - val_loss: 0.4664 - val_output1_loss: 0.3083 - val_output2_loss: 0.6245\n",
      "Epoch 102/256\n",
      "349/363 [===========================>..] - ETA: 0s - loss: 0.4438 - output1_loss: 0.3092 - output2_loss: 0.5784\n",
      "val/train: 0.982002\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.4433 - output1_loss: 0.3101 - output2_loss: 0.5766 - val_loss: 0.4354 - val_output1_loss: 0.2935 - val_output2_loss: 0.5773\n",
      "Epoch 103/256\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.4427 - output1_loss: 0.3084 - output2_loss: 0.5769\n",
      "val/train: 0.982771\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.4437 - output1_loss: 0.3097 - output2_loss: 0.5777 - val_loss: 0.4360 - val_output1_loss: 0.2936 - val_output2_loss: 0.5784\n",
      "Epoch 104/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4427 - output1_loss: 0.3092 - output2_loss: 0.5761\n",
      "val/train: 0.994361\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4429 - output1_loss: 0.3090 - output2_loss: 0.5768 - val_loss: 0.4404 - val_output1_loss: 0.2977 - val_output2_loss: 0.5832\n",
      "Epoch 105/256\n",
      "346/363 [===========================>..] - ETA: 0s - loss: 0.4434 - output1_loss: 0.3102 - output2_loss: 0.5766\n",
      "val/train: 1.004459\n",
      "363/363 [==============================] - 0s 586us/step - loss: 0.4425 - output1_loss: 0.3102 - output2_loss: 0.5749 - val_loss: 0.4445 - val_output1_loss: 0.2974 - val_output2_loss: 0.5916\n",
      "Epoch 106/256\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.4460 - output1_loss: 0.3127 - output2_loss: 0.5793\n",
      "val/train: 0.989602\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4446 - output1_loss: 0.3111 - output2_loss: 0.5781 - val_loss: 0.4399 - val_output1_loss: 0.2975 - val_output2_loss: 0.5824\n",
      "Epoch 107/256\n",
      "350/363 [===========================>..] - ETA: 0s - loss: 0.4422 - output1_loss: 0.3077 - output2_loss: 0.5767\n",
      "val/train: 0.971461\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.4439 - output1_loss: 0.3106 - output2_loss: 0.5772 - val_loss: 0.4312 - val_output1_loss: 0.2923 - val_output2_loss: 0.5701\n",
      "Epoch 108/256\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.4414 - output1_loss: 0.3077 - output2_loss: 0.5751\n",
      "val/train: 1.036167\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4426 - output1_loss: 0.3094 - output2_loss: 0.5758 - val_loss: 0.4586 - val_output1_loss: 0.3144 - val_output2_loss: 0.6028\n",
      "Epoch 109/256\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.4390 - output1_loss: 0.3090 - output2_loss: 0.5690\n",
      "val/train: 0.982289\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.4403 - output1_loss: 0.3088 - output2_loss: 0.5719 - val_loss: 0.4325 - val_output1_loss: 0.2917 - val_output2_loss: 0.5734\n",
      "Epoch 110/256\n",
      "346/363 [===========================>..] - ETA: 0s - loss: 0.4426 - output1_loss: 0.3092 - output2_loss: 0.5760\n",
      "val/train: 0.973054\n",
      "363/363 [==============================] - 0s 618us/step - loss: 0.4419 - output1_loss: 0.3083 - output2_loss: 0.5755 - val_loss: 0.4300 - val_output1_loss: 0.2924 - val_output2_loss: 0.5675\n",
      "Epoch 111/256\n",
      "358/363 [============================>.] - ETA: 0s - loss: 0.4411 - output1_loss: 0.3089 - output2_loss: 0.5733\n",
      "val/train: 1.040631\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.4407 - output1_loss: 0.3083 - output2_loss: 0.5731 - val_loss: 0.4586 - val_output1_loss: 0.3058 - val_output2_loss: 0.6115\n",
      "Epoch 112/256\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.4404 - output1_loss: 0.3079 - output2_loss: 0.5728\n",
      "val/train: 0.984694\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.4425 - output1_loss: 0.3095 - output2_loss: 0.5754 - val_loss: 0.4357 - val_output1_loss: 0.2937 - val_output2_loss: 0.5776\n",
      "Epoch 113/256\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.4397 - output1_loss: 0.3074 - output2_loss: 0.5720\n",
      "val/train: 1.015249\n",
      "363/363 [==============================] - 0s 587us/step - loss: 0.4407 - output1_loss: 0.3080 - output2_loss: 0.5734 - val_loss: 0.4474 - val_output1_loss: 0.2994 - val_output2_loss: 0.5955\n",
      "Epoch 114/256\n",
      "356/363 [============================>.] - ETA: 0s - loss: 0.4404 - output1_loss: 0.3076 - output2_loss: 0.5731\n",
      "val/train: 0.986549\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.4415 - output1_loss: 0.3091 - output2_loss: 0.5740 - val_loss: 0.4356 - val_output1_loss: 0.2956 - val_output2_loss: 0.5756\n",
      "Epoch 115/256\n",
      "358/363 [============================>.] - ETA: 0s - loss: 0.4391 - output1_loss: 0.3076 - output2_loss: 0.5706\n",
      "val/train: 1.003936\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.4402 - output1_loss: 0.3079 - output2_loss: 0.5725 - val_loss: 0.4420 - val_output1_loss: 0.3013 - val_output2_loss: 0.5826\n",
      "Epoch 116/256\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.4422 - output1_loss: 0.3090 - output2_loss: 0.5754\n",
      "val/train: 1.021667\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.4420 - output1_loss: 0.3087 - output2_loss: 0.5754 - val_loss: 0.4516 - val_output1_loss: 0.3017 - val_output2_loss: 0.6016\n",
      "Epoch 117/256\n",
      "354/363 [============================>.] - ETA: 0s - loss: 0.4402 - output1_loss: 0.3074 - output2_loss: 0.5729\n",
      "val/train: 1.028885\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.4410 - output1_loss: 0.3084 - output2_loss: 0.5737 - val_loss: 0.4538 - val_output1_loss: 0.3022 - val_output2_loss: 0.6054\n",
      "Epoch 118/256\n",
      "355/363 [============================>.] - ETA: 0s - loss: 0.4409 - output1_loss: 0.3089 - output2_loss: 0.5730\n",
      "val/train: 0.990533\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.4406 - output1_loss: 0.3084 - output2_loss: 0.5728 - val_loss: 0.4364 - val_output1_loss: 0.2941 - val_output2_loss: 0.5787\n",
      "Epoch 119/256\n",
      "312/363 [========================>.....] - ETA: 0s - loss: 0.4429 - output1_loss: 0.3080 - output2_loss: 0.5778\n",
      "val/train: 0.983772\n",
      "363/363 [==============================] - 0s 624us/step - loss: 0.4428 - output1_loss: 0.3089 - output2_loss: 0.5767 - val_loss: 0.4356 - val_output1_loss: 0.2919 - val_output2_loss: 0.5794\n",
      "Epoch 120/256\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4441 - output1_loss: 0.3113 - output2_loss: 0.5769\n",
      "val/train: 0.964071\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.4433 - output1_loss: 0.3100 - output2_loss: 0.5765 - val_loss: 0.4273 - val_output1_loss: 0.2905 - val_output2_loss: 0.5642\n",
      "Epoch 121/256\n",
      "357/363 [============================>.] - ETA: 0s - loss: 0.4415 - output1_loss: 0.3094 - output2_loss: 0.5735\n",
      "val/train: 0.982334\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.4414 - output1_loss: 0.3089 - output2_loss: 0.5738 - val_loss: 0.4336 - val_output1_loss: 0.2937 - val_output2_loss: 0.5734\n",
      "Epoch 122/256\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.4388 - output1_loss: 0.3083 - output2_loss: 0.5694\n",
      "val/train: 0.972588\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.4401 - output1_loss: 0.3085 - output2_loss: 0.5717 - val_loss: 0.4280 - val_output1_loss: 0.2906 - val_output2_loss: 0.5654\n",
      "Epoch 123/256\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.4388 - output1_loss: 0.3062 - output2_loss: 0.5713\n",
      "val/train: 0.982800\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.4409 - output1_loss: 0.3087 - output2_loss: 0.5732 - val_loss: 0.4334 - val_output1_loss: 0.2936 - val_output2_loss: 0.5731\n",
      "Epoch 124/256\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.4403 - output1_loss: 0.3081 - output2_loss: 0.5725\n",
      "val/train: 1.012134\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.4386 - output1_loss: 0.3077 - output2_loss: 0.5695 - val_loss: 0.4439 - val_output1_loss: 0.2984 - val_output2_loss: 0.5894\n",
      "Epoch 125/256\n",
      "351/363 [============================>.] - ETA: 0s - loss: 0.4363 - output1_loss: 0.3064 - output2_loss: 0.5663\n",
      "val/train: 0.989169\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.4378 - output1_loss: 0.3070 - output2_loss: 0.5685 - val_loss: 0.4330 - val_output1_loss: 0.2936 - val_output2_loss: 0.5725\n",
      "Epoch 126/256\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.4349 - output1_loss: 0.3054 - output2_loss: 0.5645\n",
      "val/train: 1.038721\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4378 - output1_loss: 0.3067 - output2_loss: 0.5688 - val_loss: 0.4547 - val_output1_loss: 0.3070 - val_output2_loss: 0.6024\n",
      "Epoch 127/256\n",
      "353/363 [============================>.] - ETA: 0s - loss: 0.4418 - output1_loss: 0.3097 - output2_loss: 0.5740\n",
      "val/train: 0.990137\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.4416 - output1_loss: 0.3091 - output2_loss: 0.5740 - val_loss: 0.4372 - val_output1_loss: 0.2954 - val_output2_loss: 0.5790\n",
      "Epoch 128/256\n",
      "340/363 [===========================>..] - ETA: 0s - loss: 0.4385 - output1_loss: 0.3090 - output2_loss: 0.5681\n",
      "val/train: 0.984179\n",
      "363/363 [==============================] - 0s 593us/step - loss: 0.4409 - output1_loss: 0.3101 - output2_loss: 0.5718 - val_loss: 0.4340 - val_output1_loss: 0.2933 - val_output2_loss: 0.5746\n",
      "Epoch 129/256\n",
      "310/363 [========================>.....] - ETA: 0s - loss: 0.4425 - output1_loss: 0.3098 - output2_loss: 0.5752\n",
      "val/train: 1.010514\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.4403 - output1_loss: 0.3082 - output2_loss: 0.5724 - val_loss: 0.4449 - val_output1_loss: 0.3031 - val_output2_loss: 0.5868\n",
      "Epoch 130/256\n",
      "349/363 [===========================>..] - ETA: 0s - loss: 0.4420 - output1_loss: 0.3112 - output2_loss: 0.5728\n",
      "val/train: 1.000967\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.4391 - output1_loss: 0.3084 - output2_loss: 0.5697 - val_loss: 0.4395 - val_output1_loss: 0.2930 - val_output2_loss: 0.5859\n",
      "Epoch 131/256\n",
      "350/363 [===========================>..] - ETA: 0s - loss: 0.4379 - output1_loss: 0.3061 - output2_loss: 0.5697\n",
      "val/train: 0.984613\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.4406 - output1_loss: 0.3088 - output2_loss: 0.5724 - val_loss: 0.4339 - val_output1_loss: 0.2940 - val_output2_loss: 0.5737\n",
      "Epoch 132/256\n",
      "349/363 [===========================>..] - ETA: 0s - loss: 0.4400 - output1_loss: 0.3092 - output2_loss: 0.5709\n",
      "val/train: 0.994640\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.4393 - output1_loss: 0.3087 - output2_loss: 0.5700 - val_loss: 0.4370 - val_output1_loss: 0.2950 - val_output2_loss: 0.5789\n",
      "Epoch 133/256\n",
      "352/363 [============================>.] - ETA: 0s - loss: 0.4391 - output1_loss: 0.3075 - output2_loss: 0.5708\n",
      "val/train: 0.997771\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.4389 - output1_loss: 0.3072 - output2_loss: 0.5706 - val_loss: 0.4379 - val_output1_loss: 0.2962 - val_output2_loss: 0.5796\n",
      "Epoch 134/256\n",
      "350/363 [===========================>..] - ETA: 0s - loss: 0.4420 - output1_loss: 0.3086 - output2_loss: 0.5755\n",
      "val/train: 0.995920\n",
      "363/363 [==============================] - 0s 583us/step - loss: 0.4402 - output1_loss: 0.3070 - output2_loss: 0.5733 - val_loss: 0.4384 - val_output1_loss: 0.2998 - val_output2_loss: 0.5770\n",
      "Epoch 135/256\n",
      "350/363 [===========================>..] - ETA: 0s - loss: 0.4383 - output1_loss: 0.3074 - output2_loss: 0.5692\n",
      "val/train: 1.002625\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.4378 - output1_loss: 0.3063 - output2_loss: 0.5692 - val_loss: 0.4389 - val_output1_loss: 0.2923 - val_output2_loss: 0.5856\n",
      "Epoch 136/256\n",
      "342/363 [===========================>..] - ETA: 0s - loss: 0.4389 - output1_loss: 0.3079 - output2_loss: 0.5699\n",
      "val/train: 0.989316\n",
      "363/363 [==============================] - 0s 602us/step - loss: 0.4384 - output1_loss: 0.3073 - output2_loss: 0.5694 - val_loss: 0.4337 - val_output1_loss: 0.2927 - val_output2_loss: 0.5747\n",
      "Epoch 137/256\n",
      "311/363 [========================>.....] - ETA: 0s - loss: 0.4311 - output1_loss: 0.3041 - output2_loss: 0.5580\n",
      "val/train: 1.015459\n",
      "363/363 [==============================] - 0s 626us/step - loss: 0.4360 - output1_loss: 0.3062 - output2_loss: 0.5659 - val_loss: 0.4428 - val_output1_loss: 0.2991 - val_output2_loss: 0.5864\n",
      "Epoch 138/256\n",
      "341/363 [===========================>..] - ETA: 0s - loss: 0.4412 - output1_loss: 0.3067 - output2_loss: 0.5756\n",
      "val/train: 0.990130\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.4385 - output1_loss: 0.3067 - output2_loss: 0.5703 - val_loss: 0.4342 - val_output1_loss: 0.2967 - val_output2_loss: 0.5717\n",
      "Epoch 139/256\n",
      "346/363 [===========================>..] - ETA: 0s - loss: 0.4372 - output1_loss: 0.3062 - output2_loss: 0.5682\n",
      "val/train: 1.089365\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.4370 - output1_loss: 0.3069 - output2_loss: 0.5671 - val_loss: 0.4761 - val_output1_loss: 0.3243 - val_output2_loss: 0.6279\n",
      "Epoch 140/256\n",
      "343/363 [===========================>..] - ETA: 0s - loss: 0.4358 - output1_loss: 0.3044 - output2_loss: 0.5671\n",
      "val/train: 0.984015\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.4386 - output1_loss: 0.3066 - output2_loss: 0.5707 - val_loss: 0.4316 - val_output1_loss: 0.2966 - val_output2_loss: 0.5666\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    print(\"\\nval/train: {:2f}\".format(logs['val_loss'] / logs['loss']))\n",
    "\n",
    "history = model.fit((X_train[:, :4], X_train[:, 4:]), (y_train, y_train), epochs=256, validation_data=((X_val[:, :4], X_val[:, 4:]), y_val), callbacks=(save_on_epoch_cb, early_stopping_cb, CustomCallback()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.8833 - val_loss: 0.5690\n",
      "Epoch 2/256\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5644 - val_loss: 0.4652\n",
      "Epoch 3/256\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4845 - val_loss: 0.4277\n",
      "Epoch 4/256\n",
      "363/363 [==============================] - 0s 935us/step - loss: 0.4539 - val_loss: 0.4040\n",
      "Epoch 5/256\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.4315 - val_loss: 0.3891\n",
      "Epoch 6/256\n",
      "363/363 [==============================] - 0s 663us/step - loss: 0.4244 - val_loss: 0.3795\n",
      "Epoch 7/256\n",
      "363/363 [==============================] - 0s 638us/step - loss: 0.4093 - val_loss: 0.3672\n",
      "Epoch 8/256\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.3963 - val_loss: 0.3636\n",
      "Epoch 9/256\n",
      "363/363 [==============================] - 0s 529us/step - loss: 0.3892 - val_loss: 0.3579\n",
      "Epoch 10/256\n",
      "363/363 [==============================] - 0s 532us/step - loss: 0.3849 - val_loss: 0.3536\n",
      "Epoch 11/256\n",
      "363/363 [==============================] - 0s 514us/step - loss: 0.3800 - val_loss: 0.3508\n",
      "Epoch 12/256\n",
      "363/363 [==============================] - 0s 556us/step - loss: 0.3790 - val_loss: 0.3480\n",
      "Epoch 13/256\n",
      "363/363 [==============================] - 0s 528us/step - loss: 0.3785 - val_loss: 0.3479\n",
      "Epoch 14/256\n",
      "363/363 [==============================] - 0s 498us/step - loss: 0.3764 - val_loss: 0.3436\n",
      "Epoch 15/256\n",
      "363/363 [==============================] - 0s 509us/step - loss: 0.3672 - val_loss: 0.3416\n",
      "Epoch 16/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.3647 - val_loss: 0.3363\n",
      "Epoch 17/256\n",
      "363/363 [==============================] - 0s 503us/step - loss: 0.3651 - val_loss: 0.3367\n",
      "Epoch 18/256\n",
      "363/363 [==============================] - 0s 511us/step - loss: 0.3603 - val_loss: 0.3331\n",
      "Epoch 19/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.3615 - val_loss: 0.3373\n",
      "Epoch 20/256\n",
      "363/363 [==============================] - 0s 487us/step - loss: 0.3632 - val_loss: 0.3356\n",
      "Epoch 21/256\n",
      "363/363 [==============================] - 0s 500us/step - loss: 0.3594 - val_loss: 0.3296\n",
      "Epoch 22/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.3543 - val_loss: 0.3311\n",
      "Epoch 23/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.3520 - val_loss: 0.3347\n",
      "Epoch 24/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.3515 - val_loss: 0.3248\n",
      "Epoch 25/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.3519 - val_loss: 0.3244\n",
      "Epoch 26/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.3481 - val_loss: 0.3244\n",
      "Epoch 27/256\n",
      "363/363 [==============================] - 0s 487us/step - loss: 0.3473 - val_loss: 0.3223\n",
      "Epoch 28/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.3482 - val_loss: 0.3223\n",
      "Epoch 29/256\n",
      "363/363 [==============================] - 0s 487us/step - loss: 0.3451 - val_loss: 0.3189\n",
      "Epoch 30/256\n",
      "363/363 [==============================] - 0s 488us/step - loss: 0.3454 - val_loss: 0.3225\n",
      "Epoch 31/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.3411 - val_loss: 0.3207\n",
      "Epoch 32/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.3397 - val_loss: 0.3167\n",
      "Epoch 33/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.3439 - val_loss: 0.3189\n",
      "Epoch 34/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.3450 - val_loss: 0.3213\n",
      "Epoch 35/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.3381 - val_loss: 0.3257\n",
      "Epoch 36/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.3371 - val_loss: 0.3178\n",
      "Epoch 37/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.3338 - val_loss: 0.3116\n",
      "Epoch 38/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.3328 - val_loss: 0.3100\n",
      "Epoch 39/256\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.3328 - val_loss: 0.3125\n",
      "Epoch 40/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.3305 - val_loss: 0.3169\n",
      "Epoch 41/256\n",
      "363/363 [==============================] - 0s 472us/step - loss: 0.3300 - val_loss: 0.3141\n",
      "Epoch 42/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.3311 - val_loss: 0.3140\n",
      "Epoch 43/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.3302 - val_loss: 0.3111\n",
      "Epoch 44/256\n",
      "363/363 [==============================] - 0s 512us/step - loss: 0.3301 - val_loss: 0.3084\n",
      "Epoch 45/256\n",
      "363/363 [==============================] - 0s 487us/step - loss: 0.3253 - val_loss: 0.3163\n",
      "Epoch 46/256\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.3228 - val_loss: 0.3022\n",
      "Epoch 47/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.3223 - val_loss: 0.3028\n",
      "Epoch 48/256\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.3206 - val_loss: 0.3091\n",
      "Epoch 49/256\n",
      "363/363 [==============================] - 0s 504us/step - loss: 0.3223 - val_loss: 0.3050\n",
      "Epoch 50/256\n",
      "363/363 [==============================] - 0s 557us/step - loss: 0.3213 - val_loss: 0.3048\n",
      "Epoch 51/256\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.3223 - val_loss: 0.3052\n",
      "Epoch 52/256\n",
      "363/363 [==============================] - 0s 482us/step - loss: 0.3170 - val_loss: 0.3036\n",
      "Epoch 53/256\n",
      "363/363 [==============================] - 0s 509us/step - loss: 0.3152 - val_loss: 0.2996\n",
      "Epoch 54/256\n",
      "363/363 [==============================] - 0s 522us/step - loss: 0.3139 - val_loss: 0.3030\n",
      "Epoch 55/256\n",
      "363/363 [==============================] - 0s 514us/step - loss: 0.3129 - val_loss: 0.3045\n",
      "Epoch 56/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.3126 - val_loss: 0.2965\n",
      "Epoch 57/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.3131 - val_loss: 0.2999\n",
      "Epoch 58/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.3132 - val_loss: 0.3023\n",
      "Epoch 59/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.3101 - val_loss: 0.3012\n",
      "Epoch 60/256\n",
      "363/363 [==============================] - 0s 490us/step - loss: 0.3102 - val_loss: 0.2970\n",
      "Epoch 61/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.3095 - val_loss: 0.2931\n",
      "Epoch 62/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.3068 - val_loss: 0.2956\n",
      "Epoch 63/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.3058 - val_loss: 0.2995\n",
      "Epoch 64/256\n",
      "363/363 [==============================] - 0s 484us/step - loss: 0.3081 - val_loss: 0.2933\n",
      "Epoch 65/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.3043 - val_loss: 0.2999\n",
      "Epoch 66/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.3037 - val_loss: 0.2884\n",
      "Epoch 67/256\n",
      "363/363 [==============================] - 0s 484us/step - loss: 0.3029 - val_loss: 0.2888\n",
      "Epoch 68/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.3058 - val_loss: 0.2900\n",
      "Epoch 69/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.3034 - val_loss: 0.2874\n",
      "Epoch 70/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.3011 - val_loss: 0.2866\n",
      "Epoch 71/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.3007 - val_loss: 0.2873\n",
      "Epoch 72/256\n",
      "363/363 [==============================] - 0s 490us/step - loss: 0.3022 - val_loss: 0.2870\n",
      "Epoch 73/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2997 - val_loss: 0.2888\n",
      "Epoch 74/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2977 - val_loss: 0.2887\n",
      "Epoch 75/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.3001 - val_loss: 0.2869\n",
      "Epoch 76/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2972 - val_loss: 0.3054\n",
      "Epoch 77/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2987 - val_loss: 0.2842\n",
      "Epoch 78/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2952 - val_loss: 0.2878\n",
      "Epoch 79/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2959 - val_loss: 0.2871\n",
      "Epoch 80/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2988 - val_loss: 0.2839\n",
      "Epoch 81/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2968 - val_loss: 0.2859\n",
      "Epoch 82/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2939 - val_loss: 0.2852\n",
      "Epoch 83/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2959 - val_loss: 0.2940\n",
      "Epoch 84/256\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.2930 - val_loss: 0.2821\n",
      "Epoch 85/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2916 - val_loss: 0.2818\n",
      "Epoch 86/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2912 - val_loss: 0.2846\n",
      "Epoch 87/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.2907 - val_loss: 0.2817\n",
      "Epoch 88/256\n",
      "363/363 [==============================] - 0s 470us/step - loss: 0.2933 - val_loss: 0.2809\n",
      "Epoch 89/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2907 - val_loss: 0.2822\n",
      "Epoch 90/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2919 - val_loss: 0.2806\n",
      "Epoch 91/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2897 - val_loss: 0.2886\n",
      "Epoch 92/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2902 - val_loss: 0.2934\n",
      "Epoch 93/256\n",
      "363/363 [==============================] - 0s 483us/step - loss: 0.2931 - val_loss: 0.2909\n",
      "Epoch 94/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2936 - val_loss: 0.2899\n",
      "Epoch 95/256\n",
      "363/363 [==============================] - 0s 472us/step - loss: 0.2889 - val_loss: 0.2801\n",
      "Epoch 96/256\n",
      "363/363 [==============================] - 0s 485us/step - loss: 0.2909 - val_loss: 0.2827\n",
      "Epoch 97/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2870 - val_loss: 0.2805\n",
      "Epoch 98/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2873 - val_loss: 0.2821\n",
      "Epoch 99/256\n",
      "363/363 [==============================] - 0s 519us/step - loss: 0.2877 - val_loss: 0.2816\n",
      "Epoch 100/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2867 - val_loss: 0.2779\n",
      "Epoch 101/256\n",
      "363/363 [==============================] - 0s 471us/step - loss: 0.2859 - val_loss: 0.2836\n",
      "Epoch 102/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.2858 - val_loss: 0.2787\n",
      "Epoch 103/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2881 - val_loss: 0.2774\n",
      "Epoch 104/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2882 - val_loss: 0.2789\n",
      "Epoch 105/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2905 - val_loss: 0.2769\n",
      "Epoch 106/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2846 - val_loss: 0.2786\n",
      "Epoch 107/256\n",
      "363/363 [==============================] - 0s 485us/step - loss: 0.2851 - val_loss: 0.2852\n",
      "Epoch 108/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2881 - val_loss: 0.2821\n",
      "Epoch 109/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2903 - val_loss: 0.2779\n",
      "Epoch 110/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2842 - val_loss: 0.2788\n",
      "Epoch 111/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2844 - val_loss: 0.2829\n",
      "Epoch 112/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2833 - val_loss: 0.2768\n",
      "Epoch 113/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.2835 - val_loss: 0.2848\n",
      "Epoch 114/256\n",
      "363/363 [==============================] - 0s 472us/step - loss: 0.2874 - val_loss: 0.2761\n",
      "Epoch 115/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2818 - val_loss: 0.2755\n",
      "Epoch 116/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.2849 - val_loss: 0.2750\n",
      "Epoch 117/256\n",
      "363/363 [==============================] - 0s 483us/step - loss: 0.2805 - val_loss: 0.2750\n",
      "Epoch 118/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2800 - val_loss: 0.2748\n",
      "Epoch 119/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2806 - val_loss: 0.2745\n",
      "Epoch 120/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2804 - val_loss: 0.2755\n",
      "Epoch 121/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2804 - val_loss: 0.2763\n",
      "Epoch 122/256\n",
      "363/363 [==============================] - 0s 470us/step - loss: 0.2820 - val_loss: 0.2721\n",
      "Epoch 123/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2831 - val_loss: 0.2763\n",
      "Epoch 124/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2791 - val_loss: 0.2783\n",
      "Epoch 125/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2792 - val_loss: 0.2735\n",
      "Epoch 126/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2806 - val_loss: 0.2748\n",
      "Epoch 127/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2817 - val_loss: 0.2884\n",
      "Epoch 128/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2785 - val_loss: 0.2838\n",
      "Epoch 129/256\n",
      "363/363 [==============================] - 0s 485us/step - loss: 0.2812 - val_loss: 0.2706\n",
      "Epoch 130/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2804 - val_loss: 0.2769\n",
      "Epoch 131/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.2800 - val_loss: 0.2752\n",
      "Epoch 132/256\n",
      "363/363 [==============================] - 0s 484us/step - loss: 0.2798 - val_loss: 0.2770\n",
      "Epoch 133/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2771 - val_loss: 0.2711\n",
      "Epoch 134/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2791 - val_loss: 0.2747\n",
      "Epoch 135/256\n",
      "363/363 [==============================] - 0s 472us/step - loss: 0.2769 - val_loss: 0.2749\n",
      "Epoch 136/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2756 - val_loss: 0.2677\n",
      "Epoch 137/256\n",
      "363/363 [==============================] - 0s 469us/step - loss: 0.2787 - val_loss: 0.2778\n",
      "Epoch 138/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2757 - val_loss: 0.2779\n",
      "Epoch 139/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2776 - val_loss: 0.2770\n",
      "Epoch 140/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2784 - val_loss: 0.2688\n",
      "Epoch 141/256\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.2752 - val_loss: 0.2731\n",
      "Epoch 142/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2761 - val_loss: 0.2736\n",
      "Epoch 143/256\n",
      "363/363 [==============================] - 0s 472us/step - loss: 0.2763 - val_loss: 0.2752\n",
      "Epoch 144/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2749 - val_loss: 0.2664\n",
      "Epoch 145/256\n",
      "363/363 [==============================] - 0s 482us/step - loss: 0.2758 - val_loss: 0.2758\n",
      "Epoch 146/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2734 - val_loss: 0.2987\n",
      "Epoch 147/256\n",
      "363/363 [==============================] - 0s 482us/step - loss: 0.2768 - val_loss: 0.2687\n",
      "Epoch 148/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2756 - val_loss: 0.2717\n",
      "Epoch 149/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2739 - val_loss: 0.2735\n",
      "Epoch 150/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.2760 - val_loss: 0.2786\n",
      "Epoch 151/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2743 - val_loss: 0.2808\n",
      "Epoch 152/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2761 - val_loss: 0.2729\n",
      "Epoch 153/256\n",
      "363/363 [==============================] - 0s 488us/step - loss: 0.2743 - val_loss: 0.2674\n",
      "Epoch 154/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2735 - val_loss: 0.2657\n",
      "Epoch 155/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2731 - val_loss: 0.2721\n",
      "Epoch 156/256\n",
      "363/363 [==============================] - 0s 473us/step - loss: 0.2715 - val_loss: 0.2679\n",
      "Epoch 157/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2723 - val_loss: 0.2707\n",
      "Epoch 158/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2709 - val_loss: 0.2747\n",
      "Epoch 159/256\n",
      "363/363 [==============================] - 0s 470us/step - loss: 0.2753 - val_loss: 0.2661\n",
      "Epoch 160/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2739 - val_loss: 0.2766\n",
      "Epoch 161/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2720 - val_loss: 0.2710\n",
      "Epoch 162/256\n",
      "363/363 [==============================] - 0s 482us/step - loss: 0.2709 - val_loss: 0.2674\n",
      "Epoch 163/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2710 - val_loss: 0.2696\n",
      "Epoch 164/256\n",
      "363/363 [==============================] - 0s 487us/step - loss: 0.2726 - val_loss: 0.2644\n",
      "Epoch 165/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2731 - val_loss: 0.2672\n",
      "Epoch 166/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2737 - val_loss: 0.2667\n",
      "Epoch 167/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2721 - val_loss: 0.2642\n",
      "Epoch 168/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2701 - val_loss: 0.2736\n",
      "Epoch 169/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2707 - val_loss: 0.2698\n",
      "Epoch 170/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2710 - val_loss: 0.2640\n",
      "Epoch 171/256\n",
      "363/363 [==============================] - 0s 471us/step - loss: 0.2700 - val_loss: 0.2757\n",
      "Epoch 172/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2685 - val_loss: 0.2721\n",
      "Epoch 173/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.2686 - val_loss: 0.2713\n",
      "Epoch 174/256\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.2694 - val_loss: 0.2662\n",
      "Epoch 175/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2707 - val_loss: 0.2649\n",
      "Epoch 176/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2695 - val_loss: 0.2720\n",
      "Epoch 177/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.2699 - val_loss: 0.2671\n",
      "Epoch 178/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2708 - val_loss: 0.2708\n",
      "Epoch 179/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2675 - val_loss: 0.2636\n",
      "Epoch 180/256\n",
      "363/363 [==============================] - 0s 515us/step - loss: 0.2682 - val_loss: 0.2724\n",
      "Epoch 181/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2678 - val_loss: 0.2627\n",
      "Epoch 182/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2684 - val_loss: 0.2641\n",
      "Epoch 183/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2679 - val_loss: 0.2730\n",
      "Epoch 184/256\n",
      "363/363 [==============================] - 0s 491us/step - loss: 0.2674 - val_loss: 0.2646\n",
      "Epoch 185/256\n",
      "363/363 [==============================] - 0s 473us/step - loss: 0.2675 - val_loss: 0.2717\n",
      "Epoch 186/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2677 - val_loss: 0.2690\n",
      "Epoch 187/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2663 - val_loss: 0.2850\n",
      "Epoch 188/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.2671 - val_loss: 0.2665\n",
      "Epoch 189/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2673 - val_loss: 0.2680\n",
      "Epoch 190/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2682 - val_loss: 0.2706\n",
      "Epoch 191/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2672 - val_loss: 0.2696\n",
      "Epoch 192/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2658 - val_loss: 0.2647\n",
      "Epoch 193/256\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.2681 - val_loss: 0.2719\n",
      "Epoch 194/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2661 - val_loss: 0.2735\n",
      "Epoch 195/256\n",
      "363/363 [==============================] - 0s 473us/step - loss: 0.2657 - val_loss: 0.2697\n",
      "Epoch 196/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2683 - val_loss: 0.2627\n",
      "Epoch 197/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2662 - val_loss: 0.2623\n",
      "Epoch 198/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2659 - val_loss: 0.2655\n",
      "Epoch 199/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2678 - val_loss: 0.2638\n",
      "Epoch 200/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2683 - val_loss: 0.2644\n",
      "Epoch 201/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2662 - val_loss: 0.2628\n",
      "Epoch 202/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2651 - val_loss: 0.2676\n",
      "Epoch 203/256\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.2674 - val_loss: 0.2713\n",
      "Epoch 204/256\n",
      "363/363 [==============================] - 0s 497us/step - loss: 0.2696 - val_loss: 0.2707\n",
      "Epoch 205/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2661 - val_loss: 0.2621\n",
      "Epoch 206/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2658 - val_loss: 0.2832\n",
      "Epoch 207/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2654 - val_loss: 0.2620\n",
      "Epoch 208/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2662 - val_loss: 0.2649\n",
      "Epoch 209/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2659 - val_loss: 0.2622\n",
      "Epoch 210/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2639 - val_loss: 0.2639\n",
      "Epoch 211/256\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.2640 - val_loss: 0.2716\n",
      "Epoch 212/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2659 - val_loss: 0.2809\n",
      "Epoch 213/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2653 - val_loss: 0.2640\n",
      "Epoch 214/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2637 - val_loss: 0.2648\n",
      "Epoch 215/256\n",
      "363/363 [==============================] - 0s 483us/step - loss: 0.2643 - val_loss: 0.2610\n",
      "Epoch 216/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2633 - val_loss: 0.2655\n",
      "Epoch 217/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2654 - val_loss: 0.2669\n",
      "Epoch 218/256\n",
      "363/363 [==============================] - 0s 472us/step - loss: 0.2636 - val_loss: 0.2683\n",
      "Epoch 219/256\n",
      "363/363 [==============================] - 0s 482us/step - loss: 0.2645 - val_loss: 0.2620\n",
      "Epoch 220/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2626 - val_loss: 0.2678\n",
      "Epoch 221/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2627 - val_loss: 0.2653\n",
      "Epoch 222/256\n",
      "363/363 [==============================] - 0s 515us/step - loss: 0.2642 - val_loss: 0.2611\n",
      "Epoch 223/256\n",
      "363/363 [==============================] - 0s 484us/step - loss: 0.2661 - val_loss: 0.2606\n",
      "Epoch 224/256\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.2613 - val_loss: 0.2751\n",
      "Epoch 225/256\n",
      "363/363 [==============================] - 0s 472us/step - loss: 0.2620 - val_loss: 0.2733\n",
      "Epoch 226/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2613 - val_loss: 0.2714\n",
      "Epoch 227/256\n",
      "363/363 [==============================] - 0s 487us/step - loss: 0.2627 - val_loss: 0.2635\n",
      "Epoch 228/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2623 - val_loss: 0.2615\n",
      "Epoch 229/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2613 - val_loss: 0.2822\n",
      "Epoch 230/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2625 - val_loss: 0.2718\n",
      "Epoch 231/256\n",
      "363/363 [==============================] - 0s 473us/step - loss: 0.2636 - val_loss: 0.2619\n",
      "Epoch 232/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2606 - val_loss: 0.2645\n",
      "Epoch 233/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2626 - val_loss: 0.2682\n",
      "Epoch 234/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2609 - val_loss: 0.2664\n",
      "Epoch 235/256\n",
      "363/363 [==============================] - 0s 484us/step - loss: 0.2598 - val_loss: 0.2604\n",
      "Epoch 236/256\n",
      "363/363 [==============================] - 0s 471us/step - loss: 0.2625 - val_loss: 0.2745\n",
      "Epoch 237/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2604 - val_loss: 0.2671\n",
      "Epoch 238/256\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.2612 - val_loss: 0.2650\n",
      "Epoch 239/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2612 - val_loss: 0.2616\n",
      "Epoch 240/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2605 - val_loss: 0.2619\n",
      "Epoch 241/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2599 - val_loss: 0.2700\n",
      "Epoch 242/256\n",
      "363/363 [==============================] - 0s 496us/step - loss: 0.2614 - val_loss: 0.2637\n",
      "Epoch 243/256\n",
      "363/363 [==============================] - 0s 485us/step - loss: 0.2584 - val_loss: 0.2670\n",
      "Epoch 244/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2600 - val_loss: 0.2627\n",
      "Epoch 245/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2607 - val_loss: 0.2594\n",
      "Epoch 246/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2618 - val_loss: 0.2674\n",
      "Epoch 247/256\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.2590 - val_loss: 0.2675\n",
      "Epoch 248/256\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.2593 - val_loss: 0.2627\n",
      "Epoch 249/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2606 - val_loss: 0.2678\n",
      "Epoch 250/256\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.2605 - val_loss: 0.2636\n",
      "Epoch 251/256\n",
      "363/363 [==============================] - 0s 474us/step - loss: 0.2625 - val_loss: 0.2681\n",
      "Epoch 252/256\n",
      "363/363 [==============================] - 0s 471us/step - loss: 0.2615 - val_loss: 0.2653\n",
      "Epoch 253/256\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.2579 - val_loss: 0.2626\n",
      "Epoch 254/256\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.2589 - val_loss: 0.2620\n",
      "Epoch 255/256\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.2593 - val_loss: 0.2609\n",
      "Epoch 256/256\n",
      "363/363 [==============================] - 0s 483us/step - loss: 0.2586 - val_loss: 0.2624\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "\n",
    "log_dir = os.path.join(os.curdir, 'logs')\n",
    "\n",
    "def get_run_log_dir():\n",
    "  run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "  return os.path.join(log_dir, run_id)\n",
    "\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "output = keras.layers.Dense(1)(hidden2)\n",
    "\n",
    "model = keras.Model(inputs=(input_), outputs=(output))\n",
    "model.compile(optimizer=keras.optimizers.SGD(0.005), loss='mse')\n",
    "\n",
    "run_log_dir = get_run_log_dir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_log_dir)\n",
    "history = model.fit(X_train, y_train, epochs=256, validation_data=(X_val, y_val), callbacks=(tensorboard_cb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/q7k30xcj51x3tc06z506z3n00000gn/T/ipykernel_42341/401080360.py:12: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "# fine tuning\n",
    "\n",
    "def build_model(n_hidden=1, n_nodes=30, lr=0.005, input_shape=(8), output_nodes=1, output_activation=keras.activations.linear, loss='mse'):\n",
    "  model = keras.models.Sequential()\n",
    "  model.add(keras.layers.Input(shape=input_shape))\n",
    "  for _ in range(n_hidden):\n",
    "    model.add(keras.layers.Dense(n_nodes, activation='relu'))\n",
    "  model.add(keras.layers.Dense(output_nodes, activation=output_activation))\n",
    "  model.compile(optimizer=keras.optimizers.SGD(lr), loss=loss)\n",
    "  return model\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7755 - val_loss: 0.4897\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.4518 - val_loss: 0.3668\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.4114 - val_loss: 0.3680\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.3880 - val_loss: 0.3415\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3751 - val_loss: 0.3548\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.3510 - val_loss: 0.3378\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.3511 - val_loss: 0.3311\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3446 - val_loss: 0.3116\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.3374 - val_loss: 0.3216\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3382 - val_loss: 0.3283\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.3323 - val_loss: 0.3038\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.3218 - val_loss: 0.3237\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.3168 - val_loss: 0.3086\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.3195 - val_loss: 0.3176\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.3150 - val_loss: 0.3115\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3017 - val_loss: 0.3737\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.3085 - val_loss: 0.4169\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2934 - val_loss: 0.3034\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.2963 - val_loss: 0.3678\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.2923 - val_loss: 0.3042\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.2844 - val_loss: 0.2769\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.2819 - val_loss: 0.3079\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.2831 - val_loss: 0.3003\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.2791 - val_loss: 0.2991\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.2812 - val_loss: 0.3318\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.2779 - val_loss: 0.2832\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2766 - val_loss: 0.3451\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.2719 - val_loss: 0.2950\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.2747 - val_loss: 0.3035\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.2694 - val_loss: 0.3161\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.2628 - val_loss: 0.2773\n",
      "121/121 [==============================] - 0s 391us/step - loss: 0.3211\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.7104 - val_loss: 0.4135\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.4597 - val_loss: 0.4549\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.4118 - val_loss: 0.5024\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3975 - val_loss: 0.4145\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3779 - val_loss: 0.3428\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3677 - val_loss: 0.3298\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.3634 - val_loss: 0.3130\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.3524 - val_loss: 0.3198\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3469 - val_loss: 0.3173\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3415 - val_loss: 0.3261\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3313 - val_loss: 0.4258\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3276 - val_loss: 0.4136\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3239 - val_loss: 0.3752\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.3241 - val_loss: 0.2925\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3127 - val_loss: 0.2957\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.3166 - val_loss: 0.3307\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.3096 - val_loss: 0.3260\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3081 - val_loss: 0.3122\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3191 - val_loss: 0.3540\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3126 - val_loss: 0.2886\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3031 - val_loss: 0.2855\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.2973 - val_loss: 0.3634\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.2909 - val_loss: 0.3389\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.2924 - val_loss: 0.3636\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.2906 - val_loss: 0.2966\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.2913 - val_loss: 0.3080\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.2841 - val_loss: 0.2848\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.2805 - val_loss: 0.2848\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2868 - val_loss: 0.3880\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.2814 - val_loss: 0.3308\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.2805 - val_loss: 0.2988\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.2819 - val_loss: 0.2814\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.2799 - val_loss: 0.3227\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.2752 - val_loss: 0.2745\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.2739 - val_loss: 0.2751\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.2728 - val_loss: 0.2798\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.2667 - val_loss: 0.2859\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.2666 - val_loss: 0.2979\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.2690 - val_loss: 0.3112\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.2650 - val_loss: 0.2885\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.2626 - val_loss: 0.2986\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.2601 - val_loss: 0.2804\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.2566 - val_loss: 0.2837\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.2571 - val_loss: 0.2760\n",
      "121/121 [==============================] - 0s 368us/step - loss: 0.3043\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.7140 - val_loss: 0.4639\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.4563 - val_loss: 0.4161\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.4163 - val_loss: 0.3850\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3994 - val_loss: 0.3699\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3871 - val_loss: 0.3394\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3709 - val_loss: 0.3314\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3552 - val_loss: 0.3276\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.3550 - val_loss: 0.3141\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.3513 - val_loss: 0.3455\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.3421 - val_loss: 0.3748\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.3311 - val_loss: 0.3179\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3343 - val_loss: 0.2940\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.3244 - val_loss: 0.4198\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.3183 - val_loss: 0.2808\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3181 - val_loss: 0.2828\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3136 - val_loss: 0.3380\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3063 - val_loss: 0.3013\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3101 - val_loss: 0.3233\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.3037 - val_loss: 0.2966\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.3032 - val_loss: 0.2780\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.3283 - val_loss: 0.3105\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.3034 - val_loss: 0.2772\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.2932 - val_loss: 0.2983\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.2931 - val_loss: 0.2878\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.2966 - val_loss: 0.2687\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.2860 - val_loss: 0.2761\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.2911 - val_loss: 0.2846\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.2853 - val_loss: 0.2767\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.2834 - val_loss: 0.2812\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2847 - val_loss: 0.3030\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.2796 - val_loss: 0.2969\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.2799 - val_loss: 0.2945\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.2750 - val_loss: 0.2944\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.2743 - val_loss: 0.2822\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.2788 - val_loss: 0.2796\n",
      "121/121 [==============================] - 0s 388us/step - loss: 0.2936\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8493 - val_loss: 0.5442\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4673 - val_loss: 0.3934\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.4155 - val_loss: 0.3794\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.3864 - val_loss: 0.4468\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3776 - val_loss: 0.3543\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.3572 - val_loss: 0.3325\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.3531 - val_loss: 0.3270\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.3420 - val_loss: 0.4622\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3371 - val_loss: 0.3280\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.3325 - val_loss: 0.3221\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3273 - val_loss: 0.3246\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3201 - val_loss: 0.3066\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.3137 - val_loss: 0.3174\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.3135 - val_loss: 0.3221\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.3091 - val_loss: 0.2922\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.3030 - val_loss: 0.3013\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.3085 - val_loss: 0.3109\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.3011 - val_loss: 0.2993\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2953 - val_loss: 0.2964\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.2951 - val_loss: 0.2943\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.2902 - val_loss: 0.2832\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.2857 - val_loss: 0.3051\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2903 - val_loss: 0.2978\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2830 - val_loss: 0.2810\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.2815 - val_loss: 0.3048\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2783 - val_loss: 0.2871\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2820 - val_loss: 0.2842\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2794 - val_loss: 0.3318\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2775 - val_loss: 0.2867\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.2747 - val_loss: 0.2794\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.2739 - val_loss: 0.2789\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.2720 - val_loss: 0.2991\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.2676 - val_loss: 0.2927\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.2668 - val_loss: 0.3787\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.2653 - val_loss: 0.2759\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2623 - val_loss: 0.2803\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.2620 - val_loss: 0.3096\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.2675 - val_loss: 0.2863\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2655 - val_loss: 0.2742\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.2563 - val_loss: 0.2763\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2585 - val_loss: 0.3145\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.2570 - val_loss: 0.2725\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.2585 - val_loss: 0.2730\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.2562 - val_loss: 0.2780\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2547 - val_loss: 0.2794\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2554 - val_loss: 0.2843\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.2531 - val_loss: 0.3185\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2514 - val_loss: 0.3037\n",
      "Epoch 49/128\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.2520 - val_loss: 0.2844\n",
      "Epoch 50/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2504 - val_loss: 0.2923\n",
      "Epoch 51/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.2460 - val_loss: 0.2881\n",
      "Epoch 52/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2473 - val_loss: 0.3271\n",
      "121/121 [==============================] - 0s 409us/step - loss: 0.3576\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8279 - val_loss: 0.4553\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.4629 - val_loss: 0.5206\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4210 - val_loss: 0.4128\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.4001 - val_loss: 0.3486\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.3860 - val_loss: 0.3491\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.3695 - val_loss: 0.3492\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3660 - val_loss: 0.4022\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3608 - val_loss: 0.3241\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3534 - val_loss: 0.3477\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.3489 - val_loss: 0.3146\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3394 - val_loss: 0.3219\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.3375 - val_loss: 0.2984\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.3337 - val_loss: 0.3133\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3307 - val_loss: 0.3433\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.3220 - val_loss: 0.3133\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3187 - val_loss: 0.2932\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3177 - val_loss: 0.2955\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.3169 - val_loss: 0.3063\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.3084 - val_loss: 0.3025\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.3091 - val_loss: 0.3056\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3054 - val_loss: 0.3090\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.3021 - val_loss: 0.2982\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3013 - val_loss: 0.2814\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.3005 - val_loss: 0.2884\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.2941 - val_loss: 0.3023\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.2932 - val_loss: 0.3016\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2964 - val_loss: 0.2804\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2859 - val_loss: 0.3122\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2892 - val_loss: 0.2902\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.2859 - val_loss: 0.3029\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.2861 - val_loss: 0.2842\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.2793 - val_loss: 0.2874\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.2825 - val_loss: 0.3290\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2755 - val_loss: 0.2861\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.2763 - val_loss: 0.2844\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.2784 - val_loss: 0.3017\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.2734 - val_loss: 0.2834\n",
      "121/121 [==============================] - 0s 389us/step - loss: 0.2966\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8851 - val_loss: 0.4499\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.4776 - val_loss: 0.4143\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.4334 - val_loss: 0.3677\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.4143 - val_loss: 0.3548\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3951 - val_loss: 0.3860\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.3854 - val_loss: 0.3357\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.3796 - val_loss: 0.3399\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.3718 - val_loss: 0.3277\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.3625 - val_loss: 0.3438\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.3618 - val_loss: 0.3176\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.3464 - val_loss: 0.3202\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3480 - val_loss: 0.3180\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3414 - val_loss: 0.3124\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3356 - val_loss: 0.3364\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.3325 - val_loss: 0.3472\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.3327 - val_loss: 0.3138\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3232 - val_loss: 0.3788\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.3206 - val_loss: 0.3595\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3188 - val_loss: 0.3105\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.3136 - val_loss: 0.3356\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3109 - val_loss: 0.3432\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3104 - val_loss: 0.3258\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.3091 - val_loss: 0.3252\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.3023 - val_loss: 0.2967\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3110 - val_loss: 0.2950\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.3036 - val_loss: 0.3034\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.2996 - val_loss: 0.2995\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.2997 - val_loss: 0.3029\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.2944 - val_loss: 0.2946\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.2976 - val_loss: 0.2900\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2938 - val_loss: 0.2830\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.2925 - val_loss: 0.2860\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2894 - val_loss: 0.3287\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2870 - val_loss: 0.2806\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.2844 - val_loss: 0.2848\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2830 - val_loss: 0.2894\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.2840 - val_loss: 0.2805\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.2849 - val_loss: 0.4144\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.2815 - val_loss: 0.2913\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.2814 - val_loss: 0.2896\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2819 - val_loss: 0.2863\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.2785 - val_loss: 0.3169\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.2752 - val_loss: 0.3138\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.2747 - val_loss: 0.2768\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.2759 - val_loss: 0.3424\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.2719 - val_loss: 0.2770\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.2705 - val_loss: 0.2872\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.2719 - val_loss: 0.3047\n",
      "Epoch 49/128\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.2751 - val_loss: 0.2863\n",
      "Epoch 50/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.2706 - val_loss: 0.2848\n",
      "Epoch 51/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.2643 - val_loss: 0.3137\n",
      "Epoch 52/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.2682 - val_loss: 0.2758\n",
      "Epoch 53/128\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.2661 - val_loss: 0.2715\n",
      "Epoch 54/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.2678 - val_loss: 0.3004\n",
      "Epoch 55/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.2632 - val_loss: 0.3018\n",
      "Epoch 56/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.2595 - val_loss: 0.2814\n",
      "Epoch 57/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.2605 - val_loss: 0.2915\n",
      "Epoch 58/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2638 - val_loss: 0.2721\n",
      "Epoch 59/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.2569 - val_loss: 0.2920\n",
      "Epoch 60/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2566 - val_loss: 0.2740\n",
      "Epoch 61/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.2571 - val_loss: 0.2864\n",
      "Epoch 62/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2560 - val_loss: 0.2697\n",
      "Epoch 63/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2521 - val_loss: 0.2823\n",
      "Epoch 64/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2523 - val_loss: 0.2903\n",
      "Epoch 65/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.2552 - val_loss: 0.2737\n",
      "Epoch 66/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2556 - val_loss: 0.2729\n",
      "Epoch 67/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.2550 - val_loss: 0.2758\n",
      "Epoch 68/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.2493 - val_loss: 0.2775\n",
      "Epoch 69/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.2484 - val_loss: 0.2702\n",
      "Epoch 70/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.2501 - val_loss: 0.2884\n",
      "Epoch 71/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2510 - val_loss: 0.2767\n",
      "Epoch 72/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2443 - val_loss: 0.2741\n",
      "121/121 [==============================] - 0s 386us/step - loss: 0.2739\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.7246 - val_loss: 0.4263\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.4731 - val_loss: 0.3859\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.4186 - val_loss: 0.3625\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.3991 - val_loss: 0.3487\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3761 - val_loss: 0.3191\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.3682 - val_loss: 0.3564\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3607 - val_loss: 0.3257\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3472 - val_loss: 0.3863\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.3378 - val_loss: 0.3257\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3331 - val_loss: 0.3039\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.3285 - val_loss: 0.3138\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.3287 - val_loss: 0.3028\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3116 - val_loss: 0.2917\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.3076 - val_loss: 0.3506\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.3092 - val_loss: 0.3186\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.3104 - val_loss: 0.2982\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.3046 - val_loss: 0.2994\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3049 - val_loss: 0.3438\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2958 - val_loss: 0.3511\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.3015 - val_loss: 0.2879\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.2884 - val_loss: 0.2869\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.2844 - val_loss: 0.3237\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.2851 - val_loss: 0.3806\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.2880 - val_loss: 0.2798\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.2874 - val_loss: 0.2825\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.2837 - val_loss: 0.2847\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.2796 - val_loss: 0.3285\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.2787 - val_loss: 0.2755\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.2737 - val_loss: 0.2945\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.2737 - val_loss: 0.2746\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.2733 - val_loss: 0.3487\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.2701 - val_loss: 0.2748\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.2703 - val_loss: 0.2947\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2678 - val_loss: 0.3212\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.2655 - val_loss: 0.2840\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2636 - val_loss: 0.2855\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.2592 - val_loss: 0.2950\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2594 - val_loss: 0.2711\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2566 - val_loss: 0.2813\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.2565 - val_loss: 0.2762\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.2546 - val_loss: 0.3424\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.2557 - val_loss: 0.2819\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2480 - val_loss: 0.3231\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2574 - val_loss: 0.3004\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.2479 - val_loss: 0.2782\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.2486 - val_loss: 0.2677\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2526 - val_loss: 0.2828\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2485 - val_loss: 0.2788\n",
      "Epoch 49/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2446 - val_loss: 0.2805\n",
      "Epoch 50/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.2477 - val_loss: 0.3154\n",
      "Epoch 51/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2441 - val_loss: 0.2763\n",
      "Epoch 52/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2460 - val_loss: 0.2765\n",
      "Epoch 53/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.2419 - val_loss: 0.2754\n",
      "Epoch 54/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2372 - val_loss: 0.2763\n",
      "Epoch 55/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2393 - val_loss: 0.2754\n",
      "Epoch 56/128\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.2375 - val_loss: 0.3499\n",
      "121/121 [==============================] - 0s 386us/step - loss: 0.3756\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8407 - val_loss: 0.5261\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.4869 - val_loss: 0.4173\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.4247 - val_loss: 0.3487\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.4060 - val_loss: 0.3911\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3818 - val_loss: 0.3349\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3720 - val_loss: 0.3586\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3678 - val_loss: 0.3561\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.3605 - val_loss: 0.3431\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3479 - val_loss: 0.3619\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.3477 - val_loss: 0.3267\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3364 - val_loss: 0.3224\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.3429 - val_loss: 0.3091\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3307 - val_loss: 0.2943\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.3290 - val_loss: 0.3010\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.3215 - val_loss: 0.3339\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.3211 - val_loss: 0.3105\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3118 - val_loss: 0.3170\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3084 - val_loss: 0.3803\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.3101 - val_loss: 0.2841\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 756us/step - loss: 0.3038 - val_loss: 0.2827\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3020 - val_loss: 0.3149\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.2975 - val_loss: 0.2827\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2985 - val_loss: 0.2828\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2952 - val_loss: 0.3093\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2902 - val_loss: 0.7384\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2924 - val_loss: 0.3716\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2939 - val_loss: 0.2957\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.2908 - val_loss: 0.2860\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2899 - val_loss: 0.2821\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.2890 - val_loss: 0.2937\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.2774 - val_loss: 0.2837\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.2795 - val_loss: 0.2825\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.2854 - val_loss: 0.2782\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2857 - val_loss: 0.3710\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.2835 - val_loss: 0.3015\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2776 - val_loss: 0.4464\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.2748 - val_loss: 0.3385\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.2763 - val_loss: 0.2689\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.2699 - val_loss: 0.2741\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.2683 - val_loss: 0.2922\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2688 - val_loss: 0.3355\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2682 - val_loss: 0.3152\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2664 - val_loss: 0.2753\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.2638 - val_loss: 0.3883\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2608 - val_loss: 0.2911\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2614 - val_loss: 0.3491\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2547 - val_loss: 0.2870\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2614 - val_loss: 0.2788\n",
      "121/121 [==============================] - 0s 387us/step - loss: 0.3032\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8466 - val_loss: 0.5631\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.5094 - val_loss: 0.3791\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.4533 - val_loss: 0.3696\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.4118 - val_loss: 0.5641\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.4069 - val_loss: 0.3444\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.3963 - val_loss: 0.4222\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.3859 - val_loss: 0.3272\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3610 - val_loss: 0.3546\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3639 - val_loss: 0.3073\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3518 - val_loss: 0.3209\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.3499 - val_loss: 0.3020\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.3332 - val_loss: 0.2961\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3277 - val_loss: 0.3145\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.3287 - val_loss: 0.2909\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3236 - val_loss: 0.2976\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.3200 - val_loss: 0.3319\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3202 - val_loss: 0.2888\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.3132 - val_loss: 0.5870\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.3093 - val_loss: 0.3123\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.3055 - val_loss: 0.2956\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.3078 - val_loss: 0.2823\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3052 - val_loss: 0.2961\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.2992 - val_loss: 0.2758\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3001 - val_loss: 0.3059\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.2980 - val_loss: 0.2841\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.2977 - val_loss: 0.2858\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.2916 - val_loss: 0.2968\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2914 - val_loss: 0.3239\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.2871 - val_loss: 0.2765\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.2894 - val_loss: 0.2769\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.2861 - val_loss: 0.2923\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.2858 - val_loss: 0.2679\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.2800 - val_loss: 0.2973\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.2805 - val_loss: 0.3253\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.2825 - val_loss: 0.2845\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.2746 - val_loss: 0.2806\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.2767 - val_loss: 0.2699\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2807 - val_loss: 0.2825\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.2707 - val_loss: 0.2819\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2729 - val_loss: 0.2878\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2767 - val_loss: 0.4230\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.2688 - val_loss: 0.4352\n",
      "121/121 [==============================] - 0s 385us/step - loss: 0.4277\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.6784 - val_loss: 0.4397\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.4595 - val_loss: 0.5357\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.4119 - val_loss: 0.3559\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.3857 - val_loss: 0.4083\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.3648 - val_loss: 0.3809\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3606 - val_loss: 0.3261\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.3631 - val_loss: 0.3065\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3470 - val_loss: 0.3238\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3369 - val_loss: 0.3135\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.3294 - val_loss: 0.3351\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3256 - val_loss: 0.3606\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3207 - val_loss: 0.3094\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.3207 - val_loss: 0.3074\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.3210 - val_loss: 0.2886\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3136 - val_loss: 0.2925\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.2960 - val_loss: 0.2831\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.3023 - val_loss: 0.2821\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.3048 - val_loss: 0.3009\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.2930 - val_loss: 0.4451\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.2909 - val_loss: 0.3226\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.2827 - val_loss: 0.3190\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2890 - val_loss: 0.3109\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.2823 - val_loss: 0.3016\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.2837 - val_loss: 0.2929\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.2806 - val_loss: 0.2799\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.2829 - val_loss: 0.3112\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.2746 - val_loss: 0.2908\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.2741 - val_loss: 0.2923\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.2721 - val_loss: 0.2941\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.2668 - val_loss: 0.2927\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.2646 - val_loss: 0.4787\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.2618 - val_loss: 0.2875\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.2613 - val_loss: 0.2818\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.2623 - val_loss: 0.2830\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2602 - val_loss: 0.2965\n",
      "121/121 [==============================] - 0s 540us/step - loss: 0.3231\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.7090 - val_loss: 0.4356\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.4782 - val_loss: 0.4087\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.4285 - val_loss: 0.6720\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.4165 - val_loss: 0.3512\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.3976 - val_loss: 0.3466\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3907 - val_loss: 0.3444\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3758 - val_loss: 0.3835\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3554 - val_loss: 0.3623\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.3542 - val_loss: 0.3110\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.3497 - val_loss: 0.3147\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.3469 - val_loss: 0.3547\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3408 - val_loss: 0.3049\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 639us/step - loss: 0.3333 - val_loss: 0.3449\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 639us/step - loss: 0.3227 - val_loss: 0.3057\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.3183 - val_loss: 0.3356\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.3195 - val_loss: 0.2872\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.3194 - val_loss: 0.2785\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3104 - val_loss: 0.3391\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.3069 - val_loss: 0.2921\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3086 - val_loss: 0.2912\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.3058 - val_loss: 0.2843\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 637us/step - loss: 0.2985 - val_loss: 0.3996\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3025 - val_loss: 0.2772\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.2981 - val_loss: 0.3075\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.2910 - val_loss: 0.2801\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2902 - val_loss: 0.2769\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2949 - val_loss: 0.2811\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2868 - val_loss: 0.2820\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.2844 - val_loss: 0.2996\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.2835 - val_loss: 0.2702\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.2792 - val_loss: 0.2798\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.2815 - val_loss: 0.3117\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.2749 - val_loss: 0.3034\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.2731 - val_loss: 0.2847\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.2772 - val_loss: 0.2721\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2722 - val_loss: 0.3072\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.2718 - val_loss: 0.2873\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.2686 - val_loss: 0.3192\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.2651 - val_loss: 0.2895\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.2616 - val_loss: 0.2749\n",
      "121/121 [==============================] - 0s 378us/step - loss: 0.2955\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.7248 - val_loss: 0.4071\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.4749 - val_loss: 0.5025\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.4427 - val_loss: 0.6567\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.4263 - val_loss: 0.5646\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.3997 - val_loss: 0.3753\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.4075 - val_loss: 0.3514\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3815 - val_loss: 0.3204\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3629 - val_loss: 0.3049\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3579 - val_loss: 0.3180\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3577 - val_loss: 0.3132\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.3442 - val_loss: 0.3086\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.3409 - val_loss: 0.3468\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.3351 - val_loss: 0.3037\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.3321 - val_loss: 0.3058\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.3248 - val_loss: 0.3025\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.3248 - val_loss: 0.3067\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3197 - val_loss: 0.3064\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.3160 - val_loss: 0.2814\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.3143 - val_loss: 0.3339\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3119 - val_loss: 0.2866\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3076 - val_loss: 0.2848\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2998 - val_loss: 0.3080\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.2948 - val_loss: 0.2930\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.3054 - val_loss: 0.2873\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2958 - val_loss: 0.2789\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.2953 - val_loss: 0.2843\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.2903 - val_loss: 0.3057\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.2905 - val_loss: 0.3099\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.2810 - val_loss: 0.2898\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.2895 - val_loss: 0.2686\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2858 - val_loss: 0.3905\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2846 - val_loss: 0.3198\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.2818 - val_loss: 0.3040\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.2806 - val_loss: 0.2835\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.2776 - val_loss: 0.2806\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.2700 - val_loss: 0.2942\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.2708 - val_loss: 0.3066\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.2719 - val_loss: 0.2840\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.2663 - val_loss: 0.2818\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.2702 - val_loss: 0.2779\n",
      "121/121 [==============================] - 0s 479us/step - loss: 0.2861\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.7455 - val_loss: 0.4627\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4665 - val_loss: 0.4224\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.4146 - val_loss: 0.4681\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3948 - val_loss: 0.3371\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.3697 - val_loss: 0.3419\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.3660 - val_loss: 0.3566\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.3555 - val_loss: 0.3308\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.3468 - val_loss: 0.3258\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.3346 - val_loss: 0.3146\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.3332 - val_loss: 0.3063\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.3271 - val_loss: 0.3044\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.3267 - val_loss: 0.3159\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.3178 - val_loss: 0.3011\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.3173 - val_loss: 0.2989\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3072 - val_loss: 0.2961\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.3054 - val_loss: 0.3026\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3015 - val_loss: 0.3136\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 610us/step - loss: 0.3027 - val_loss: 0.3051\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.2971 - val_loss: 0.3174\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.2982 - val_loss: 0.2838\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.2949 - val_loss: 0.3318\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 637us/step - loss: 0.2853 - val_loss: 0.3203\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.2890 - val_loss: 0.2890\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.2853 - val_loss: 0.2934\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.2844 - val_loss: 0.2948\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.2808 - val_loss: 0.2999\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.2767 - val_loss: 0.2905\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2787 - val_loss: 0.2920\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2767 - val_loss: 0.2992\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2743 - val_loss: 0.2837\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.2770 - val_loss: 0.2828\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.2683 - val_loss: 0.2914\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.2633 - val_loss: 0.2842\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.2682 - val_loss: 0.3004\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2675 - val_loss: 0.3053\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.2672 - val_loss: 0.3536\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 637us/step - loss: 0.2600 - val_loss: 0.2754\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.2596 - val_loss: 0.2763\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.2555 - val_loss: 0.2883\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2612 - val_loss: 0.2901\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.2577 - val_loss: 0.4191\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 613us/step - loss: 0.2566 - val_loss: 0.2804\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.2514 - val_loss: 0.2886\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.2531 - val_loss: 0.2991\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.2521 - val_loss: 0.2835\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 614us/step - loss: 0.2527 - val_loss: 0.2925\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.2486 - val_loss: 0.2917\n",
      "121/121 [==============================] - 0s 375us/step - loss: 0.3152\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.6912 - val_loss: 0.4393\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.4667 - val_loss: 0.4196\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.4172 - val_loss: 0.3584\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.4119 - val_loss: 0.3504\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3816 - val_loss: 0.3433\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.3764 - val_loss: 0.3243\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.3616 - val_loss: 0.3255\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.3549 - val_loss: 0.3409\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.3484 - val_loss: 0.3410\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.3394 - val_loss: 0.3253\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 635us/step - loss: 0.3306 - val_loss: 0.3497\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3311 - val_loss: 0.3708\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.3308 - val_loss: 0.3032\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3244 - val_loss: 0.2987\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3198 - val_loss: 0.3511\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 637us/step - loss: 0.3164 - val_loss: 0.3043\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.3125 - val_loss: 0.2874\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.3113 - val_loss: 0.3473\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 635us/step - loss: 0.3135 - val_loss: 0.3143\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.2997 - val_loss: 0.2971\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.3000 - val_loss: 0.2846\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 639us/step - loss: 0.2995 - val_loss: 0.2852\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2938 - val_loss: 0.2852\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.2902 - val_loss: 0.3310\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.2914 - val_loss: 0.3116\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2864 - val_loss: 0.2830\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.2952 - val_loss: 0.2814\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.2814 - val_loss: 0.2915\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.2880 - val_loss: 0.3041\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 613us/step - loss: 0.2863 - val_loss: 0.2767\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 637us/step - loss: 0.2829 - val_loss: 0.2813\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 621us/step - loss: 0.2793 - val_loss: 0.2811\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2810 - val_loss: 0.3493\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.2773 - val_loss: 0.2793\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.2732 - val_loss: 0.3618\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.2728 - val_loss: 0.2943\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.2717 - val_loss: 0.2776\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.2706 - val_loss: 0.2889\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.2676 - val_loss: 0.2807\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.2739 - val_loss: 0.2743\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2694 - val_loss: 0.2765\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.2682 - val_loss: 0.2804\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.2583 - val_loss: 0.2818\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2614 - val_loss: 0.2755\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.2601 - val_loss: 0.2743\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2592 - val_loss: 0.2751\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 615us/step - loss: 0.2563 - val_loss: 0.2808\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 621us/step - loss: 0.2619 - val_loss: 0.2787\n",
      "Epoch 49/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.2522 - val_loss: 0.2712\n",
      "Epoch 50/128\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.2554 - val_loss: 0.2914\n",
      "Epoch 51/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2536 - val_loss: 0.2873\n",
      "Epoch 52/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.2533 - val_loss: 0.2733\n",
      "Epoch 53/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2511 - val_loss: 0.2868\n",
      "Epoch 54/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.2487 - val_loss: 0.3225\n",
      "Epoch 55/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.2516 - val_loss: 0.2794\n",
      "Epoch 56/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2463 - val_loss: 0.2701\n",
      "Epoch 57/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2418 - val_loss: 0.2811\n",
      "Epoch 58/128\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.2438 - val_loss: 0.2759\n",
      "Epoch 59/128\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.2464 - val_loss: 0.2790\n",
      "Epoch 60/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.2454 - val_loss: 0.3631\n",
      "Epoch 61/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2435 - val_loss: 0.2827\n",
      "Epoch 62/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.2389 - val_loss: 0.2855\n",
      "Epoch 63/128\n",
      "242/242 [==============================] - 0s 637us/step - loss: 0.2427 - val_loss: 0.3251\n",
      "Epoch 64/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2422 - val_loss: 0.2890\n",
      "Epoch 65/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.2391 - val_loss: 0.2899\n",
      "Epoch 66/128\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.2350 - val_loss: 0.2927\n",
      "121/121 [==============================] - 0s 376us/step - loss: 0.3051\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.7304 - val_loss: 0.4591\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.4799 - val_loss: 0.4169\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.4284 - val_loss: 0.3835\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.4134 - val_loss: 0.4956\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.3910 - val_loss: 0.4891\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.3889 - val_loss: 0.3398\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3759 - val_loss: 0.3526\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.3652 - val_loss: 0.3175\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 617us/step - loss: 0.3570 - val_loss: 0.3179\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.3550 - val_loss: 0.3254\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 639us/step - loss: 0.3526 - val_loss: 0.3155\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.3421 - val_loss: 0.3710\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.3427 - val_loss: 0.3013\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3393 - val_loss: 0.3169\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.3318 - val_loss: 0.2967\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.3264 - val_loss: 0.3433\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 635us/step - loss: 0.3201 - val_loss: 0.3722\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 611us/step - loss: 0.3192 - val_loss: 0.3013\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3218 - val_loss: 0.3492\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3164 - val_loss: 0.3580\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.3115 - val_loss: 0.3297\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.3061 - val_loss: 0.2960\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.3052 - val_loss: 0.3190\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 615us/step - loss: 0.3030 - val_loss: 0.3369\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.3012 - val_loss: 0.2800\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.2973 - val_loss: 0.3376\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.2986 - val_loss: 0.2912\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 621us/step - loss: 0.2926 - val_loss: 0.2983\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.2925 - val_loss: 0.3272\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3010 - val_loss: 0.2969\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2950 - val_loss: 0.2860\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.2950 - val_loss: 0.2910\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.2870 - val_loss: 0.2813\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.2878 - val_loss: 0.2784\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.2801 - val_loss: 0.2786\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.2839 - val_loss: 0.2953\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.2819 - val_loss: 0.2848\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.2794 - val_loss: 0.3074\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.2762 - val_loss: 0.2959\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2781 - val_loss: 0.2879\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.2740 - val_loss: 0.2832\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.2784 - val_loss: 0.2841\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.2693 - val_loss: 0.2953\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.2674 - val_loss: 0.3596\n",
      "121/121 [==============================] - 0s 362us/step - loss: 0.3516\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.7409 - val_loss: 0.4492\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.4370 - val_loss: 0.4039\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.3961 - val_loss: 0.3644\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.3783 - val_loss: 0.3923\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3657 - val_loss: 0.3432\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3557 - val_loss: 0.3272\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.3453 - val_loss: 0.3262\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3395 - val_loss: 0.3209\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.3406 - val_loss: 0.3094\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 639us/step - loss: 0.3295 - val_loss: 0.3108\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.3263 - val_loss: 0.3466\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3192 - val_loss: 0.3104\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 639us/step - loss: 0.3112 - val_loss: 0.2947\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.3078 - val_loss: 0.3201\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.3063 - val_loss: 0.3028\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.2989 - val_loss: 0.3039\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.3005 - val_loss: 0.2962\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 621us/step - loss: 0.2945 - val_loss: 0.4049\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 639us/step - loss: 0.2985 - val_loss: 0.3028\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.2957 - val_loss: 0.2863\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2919 - val_loss: 0.3029\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2873 - val_loss: 0.2843\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.2881 - val_loss: 0.3178\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.2871 - val_loss: 0.2875\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.2824 - val_loss: 0.2856\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.2846 - val_loss: 0.3065\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.2812 - val_loss: 0.2933\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2767 - val_loss: 0.2834\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.2741 - val_loss: 0.3049\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2771 - val_loss: 0.2860\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.2752 - val_loss: 0.2793\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 618us/step - loss: 0.2765 - val_loss: 0.2899\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.2723 - val_loss: 0.3115\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2662 - val_loss: 0.2864\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.2704 - val_loss: 0.3027\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.2692 - val_loss: 0.3579\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.2669 - val_loss: 0.2822\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.3097 - val_loss: 0.3233\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2816 - val_loss: 0.2823\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.2745 - val_loss: 0.2923\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.2842 - val_loss: 0.2798\n",
      "121/121 [==============================] - 0s 436us/step - loss: 0.3213\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.7947 - val_loss: 0.4666\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.4820 - val_loss: 0.4071\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.4291 - val_loss: 0.3694\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.4056 - val_loss: 0.3695\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3980 - val_loss: 0.3477\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3809 - val_loss: 0.3409\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.3660 - val_loss: 0.3312\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3575 - val_loss: 0.3480\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3476 - val_loss: 0.3098\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.3408 - val_loss: 0.3480\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.3391 - val_loss: 0.3178\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.3359 - val_loss: 0.3407\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.3293 - val_loss: 0.3026\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.3305 - val_loss: 0.3100\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.3239 - val_loss: 0.2989\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3201 - val_loss: 0.2880\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.3147 - val_loss: 0.3091\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.3123 - val_loss: 0.2922\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.3073 - val_loss: 0.2895\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.3039 - val_loss: 0.3546\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3064 - val_loss: 0.2791\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.3050 - val_loss: 0.3020\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3008 - val_loss: 0.2816\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.2987 - val_loss: 0.2830\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2952 - val_loss: 0.2892\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.3013 - val_loss: 0.3095\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.2896 - val_loss: 0.3294\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.2927 - val_loss: 0.4723\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2916 - val_loss: 0.2960\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.2928 - val_loss: 0.2932\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2845 - val_loss: 0.2708\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.2820 - val_loss: 0.2773\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.2817 - val_loss: 0.2742\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.2835 - val_loss: 0.2760\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.2829 - val_loss: 0.2842\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.2721 - val_loss: 0.2768\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.2814 - val_loss: 0.2814\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.2814 - val_loss: 0.2731\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2751 - val_loss: 0.2857\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.2729 - val_loss: 0.2910\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.2688 - val_loss: 0.3277\n",
      "121/121 [==============================] - 0s 380us/step - loss: 0.3468\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.8521 - val_loss: 0.4748\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.4733 - val_loss: 0.4061\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 643us/step - loss: 0.4233 - val_loss: 0.3952\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 617us/step - loss: 0.3984 - val_loss: 0.3584\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.3865 - val_loss: 0.4391\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.3786 - val_loss: 0.3279\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.3704 - val_loss: 0.3167\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 639us/step - loss: 0.3607 - val_loss: 0.3610\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.3506 - val_loss: 0.3393\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 618us/step - loss: 0.3472 - val_loss: 0.4422\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.3424 - val_loss: 0.3128\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3410 - val_loss: 0.3026\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 621us/step - loss: 0.3293 - val_loss: 0.3601\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.3207 - val_loss: 0.2907\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.3253 - val_loss: 0.3086\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.3159 - val_loss: 0.3223\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3235 - val_loss: 0.2896\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.3130 - val_loss: 0.3007\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3151 - val_loss: 0.2983\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.3105 - val_loss: 0.3095\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.3093 - val_loss: 0.2969\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3068 - val_loss: 0.2981\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.3038 - val_loss: 0.2910\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.3009 - val_loss: 0.2857\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 621us/step - loss: 0.2963 - val_loss: 0.2811\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.3002 - val_loss: 0.2748\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.2930 - val_loss: 0.2922\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 615us/step - loss: 0.2942 - val_loss: 0.2925\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.2918 - val_loss: 0.2873\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.2896 - val_loss: 0.3092\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.2876 - val_loss: 0.2937\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 614us/step - loss: 0.2857 - val_loss: 0.2947\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2835 - val_loss: 0.2786\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 614us/step - loss: 0.2839 - val_loss: 0.2943\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.2833 - val_loss: 0.2944\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2845 - val_loss: 0.2969\n",
      "121/121 [==============================] - 0s 374us/step - loss: 0.2971\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7982 - val_loss: 0.4278\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.4920 - val_loss: 0.3944\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.4252 - val_loss: 0.4631\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.4100 - val_loss: 0.3466\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.4524 - val_loss: 0.7100\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.3946 - val_loss: 0.4857\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3955 - val_loss: 0.3909\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3719 - val_loss: 0.3128\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3521 - val_loss: 0.3070\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3416 - val_loss: 0.3227\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3316 - val_loss: 0.3264\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3250 - val_loss: 0.3160\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.3187 - val_loss: 0.3071\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.3218 - val_loss: 0.3208\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.3164 - val_loss: 0.2945\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.3100 - val_loss: 0.2858\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.3029 - val_loss: 0.3281\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2996 - val_loss: 0.2978\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.2942 - val_loss: 0.3897\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.2927 - val_loss: 0.3025\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.2937 - val_loss: 0.3067\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.2912 - val_loss: 0.3075\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.2843 - val_loss: 0.2887\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.2805 - val_loss: 0.2886\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.2786 - val_loss: 0.2863\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.2811 - val_loss: 0.2866\n",
      "121/121 [==============================] - 0s 396us/step - loss: 0.3308\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7677 - val_loss: 0.5294\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.5020 - val_loss: 0.5717\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.4408 - val_loss: 0.4775\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.4124 - val_loss: 0.8227\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3929 - val_loss: 0.4026\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.3722 - val_loss: 0.3222\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.3621 - val_loss: 0.3146\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.3529 - val_loss: 0.3150\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3429 - val_loss: 0.3251\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3407 - val_loss: 0.3503\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.3295 - val_loss: 0.2937\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.3340 - val_loss: 0.2940\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.3258 - val_loss: 0.3183\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.3188 - val_loss: 0.3036\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.3077 - val_loss: 0.3121\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3160 - val_loss: 0.3729\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.3042 - val_loss: 0.3332\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3097 - val_loss: 0.3324\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.2979 - val_loss: 0.3078\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2974 - val_loss: 0.3302\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.2973 - val_loss: 0.2727\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2950 - val_loss: 0.3115\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 793us/step - loss: 0.2855 - val_loss: 0.3344\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2852 - val_loss: 0.3300\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.2887 - val_loss: 0.2964\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.2849 - val_loss: 0.3068\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.2793 - val_loss: 0.2734\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.2804 - val_loss: 0.3903\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.2739 - val_loss: 0.3537\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.2809 - val_loss: 0.2999\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.2671 - val_loss: 0.2863\n",
      "121/121 [==============================] - 0s 398us/step - loss: 0.3163\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9512 - val_loss: 0.4501\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.5375 - val_loss: 0.4609\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.4562 - val_loss: 0.3645\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4632 - val_loss: 0.3598\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4135 - val_loss: 0.3261\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.3914 - val_loss: 0.3820\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3760 - val_loss: 0.3628\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.3688 - val_loss: 0.3374\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3610 - val_loss: 0.3172\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.3643 - val_loss: 0.3318\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3549 - val_loss: 0.3257\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3491 - val_loss: 0.3278\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3297 - val_loss: 0.3546\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3380 - val_loss: 0.2926\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3339 - val_loss: 0.3356\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3241 - val_loss: 0.3290\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.3230 - val_loss: 0.3036\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.3150 - val_loss: 0.2953\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3200 - val_loss: 0.2802\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.3092 - val_loss: 0.3061\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.3086 - val_loss: 0.4251\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3061 - val_loss: 0.2838\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.3057 - val_loss: 0.3268\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.2955 - val_loss: 0.3585\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.3046 - val_loss: 0.2873\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3004 - val_loss: 0.2818\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.2910 - val_loss: 0.3372\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.2930 - val_loss: 0.2921\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.2910 - val_loss: 0.3185\n",
      "121/121 [==============================] - 0s 394us/step - loss: 0.3233\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.7538 - val_loss: 0.4130\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4407 - val_loss: 0.3683\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.4002 - val_loss: 0.4255\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.3851 - val_loss: 0.4599\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.3660 - val_loss: 0.3659\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.3596 - val_loss: 0.3970\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.3487 - val_loss: 0.3340\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3394 - val_loss: 0.3150\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3414 - val_loss: 0.4258\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.3258 - val_loss: 0.3090\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.3227 - val_loss: 0.3201\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3216 - val_loss: 0.3454\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3156 - val_loss: 0.3560\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3135 - val_loss: 0.3086\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 705us/step - loss: 0.3043 - val_loss: 0.2948\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.3066 - val_loss: 0.2912\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3009 - val_loss: 0.3668\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3268 - val_loss: 0.3667\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3660 - val_loss: 0.3903\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 742us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 757us/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 384us/step - loss: nan\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.7614 - val_loss: 0.4314\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.4800 - val_loss: 0.3882\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.4303 - val_loss: 0.3537\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.4076 - val_loss: 0.3482\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.4030 - val_loss: 0.3512\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3774 - val_loss: 0.3422\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3685 - val_loss: 0.3524\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3667 - val_loss: 0.3117\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3516 - val_loss: 0.3117\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.3440 - val_loss: 0.3092\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3398 - val_loss: 0.3190\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.3285 - val_loss: 0.3194\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.3200 - val_loss: 0.2953\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.3293 - val_loss: 0.3123\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3133 - val_loss: 0.2984\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.3165 - val_loss: 0.2943\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.3092 - val_loss: 0.2937\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.3120 - val_loss: 0.2879\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3076 - val_loss: 0.3104\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3064 - val_loss: 0.3484\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.3049 - val_loss: 0.2781\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3003 - val_loss: 0.2944\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.2974 - val_loss: 0.2860\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.2879 - val_loss: 0.2909\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.2877 - val_loss: 0.2855\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.2977 - val_loss: 0.3116\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.2869 - val_loss: 0.3385\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.2900 - val_loss: 0.4241\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.2825 - val_loss: 0.2869\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.2841 - val_loss: 0.2845\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2837 - val_loss: 0.2869\n",
      "121/121 [==============================] - 0s 382us/step - loss: 0.3110\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7428 - val_loss: 0.4400\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.4781 - val_loss: 0.3819\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.4222 - val_loss: 0.3705\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.4049 - val_loss: 0.3441\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.3911 - val_loss: 0.3557\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.3695 - val_loss: 0.3440\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3706 - val_loss: 0.3115\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.3480 - val_loss: 0.3943\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3569 - val_loss: 0.3669\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3399 - val_loss: 0.3129\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.3380 - val_loss: 0.3231\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.3356 - val_loss: 0.3830\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.3341 - val_loss: 0.3107\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.3214 - val_loss: 0.2923\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3247 - val_loss: 0.3074\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3166 - val_loss: 0.2940\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.3175 - val_loss: 0.3197\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.3140 - val_loss: 0.3370\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.3051 - val_loss: 0.2883\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3035 - val_loss: 0.3420\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.3023 - val_loss: 0.2910\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.3058 - val_loss: 0.2811\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.2966 - val_loss: 0.3415\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3001 - val_loss: 0.2991\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.2922 - val_loss: 0.2878\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.2957 - val_loss: 0.2765\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2842 - val_loss: 0.2945\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2862 - val_loss: 0.2738\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.2920 - val_loss: 0.2873\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.2892 - val_loss: 0.3227\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2874 - val_loss: 0.3233\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.2800 - val_loss: 0.2838\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.2784 - val_loss: 0.2795\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.2786 - val_loss: 0.4192\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.2773 - val_loss: 0.3823\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.2772 - val_loss: 0.2805\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.2765 - val_loss: 0.2846\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.2714 - val_loss: 0.2812\n",
      "121/121 [==============================] - 0s 516us/step - loss: 0.2911\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8211 - val_loss: 0.4224\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 1.8832 - val_loss: 1.2381\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.6387 - val_loss: 0.3780\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.4147 - val_loss: 0.3529\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.5433 - val_loss: 0.3437\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.3980 - val_loss: 0.4159\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.3697 - val_loss: 0.4018\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3684 - val_loss: 0.3220\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.3550 - val_loss: 0.4121\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3382 - val_loss: 0.3681\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3383 - val_loss: 0.3044\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.3406 - val_loss: 0.3164\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.3233 - val_loss: 0.3446\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.3268 - val_loss: 0.3162\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3166 - val_loss: 0.3201\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.3179 - val_loss: 0.3083\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.3155 - val_loss: 0.3079\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3094 - val_loss: 0.2960\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.3048 - val_loss: 0.3983\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.2996 - val_loss: 0.2959\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.2991 - val_loss: 0.3053\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.2955 - val_loss: 0.2858\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.2985 - val_loss: 0.3108\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.2975 - val_loss: 0.2837\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.2915 - val_loss: 0.2895\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2881 - val_loss: 0.2910\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.2853 - val_loss: 0.2801\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.2799 - val_loss: 0.2730\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.2835 - val_loss: 0.2968\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.2818 - val_loss: 0.2913\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2806 - val_loss: 0.2944\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.2819 - val_loss: 0.2810\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2769 - val_loss: 0.2896\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.2722 - val_loss: 0.3549\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2732 - val_loss: 0.2719\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.2700 - val_loss: 0.2936\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.2739 - val_loss: 0.2926\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.2677 - val_loss: 0.2908\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.2713 - val_loss: 0.3056\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.2665 - val_loss: 0.2907\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2622 - val_loss: 0.2969\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 775us/step - loss: 0.2619 - val_loss: 0.2807\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.2628 - val_loss: 0.3021\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2593 - val_loss: 0.2790\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.2610 - val_loss: 0.2706\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.2637 - val_loss: 0.2787\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.2565 - val_loss: 0.2782\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.2607 - val_loss: 0.2829\n",
      "Epoch 49/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.2579 - val_loss: 0.3204\n",
      "Epoch 50/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.2500 - val_loss: 0.2742\n",
      "Epoch 51/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.2493 - val_loss: 0.2718\n",
      "Epoch 52/128\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.2516 - val_loss: 0.2781\n",
      "Epoch 53/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2491 - val_loss: 0.2817\n",
      "Epoch 54/128\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2516 - val_loss: 0.2654\n",
      "Epoch 55/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2461 - val_loss: 0.2802\n",
      "Epoch 56/128\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.2497 - val_loss: 0.2864\n",
      "Epoch 57/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2486 - val_loss: 0.2756\n",
      "Epoch 58/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2472 - val_loss: 0.2719\n",
      "Epoch 59/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2434 - val_loss: 0.2804\n",
      "Epoch 60/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.2442 - val_loss: 0.2782\n",
      "Epoch 61/128\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.2441 - val_loss: 0.2755\n",
      "Epoch 62/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.2442 - val_loss: 0.2839\n",
      "Epoch 63/128\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.2422 - val_loss: 0.2762\n",
      "Epoch 64/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2429 - val_loss: 0.2748\n",
      "121/121 [==============================] - 0s 536us/step - loss: 0.3087\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7503 - val_loss: 0.6094\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.4972 - val_loss: 0.4548\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.4405 - val_loss: 0.4048\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.4076 - val_loss: 0.3508\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3903 - val_loss: 0.3853\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.3768 - val_loss: 0.3308\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.3659 - val_loss: 0.3124\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3651 - val_loss: 0.3306\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3502 - val_loss: 0.3236\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.3571 - val_loss: 0.3294\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.3473 - val_loss: 0.3226\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3360 - val_loss: 0.3573\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.3282 - val_loss: 0.4464\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3264 - val_loss: 0.2988\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3259 - val_loss: 0.3291\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3118 - val_loss: 0.2966\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3157 - val_loss: 0.3118\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3074 - val_loss: 0.2880\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.3074 - val_loss: 0.3136\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.3034 - val_loss: 0.2920\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.2989 - val_loss: 0.3009\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.2986 - val_loss: 0.2849\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2938 - val_loss: 0.2885\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.2922 - val_loss: 0.2783\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.2927 - val_loss: 0.2966\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.2855 - val_loss: 0.2804\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.2910 - val_loss: 0.2900\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.2837 - val_loss: 0.3015\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2838 - val_loss: 0.2851\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.2820 - val_loss: 0.3725\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.2782 - val_loss: 0.2807\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.2744 - val_loss: 0.3111\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.2726 - val_loss: 0.2841\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.2714 - val_loss: 0.3139\n",
      "121/121 [==============================] - 0s 384us/step - loss: 0.3321\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7828 - val_loss: 0.4451\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4763 - val_loss: 0.3745\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.4475 - val_loss: 0.3528\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.4296 - val_loss: 0.3569\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.3990 - val_loss: 0.3229\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.3846 - val_loss: 0.3464\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3779 - val_loss: 0.4008\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.3695 - val_loss: 0.3409\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.3659 - val_loss: 0.3121\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3547 - val_loss: 0.4892\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3604 - val_loss: 0.3148\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3454 - val_loss: 0.3536\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.3421 - val_loss: 0.3123\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.3383 - val_loss: 0.3120\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.3347 - val_loss: 0.3149\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.3313 - val_loss: 0.3231\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3310 - val_loss: 0.3012\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.3178 - val_loss: 0.2973\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.3193 - val_loss: 0.3064\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.3136 - val_loss: 0.2903\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3105 - val_loss: 0.3775\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.3111 - val_loss: 0.2958\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.3085 - val_loss: 0.3234\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3088 - val_loss: 0.2969\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3031 - val_loss: 0.2956\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.2968 - val_loss: 0.2875\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.3036 - val_loss: 0.3260\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.2976 - val_loss: 0.2862\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2931 - val_loss: 0.3407\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.2956 - val_loss: 0.2838\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.2902 - val_loss: 0.2840\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2865 - val_loss: 0.2767\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.2895 - val_loss: 0.3193\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.2796 - val_loss: 0.2775\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2830 - val_loss: 0.2729\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2818 - val_loss: 0.2766\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.2847 - val_loss: 0.2806\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.2838 - val_loss: 0.3202\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.2791 - val_loss: 0.2786\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.2718 - val_loss: 0.3024\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2716 - val_loss: 0.2870\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.2690 - val_loss: 0.3041\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 679us/step - loss: 0.2711 - val_loss: 0.2978\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.2696 - val_loss: 0.2880\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.2675 - val_loss: 0.3223\n",
      "121/121 [==============================] - 0s 507us/step - loss: 0.3244\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 934us/step - loss: 1.0093 - val_loss: 0.4472\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.6985 - val_loss: 0.4333\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.4297 - val_loss: 0.3555\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.3935 - val_loss: 0.3599\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.3716 - val_loss: 0.4077\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.3566 - val_loss: 0.7587\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.3487 - val_loss: 0.3251\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.3359 - val_loss: 0.3927\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 608us/step - loss: 0.3319 - val_loss: 0.3092\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.3309 - val_loss: 0.2993\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3108 - val_loss: 0.3085\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3080 - val_loss: 0.2984\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 608us/step - loss: 0.3121 - val_loss: 0.2855\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.3045 - val_loss: 0.2866\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 617us/step - loss: 0.3007 - val_loss: 0.2838\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.2959 - val_loss: 0.3275\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.2918 - val_loss: 0.4594\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.2972 - val_loss: 0.2816\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.2969 - val_loss: 0.2820\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.2914 - val_loss: 0.2816\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.2843 - val_loss: 0.2760\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.2870 - val_loss: 0.4960\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.2855 - val_loss: 0.3139\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2811 - val_loss: 0.2857\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.2807 - val_loss: 0.2852\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.2740 - val_loss: 0.2748\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.2750 - val_loss: 0.2924\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.2758 - val_loss: 0.2907\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2740 - val_loss: 0.2754\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 604us/step - loss: 0.2725 - val_loss: 0.3055\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.2754 - val_loss: 0.4270\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2727 - val_loss: 0.2783\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 613us/step - loss: 0.2686 - val_loss: 0.3085\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2712 - val_loss: 0.2927\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.2660 - val_loss: 0.2742\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2628 - val_loss: 0.3685\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.2678 - val_loss: 0.2873\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.2587 - val_loss: 0.2797\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.2650 - val_loss: 0.2751\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.2624 - val_loss: 0.2737\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2601 - val_loss: 0.2741\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.2604 - val_loss: 0.2698\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 608us/step - loss: 0.2554 - val_loss: 0.2831\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.2581 - val_loss: 0.2737\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 604us/step - loss: 0.2539 - val_loss: 0.2994\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.2571 - val_loss: 0.2743\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.2524 - val_loss: 0.2917\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.2534 - val_loss: 0.2767\n",
      "Epoch 49/128\n",
      "242/242 [==============================] - 0s 604us/step - loss: 0.2483 - val_loss: 0.2887\n",
      "Epoch 50/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.2469 - val_loss: 0.3124\n",
      "Epoch 51/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2512 - val_loss: 0.2780\n",
      "Epoch 52/128\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.2506 - val_loss: 0.2747\n",
      "121/121 [==============================] - 0s 372us/step - loss: 0.3087\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.7303 - val_loss: 0.4266\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.4714 - val_loss: 0.7843\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 621us/step - loss: 0.4242 - val_loss: 0.3553\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.3994 - val_loss: 0.3462\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.3967 - val_loss: 0.3373\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.3714 - val_loss: 0.3346\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.3524 - val_loss: 0.3925\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3574 - val_loss: 0.3462\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3480 - val_loss: 0.4169\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.3415 - val_loss: 0.3055\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.3386 - val_loss: 0.3033\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.3290 - val_loss: 0.2995\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3221 - val_loss: 0.2904\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.3242 - val_loss: 0.3074\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.3197 - val_loss: 0.3032\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 613us/step - loss: 0.3213 - val_loss: 0.2843\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.3154 - val_loss: 0.2961\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.3078 - val_loss: 0.2857\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.3024 - val_loss: 0.2926\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.3054 - val_loss: 0.2884\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.2986 - val_loss: 0.3207\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.2947 - val_loss: 0.3118\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2962 - val_loss: 0.2899\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.2932 - val_loss: 0.2990\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.2925 - val_loss: 0.3438\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.2890 - val_loss: 0.2818\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2905 - val_loss: 0.3770\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2853 - val_loss: 0.2931\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 621us/step - loss: 0.2834 - val_loss: 0.2810\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.2855 - val_loss: 0.2867\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.2821 - val_loss: 0.2735\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2807 - val_loss: 0.3065\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 604us/step - loss: 0.2795 - val_loss: 0.2817\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.2715 - val_loss: 0.2891\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.2736 - val_loss: 0.2858\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2700 - val_loss: 0.2707\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.2681 - val_loss: 0.2735\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.2656 - val_loss: 0.2885\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 603us/step - loss: 0.2610 - val_loss: 0.2945\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2663 - val_loss: 0.2978\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.2633 - val_loss: 0.2741\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.2657 - val_loss: 0.3153\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 619us/step - loss: 0.2678 - val_loss: 0.2875\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.2593 - val_loss: 0.2962\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 604us/step - loss: 0.2633 - val_loss: 0.2834\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.2587 - val_loss: 0.3283\n",
      "121/121 [==============================] - 0s 381us/step - loss: 0.3332\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.6941 - val_loss: 0.4261\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.4526 - val_loss: 0.4386\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.4240 - val_loss: 0.3620\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 620us/step - loss: 0.4073 - val_loss: 0.3358\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.3943 - val_loss: 0.3304\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 603us/step - loss: 0.3888 - val_loss: 0.3286\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.3730 - val_loss: 0.3148\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.3596 - val_loss: 0.3187\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 611us/step - loss: 0.3526 - val_loss: 0.3173\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.3558 - val_loss: 0.3279\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 608us/step - loss: 0.3429 - val_loss: 0.2952\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.3403 - val_loss: 0.3551\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.3374 - val_loss: 0.2949\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.3284 - val_loss: 0.2938\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3312 - val_loss: 0.2996\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 610us/step - loss: 0.3353 - val_loss: 0.2891\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.3209 - val_loss: 0.2978\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.3158 - val_loss: 0.2752\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.3137 - val_loss: 0.2941\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.3072 - val_loss: 0.3508\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3116 - val_loss: 0.3044\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.3091 - val_loss: 0.2932\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.3053 - val_loss: 0.3234\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.3048 - val_loss: 0.2776\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.3001 - val_loss: 0.2868\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 598us/step - loss: 0.2978 - val_loss: 0.2830\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.2928 - val_loss: 0.2919\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 600us/step - loss: 0.2959 - val_loss: 0.3395\n",
      "121/121 [==============================] - 0s 374us/step - loss: 0.3596\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.7373 - val_loss: 0.4444\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.4809 - val_loss: 0.4024\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.4325 - val_loss: 0.4218\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4054 - val_loss: 0.3838\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3855 - val_loss: 0.3292\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.3691 - val_loss: 0.3344\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3626 - val_loss: 0.3283\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.3525 - val_loss: 0.3234\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.3445 - val_loss: 0.3611\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.3362 - val_loss: 0.3215\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3299 - val_loss: 0.3135\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.3258 - val_loss: 0.3854\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3205 - val_loss: 0.2922\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3144 - val_loss: 0.3038\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3109 - val_loss: 0.2989\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.3165 - val_loss: 0.3075\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.2987 - val_loss: 0.2889\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.2966 - val_loss: 0.2826\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 715us/step - loss: 0.2969 - val_loss: 0.3033\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.2966 - val_loss: 0.2912\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.2926 - val_loss: 0.2933\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.2923 - val_loss: 0.2929\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.2843 - val_loss: 0.2929\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.2870 - val_loss: 0.3162\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.2831 - val_loss: 0.2844\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.2831 - val_loss: 0.2914\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.2793 - val_loss: 0.2820\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.2789 - val_loss: 0.2816\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.2772 - val_loss: 0.2793\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.2775 - val_loss: 0.2904\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.2753 - val_loss: 0.3852\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.2755 - val_loss: 0.2754\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.2706 - val_loss: 0.3068\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2710 - val_loss: 0.3317\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.2734 - val_loss: 0.2935\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.2674 - val_loss: 0.2860\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.2624 - val_loss: 0.2985\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.2652 - val_loss: 0.2778\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.2623 - val_loss: 0.3239\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.2624 - val_loss: 0.2827\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2601 - val_loss: 0.2962\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2620 - val_loss: 0.2957\n",
      "121/121 [==============================] - 0s 390us/step - loss: 0.3143\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.7608 - val_loss: 0.4345\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.5145 - val_loss: 0.7809\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.4559 - val_loss: 0.4867\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.4061 - val_loss: 0.3365\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.3985 - val_loss: 0.3686\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3842 - val_loss: 0.3239\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3762 - val_loss: 0.3563\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.3643 - val_loss: 0.3426\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.3580 - val_loss: 0.4452\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.3448 - val_loss: 0.3044\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 658us/step - loss: 0.3429 - val_loss: 0.3494\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3403 - val_loss: 0.2998\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3424 - val_loss: 0.2951\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3255 - val_loss: 0.3003\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.3244 - val_loss: 0.2925\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.3188 - val_loss: 0.3038\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3091 - val_loss: 0.2875\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.3299 - val_loss: 0.3095\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.3098 - val_loss: 0.3123\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.3126 - val_loss: 0.4281\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.3080 - val_loss: 0.2944\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3054 - val_loss: 0.2817\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.3000 - val_loss: 0.4106\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.2983 - val_loss: 0.2928\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.2925 - val_loss: 0.2925\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.3017 - val_loss: 0.2894\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.2913 - val_loss: 0.3072\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.2948 - val_loss: 0.2799\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.2838 - val_loss: 0.2757\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.2819 - val_loss: 0.2827\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.2838 - val_loss: 0.3285\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.2857 - val_loss: 0.2936\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.2827 - val_loss: 0.3762\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.2796 - val_loss: 0.3157\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.2765 - val_loss: 0.3197\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.2815 - val_loss: 0.4183\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.2752 - val_loss: 0.2838\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2727 - val_loss: 0.2789\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.2726 - val_loss: 0.2866\n",
      "121/121 [==============================] - 0s 386us/step - loss: 0.3064\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7836 - val_loss: 0.5845\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.4968 - val_loss: 0.3653\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.4704 - val_loss: 0.3569\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.4154 - val_loss: 0.3777\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.4121 - val_loss: 0.3556\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3956 - val_loss: 0.3323\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.3968 - val_loss: 0.3457\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3711 - val_loss: 0.3138\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.3716 - val_loss: 0.3996\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.3685 - val_loss: 0.3204\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.3519 - val_loss: 0.4730\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3527 - val_loss: 0.3202\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.3539 - val_loss: 0.3393\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.3448 - val_loss: 0.3011\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3436 - val_loss: 0.3160\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 779us/step - loss: 0.3417 - val_loss: 0.3098\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3289 - val_loss: 0.2890\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.3303 - val_loss: 0.4198\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3295 - val_loss: 0.2998\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.3221 - val_loss: 0.2876\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.3189 - val_loss: 0.2930\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.3112 - val_loss: 0.3842\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.3105 - val_loss: 0.3569\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.3133 - val_loss: 0.2792\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3049 - val_loss: 0.2785\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.3105 - val_loss: 0.3105\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3021 - val_loss: 0.3038\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3057 - val_loss: 0.2830\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.2993 - val_loss: 0.3092\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.2966 - val_loss: 0.2828\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.2919 - val_loss: 0.3240\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2924 - val_loss: 0.2922\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2938 - val_loss: 0.2762\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2914 - val_loss: 0.2921\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.2896 - val_loss: 0.2781\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.2829 - val_loss: 0.2858\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.2827 - val_loss: 0.2969\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.2808 - val_loss: 0.2827\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.2787 - val_loss: 0.2990\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.2819 - val_loss: 0.2961\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.2826 - val_loss: 0.2734\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.2759 - val_loss: 0.2923\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.2771 - val_loss: 0.3103\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.2746 - val_loss: 0.2978\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.2728 - val_loss: 0.3271\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.2772 - val_loss: 0.2899\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.2679 - val_loss: 0.3061\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2724 - val_loss: 0.2907\n",
      "Epoch 49/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.2713 - val_loss: 0.2716\n",
      "Epoch 50/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.2688 - val_loss: 0.2721\n",
      "Epoch 51/128\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.2637 - val_loss: 0.2771\n",
      "Epoch 52/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2657 - val_loss: 0.2805\n",
      "Epoch 53/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.2595 - val_loss: 0.2667\n",
      "Epoch 54/128\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.2584 - val_loss: 0.2899\n",
      "Epoch 55/128\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.2610 - val_loss: 0.2750\n",
      "Epoch 56/128\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.2634 - val_loss: 0.2750\n",
      "Epoch 57/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.2572 - val_loss: 0.2966\n",
      "Epoch 58/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.2559 - val_loss: 0.2736\n",
      "Epoch 59/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.2601 - val_loss: 0.2752\n",
      "Epoch 60/128\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.2541 - val_loss: 0.2778\n",
      "Epoch 61/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.2524 - val_loss: 0.2917\n",
      "Epoch 62/128\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.2530 - val_loss: 0.2799\n",
      "Epoch 63/128\n",
      "242/242 [==============================] - 0s 753us/step - loss: 0.2526 - val_loss: 0.2716\n",
      "121/121 [==============================] - 0s 378us/step - loss: 0.2795\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.6734 - val_loss: 0.4024\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.4316 - val_loss: 0.3574\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.3977 - val_loss: 0.7519\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3951 - val_loss: 0.3531\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.3778 - val_loss: 0.3263\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.3713 - val_loss: 0.3426\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3507 - val_loss: 0.3459\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3524 - val_loss: 0.3084\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.3426 - val_loss: 0.3462\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.3373 - val_loss: 0.3283\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.3281 - val_loss: 0.2992\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3245 - val_loss: 0.3321\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3199 - val_loss: 0.2995\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.3215 - val_loss: 0.3197\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3130 - val_loss: 0.2941\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.3128 - val_loss: 0.2875\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.3051 - val_loss: 0.2949\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.3023 - val_loss: 0.3058\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 635us/step - loss: 0.2947 - val_loss: 0.2926\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.2991 - val_loss: 0.3296\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.2969 - val_loss: 0.3420\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.2879 - val_loss: 0.2955\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.2902 - val_loss: 0.2760\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.2818 - val_loss: 0.2837\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.2786 - val_loss: 0.2907\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.2785 - val_loss: 0.2772\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.2780 - val_loss: 0.2895\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.2750 - val_loss: 0.2980\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.2749 - val_loss: 0.2992\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.2763 - val_loss: 0.2884\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.2754 - val_loss: 0.2724\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2716 - val_loss: 0.2922\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 635us/step - loss: 0.2700 - val_loss: 0.2774\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.2699 - val_loss: 0.2927\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2671 - val_loss: 0.2903\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.2672 - val_loss: 0.2847\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 676us/step - loss: 0.2618 - val_loss: 0.3226\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 635us/step - loss: 0.2609 - val_loss: 0.3167\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2670 - val_loss: 0.2743\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 634us/step - loss: 0.2620 - val_loss: 0.2800\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.2597 - val_loss: 0.3000\n",
      "121/121 [==============================] - 0s 378us/step - loss: 0.3334\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.7184 - val_loss: 0.4394\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.4600 - val_loss: 0.3914\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.4272 - val_loss: 0.4250\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.3993 - val_loss: 0.3432\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3910 - val_loss: 0.3298\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 631us/step - loss: 0.3825 - val_loss: 0.3511\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.3673 - val_loss: 0.3252\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.3632 - val_loss: 0.3473\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.3527 - val_loss: 0.3177\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3437 - val_loss: 0.3170\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3388 - val_loss: 0.3222\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.3363 - val_loss: 0.3003\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3255 - val_loss: 0.3274\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 707us/step - loss: 0.3309 - val_loss: 0.3270\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.3257 - val_loss: 0.3599\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.3202 - val_loss: 0.3036\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3139 - val_loss: 0.3213\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.3162 - val_loss: 0.2994\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.3124 - val_loss: 0.3735\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.3108 - val_loss: 0.2988\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3054 - val_loss: 0.2842\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.3108 - val_loss: 0.3451\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 652us/step - loss: 0.3010 - val_loss: 0.3076\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.2999 - val_loss: 0.3701\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.2986 - val_loss: 0.3132\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 688us/step - loss: 0.2995 - val_loss: 0.2898\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.2979 - val_loss: 0.3013\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.2910 - val_loss: 0.2906\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.2934 - val_loss: 0.3162\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.2887 - val_loss: 0.2988\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 645us/step - loss: 0.2875 - val_loss: 0.2981\n",
      "121/121 [==============================] - 0s 517us/step - loss: 0.3163\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7919 - val_loss: 0.4600\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.5005 - val_loss: 0.3937\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.4571 - val_loss: 0.4013\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.4260 - val_loss: 0.4059\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.4092 - val_loss: 0.3379\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.3934 - val_loss: 0.3400\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.3744 - val_loss: 0.3209\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3787 - val_loss: 0.3327\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.3617 - val_loss: 0.3760\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.3544 - val_loss: 0.4549\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 637us/step - loss: 0.3477 - val_loss: 0.3194\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.3404 - val_loss: 0.3026\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.3399 - val_loss: 0.3643\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.3233 - val_loss: 0.2920\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.3317 - val_loss: 0.3114\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.3258 - val_loss: 0.2905\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 757us/step - loss: 0.3172 - val_loss: 0.3087\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.3157 - val_loss: 0.3191\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 672us/step - loss: 0.3066 - val_loss: 0.2888\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.3142 - val_loss: 0.3171\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.3104 - val_loss: 0.2911\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.3042 - val_loss: 0.2967\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.3040 - val_loss: 0.3210\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3061 - val_loss: 0.3088\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.3001 - val_loss: 0.2839\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.2990 - val_loss: 0.2873\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 646us/step - loss: 0.2925 - val_loss: 0.3106\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.2873 - val_loss: 0.2963\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.2885 - val_loss: 0.3078\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.2910 - val_loss: 0.4142\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.2845 - val_loss: 0.2899\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 640us/step - loss: 0.2832 - val_loss: 0.2953\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.2923 - val_loss: 0.3027\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 647us/step - loss: 0.2860 - val_loss: 0.2879\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.2845 - val_loss: 0.2861\n",
      "121/121 [==============================] - 0s 438us/step - loss: 0.2883\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 929us/step - loss: 1.1744 - val_loss: 0.5014\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 653us/step - loss: 0.6768 - val_loss: 0.3838\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4131 - val_loss: 0.3523\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 624us/step - loss: 0.3842 - val_loss: 0.3433\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.3596 - val_loss: 0.3343\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.3494 - val_loss: 0.5126\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 608us/step - loss: 0.3468 - val_loss: 0.3133\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3370 - val_loss: 0.3231\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.3265 - val_loss: 0.3209\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.3164 - val_loss: 0.3999\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.3207 - val_loss: 0.3067\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 600us/step - loss: 0.3148 - val_loss: 0.3015\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.3108 - val_loss: 0.3157\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 608us/step - loss: 0.3040 - val_loss: 0.2867\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.3031 - val_loss: 0.2994\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3016 - val_loss: 0.3364\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.2928 - val_loss: 0.2849\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 664us/step - loss: 0.2922 - val_loss: 0.2873\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.2915 - val_loss: 0.2851\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.2896 - val_loss: 0.2911\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 608us/step - loss: 0.2818 - val_loss: 0.2841\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.2851 - val_loss: 0.2871\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2797 - val_loss: 0.2810\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 608us/step - loss: 0.2849 - val_loss: 0.2881\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 660us/step - loss: 0.2770 - val_loss: 0.2937\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.2715 - val_loss: 0.2942\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2755 - val_loss: 0.2797\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.2727 - val_loss: 0.2810\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 605us/step - loss: 0.2757 - val_loss: 0.3119\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.2707 - val_loss: 0.2827\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 610us/step - loss: 0.2731 - val_loss: 0.2785\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2661 - val_loss: 0.2899\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2635 - val_loss: 0.2886\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2671 - val_loss: 0.2772\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.2706 - val_loss: 0.2816\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.2638 - val_loss: 0.2933\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 628us/step - loss: 0.2650 - val_loss: 0.2791\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.2590 - val_loss: 0.3148\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 605us/step - loss: 0.2610 - val_loss: 0.3053\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.2581 - val_loss: 0.2798\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2553 - val_loss: 0.3022\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 610us/step - loss: 0.2610 - val_loss: 0.2758\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.2525 - val_loss: 0.2825\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 605us/step - loss: 0.2564 - val_loss: 0.2767\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 659us/step - loss: 0.2567 - val_loss: 0.2990\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.2514 - val_loss: 0.2863\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 611us/step - loss: 0.2534 - val_loss: 0.2839\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.2536 - val_loss: 0.3027\n",
      "Epoch 49/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2529 - val_loss: 0.2808\n",
      "Epoch 50/128\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.2511 - val_loss: 0.2984\n",
      "Epoch 51/128\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.2494 - val_loss: 0.3228\n",
      "Epoch 52/128\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.2469 - val_loss: 0.2844\n",
      "121/121 [==============================] - 0s 375us/step - loss: 0.3216\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.7036 - val_loss: 0.4361\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.4590 - val_loss: 0.4021\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.4260 - val_loss: 0.5444\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 665us/step - loss: 0.4071 - val_loss: 0.3735\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 605us/step - loss: 0.4038 - val_loss: 0.3653\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 666us/step - loss: 0.3802 - val_loss: 0.4312\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.3703 - val_loss: 0.3321\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 604us/step - loss: 0.3637 - val_loss: 0.3378\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.3613 - val_loss: 0.3293\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 603us/step - loss: 0.3511 - val_loss: 0.3311\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3436 - val_loss: 0.3591\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.3387 - val_loss: 0.3189\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.3351 - val_loss: 0.3822\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3298 - val_loss: 0.3713\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 629us/step - loss: 0.3307 - val_loss: 0.3092\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3275 - val_loss: 0.3147\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.3215 - val_loss: 0.3064\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 656us/step - loss: 0.3128 - val_loss: 0.3066\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.3094 - val_loss: 0.3081\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 612us/step - loss: 0.3069 - val_loss: 0.3224\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 674us/step - loss: 0.3027 - val_loss: 0.3449\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 615us/step - loss: 0.3016 - val_loss: 0.2861\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.3012 - val_loss: 0.2863\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.2954 - val_loss: 0.3039\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.2952 - val_loss: 0.2845\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.2943 - val_loss: 0.2899\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.2924 - val_loss: 0.2884\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 610us/step - loss: 0.2891 - val_loss: 0.2822\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 613us/step - loss: 0.2868 - val_loss: 0.2848\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.2857 - val_loss: 0.3010\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.2852 - val_loss: 0.2857\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 655us/step - loss: 0.2847 - val_loss: 0.2825\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.2812 - val_loss: 0.2795\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.2788 - val_loss: 0.3910\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2814 - val_loss: 0.3078\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.2767 - val_loss: 0.2823\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 613us/step - loss: 0.2770 - val_loss: 0.2833\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.2739 - val_loss: 0.2961\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.2715 - val_loss: 0.2804\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.2708 - val_loss: 0.2962\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.2666 - val_loss: 0.3485\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 657us/step - loss: 0.2668 - val_loss: 0.3037\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.2663 - val_loss: 0.3122\n",
      "121/121 [==============================] - 0s 503us/step - loss: 0.3272\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.7574 - val_loss: 0.4201\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.4597 - val_loss: 0.3978\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.4228 - val_loss: 0.3567\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.4047 - val_loss: 0.3401\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 662us/step - loss: 0.3796 - val_loss: 0.3474\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3740 - val_loss: 0.3467\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 667us/step - loss: 0.3653 - val_loss: 0.3184\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 601us/step - loss: 0.3606 - val_loss: 0.3083\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 755us/step - loss: 0.3535 - val_loss: 0.3502\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 608us/step - loss: 0.3461 - val_loss: 0.3280\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 661us/step - loss: 0.3388 - val_loss: 0.3273\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.3343 - val_loss: 0.3757\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.3328 - val_loss: 0.3285\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 670us/step - loss: 0.3240 - val_loss: 0.2871\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3233 - val_loss: 0.3092\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.3182 - val_loss: 0.2990\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.3166 - val_loss: 0.3115\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.3128 - val_loss: 0.2917\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 604us/step - loss: 0.3142 - val_loss: 0.3518\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.3110 - val_loss: 0.3178\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.3038 - val_loss: 0.3650\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 654us/step - loss: 0.3063 - val_loss: 0.2931\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.3070 - val_loss: 0.2901\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3013 - val_loss: 0.3420\n",
      "121/121 [==============================] - 0s 364us/step - loss: 0.3413\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8708 - val_loss: 0.3918\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.4324 - val_loss: 0.3642\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.3951 - val_loss: 0.3526\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.3760 - val_loss: 0.6182\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.3607 - val_loss: 0.3787\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.3490 - val_loss: 0.3293\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3454 - val_loss: 0.3242\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3388 - val_loss: 0.3236\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.3303 - val_loss: 0.3248\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.3238 - val_loss: 0.3521\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3225 - val_loss: 0.3010\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.3136 - val_loss: 0.3036\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.3069 - val_loss: 0.3338\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3032 - val_loss: 0.2967\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2991 - val_loss: 0.2975\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.2930 - val_loss: 0.3000\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.2924 - val_loss: 0.2935\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.2931 - val_loss: 0.2923\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.2896 - val_loss: 0.3135\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.2900 - val_loss: 0.3332\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.2891 - val_loss: 0.3057\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.2798 - val_loss: 0.2903\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2823 - val_loss: 0.2861\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.2783 - val_loss: 0.2870\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2784 - val_loss: 0.2950\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2765 - val_loss: 0.2847\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.2776 - val_loss: 0.2977\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.2722 - val_loss: 0.3163\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.2695 - val_loss: 0.2837\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.2660 - val_loss: 0.3105\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2687 - val_loss: 0.2881\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.2669 - val_loss: 0.2944\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2605 - val_loss: 0.3014\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.2635 - val_loss: 0.3023\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.2631 - val_loss: 0.2902\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.2612 - val_loss: 0.3103\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.2611 - val_loss: 0.2858\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 724us/step - loss: 0.2615 - val_loss: 0.3078\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.2543 - val_loss: 0.2835\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.2556 - val_loss: 0.2771\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.2554 - val_loss: 0.2833\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.2595 - val_loss: 0.2944\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2530 - val_loss: 0.2845\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.2502 - val_loss: 0.2835\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.2526 - val_loss: 0.2827\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.2484 - val_loss: 0.2877\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.2468 - val_loss: 0.3157\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2584 - val_loss: 0.2791\n",
      "Epoch 49/128\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.2499 - val_loss: 0.2906\n",
      "Epoch 50/128\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.2444 - val_loss: 0.2798\n",
      "121/121 [==============================] - 0s 386us/step - loss: 0.3112\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8091 - val_loss: 0.4502\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.4735 - val_loss: 0.5654\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.4277 - val_loss: 0.3671\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.4087 - val_loss: 0.3577\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.3920 - val_loss: 0.4404\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3754 - val_loss: 0.3505\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.3716 - val_loss: 0.3507\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.3564 - val_loss: 0.3397\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 675us/step - loss: 0.3544 - val_loss: 0.3119\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.3411 - val_loss: 0.3086\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 758us/step - loss: 0.3426 - val_loss: 0.3060\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3312 - val_loss: 0.3152\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.3318 - val_loss: 0.2909\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.3259 - val_loss: 0.3140\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.3208 - val_loss: 0.3130\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 716us/step - loss: 0.3154 - val_loss: 0.3199\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 749us/step - loss: 0.3158 - val_loss: 0.2954\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3100 - val_loss: 0.2959\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.3083 - val_loss: 0.2919\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.3135 - val_loss: 0.2924\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.3092 - val_loss: 0.2845\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.2985 - val_loss: 0.2880\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.2978 - val_loss: 0.2907\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.2891 - val_loss: 0.2743\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.2926 - val_loss: 0.2891\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.2930 - val_loss: 0.3073\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.2873 - val_loss: 0.3031\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.2901 - val_loss: 0.2723\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.2896 - val_loss: 0.2783\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2857 - val_loss: 0.3155\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2845 - val_loss: 0.2787\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.2828 - val_loss: 0.2894\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.2815 - val_loss: 0.2746\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 710us/step - loss: 0.2816 - val_loss: 0.2756\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.2811 - val_loss: 0.3096\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.2739 - val_loss: 0.2794\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.2770 - val_loss: 0.2794\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 717us/step - loss: 0.2775 - val_loss: 0.3003\n",
      "121/121 [==============================] - 0s 506us/step - loss: 0.3301\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8050 - val_loss: 0.4402\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.4838 - val_loss: 0.4451\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.4418 - val_loss: 0.3711\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.4164 - val_loss: 0.3489\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4000 - val_loss: 0.3514\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.3857 - val_loss: 0.3303\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 721us/step - loss: 0.3738 - val_loss: 0.3277\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3606 - val_loss: 0.3472\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.3638 - val_loss: 0.3929\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.3548 - val_loss: 0.3465\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3459 - val_loss: 0.3077\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 671us/step - loss: 0.3380 - val_loss: 0.3334\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.3363 - val_loss: 0.3763\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.3287 - val_loss: 0.3001\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.3305 - val_loss: 0.2999\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.3205 - val_loss: 0.2979\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.3237 - val_loss: 0.3124\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 684us/step - loss: 0.3179 - val_loss: 0.2941\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.3142 - val_loss: 0.2980\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3121 - val_loss: 0.2992\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.3064 - val_loss: 0.2967\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.3008 - val_loss: 0.3304\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 727us/step - loss: 0.3009 - val_loss: 0.2956\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.2973 - val_loss: 0.3084\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.2971 - val_loss: 0.2793\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2938 - val_loss: 0.3107\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.2897 - val_loss: 0.2898\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.2879 - val_loss: 0.3417\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.2882 - val_loss: 0.2795\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.2857 - val_loss: 0.2909\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.2791 - val_loss: 0.2780\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 682us/step - loss: 0.2804 - val_loss: 0.3112\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 714us/step - loss: 0.2807 - val_loss: 0.2868\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.2820 - val_loss: 0.2699\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2781 - val_loss: 0.2866\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2744 - val_loss: 0.2753\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 712us/step - loss: 0.2720 - val_loss: 0.3187\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.2711 - val_loss: 0.2791\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.2761 - val_loss: 0.3314\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.2674 - val_loss: 0.2770\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.2670 - val_loss: 0.2736\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.2653 - val_loss: 0.2974\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.2655 - val_loss: 0.2788\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 754us/step - loss: 0.2664 - val_loss: 0.2912\n",
      "121/121 [==============================] - 0s 436us/step - loss: 0.3111\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7794 - val_loss: 0.4529\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.4454 - val_loss: 0.3857\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.3974 - val_loss: 0.3546\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 718us/step - loss: 0.3892 - val_loss: 0.3402\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 748us/step - loss: 0.3684 - val_loss: 0.3369\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.3581 - val_loss: 0.3376\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.3549 - val_loss: 0.3132\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3506 - val_loss: 0.3227\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.3364 - val_loss: 0.3128\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.3302 - val_loss: 0.3383\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.3228 - val_loss: 0.2990\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.3169 - val_loss: 0.3060\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 761us/step - loss: 0.3180 - val_loss: 0.3119\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.3089 - val_loss: 0.3891\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.3032 - val_loss: 0.3715\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 729us/step - loss: 0.3004 - val_loss: 0.3142\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.2929 - val_loss: 0.2881\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2981 - val_loss: 0.2944\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.2916 - val_loss: 0.3003\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.2843 - val_loss: 0.2997\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.2929 - val_loss: 0.2821\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.2889 - val_loss: 0.3010\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.2849 - val_loss: 0.3586\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 763us/step - loss: 0.2801 - val_loss: 0.2846\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.2790 - val_loss: 0.2996\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.2791 - val_loss: 0.2862\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.2789 - val_loss: 0.2809\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2723 - val_loss: 0.3186\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 685us/step - loss: 0.2737 - val_loss: 0.2833\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2751 - val_loss: 0.3280\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 760us/step - loss: 0.2688 - val_loss: 0.2921\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 725us/step - loss: 0.2692 - val_loss: 0.2754\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.2653 - val_loss: 0.2767\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.2625 - val_loss: 0.2803\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.2653 - val_loss: 0.3384\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.2620 - val_loss: 0.2827\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.2660 - val_loss: 0.3028\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 731us/step - loss: 0.2611 - val_loss: 0.2886\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 695us/step - loss: 0.2603 - val_loss: 0.2921\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.2625 - val_loss: 0.2761\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.2574 - val_loss: 0.2796\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2548 - val_loss: 0.2857\n",
      "121/121 [==============================] - 0s 519us/step - loss: 0.3173\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 0.7480 - val_loss: 0.4336\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4724 - val_loss: 0.3867\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.4184 - val_loss: 0.3502\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 709us/step - loss: 0.3980 - val_loss: 0.4137\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.3807 - val_loss: 0.4932\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3769 - val_loss: 0.3684\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.3656 - val_loss: 0.3632\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.3587 - val_loss: 0.3126\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.3545 - val_loss: 0.3327\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.3417 - val_loss: 0.3087\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.3336 - val_loss: 0.4136\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 680us/step - loss: 0.3349 - val_loss: 0.2955\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 741us/step - loss: 0.3303 - val_loss: 0.2989\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.3306 - val_loss: 0.3022\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3200 - val_loss: 0.2930\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 681us/step - loss: 0.3229 - val_loss: 0.2919\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.3101 - val_loss: 0.2920\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.3182 - val_loss: 0.2802\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3096 - val_loss: 0.3625\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 678us/step - loss: 0.3055 - val_loss: 0.2823\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.3027 - val_loss: 0.3556\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.3027 - val_loss: 0.2967\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.3018 - val_loss: 0.2859\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 759us/step - loss: 0.2963 - val_loss: 0.2871\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2944 - val_loss: 0.3685\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.2937 - val_loss: 0.2796\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.2941 - val_loss: 0.2749\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.2861 - val_loss: 0.2953\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.2888 - val_loss: 0.3086\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.2865 - val_loss: 0.2718\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 739us/step - loss: 0.2826 - val_loss: 0.2779\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.2783 - val_loss: 0.3869\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.2810 - val_loss: 0.2953\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.2747 - val_loss: 0.2782\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.2714 - val_loss: 0.3072\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 698us/step - loss: 0.2740 - val_loss: 0.2779\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.2748 - val_loss: 0.2985\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 689us/step - loss: 0.2733 - val_loss: 0.2776\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.2739 - val_loss: 0.2652\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2676 - val_loss: 0.3384\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 733us/step - loss: 0.2659 - val_loss: 0.2910\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.2639 - val_loss: 0.2984\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.2636 - val_loss: 0.2691\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.2638 - val_loss: 0.2676\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.2603 - val_loss: 0.2739\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.2557 - val_loss: 0.2663\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 774us/step - loss: 0.2553 - val_loss: 0.2810\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.2556 - val_loss: 0.2741\n",
      "Epoch 49/128\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.2588 - val_loss: 0.2636\n",
      "Epoch 50/128\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.2532 - val_loss: 0.2819\n",
      "Epoch 51/128\n",
      "242/242 [==============================] - 0s 735us/step - loss: 0.2506 - val_loss: 0.2717\n",
      "Epoch 52/128\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.2542 - val_loss: 0.2678\n",
      "Epoch 53/128\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.2551 - val_loss: 0.2781\n",
      "Epoch 54/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.2519 - val_loss: 0.2662\n",
      "Epoch 55/128\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.2489 - val_loss: 0.2859\n",
      "Epoch 56/128\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.2452 - val_loss: 0.3433\n",
      "Epoch 57/128\n",
      "242/242 [==============================] - 0s 736us/step - loss: 0.2467 - val_loss: 0.2893\n",
      "Epoch 58/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.2454 - val_loss: 0.2862\n",
      "Epoch 59/128\n",
      "242/242 [==============================] - 0s 782us/step - loss: 0.2447 - val_loss: 0.2833\n",
      "121/121 [==============================] - 0s 398us/step - loss: 0.3145\n",
      "Epoch 1/128\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9631 - val_loss: 0.5143\n",
      "Epoch 2/128\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.4753 - val_loss: 0.3942\n",
      "Epoch 3/128\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.4259 - val_loss: 0.3650\n",
      "Epoch 4/128\n",
      "242/242 [==============================] - 0s 693us/step - loss: 0.4143 - val_loss: 0.3459\n",
      "Epoch 5/128\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.3875 - val_loss: 0.3571\n",
      "Epoch 6/128\n",
      "242/242 [==============================] - 0s 750us/step - loss: 0.3854 - val_loss: 0.3295\n",
      "Epoch 7/128\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.3798 - val_loss: 0.3451\n",
      "Epoch 8/128\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.3683 - val_loss: 0.3496\n",
      "Epoch 9/128\n",
      "242/242 [==============================] - 0s 745us/step - loss: 0.3593 - val_loss: 0.3353\n",
      "Epoch 10/128\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.3539 - val_loss: 0.3439\n",
      "Epoch 11/128\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3406 - val_loss: 0.3031\n",
      "Epoch 12/128\n",
      "242/242 [==============================] - 0s 706us/step - loss: 0.3470 - val_loss: 0.2965\n",
      "Epoch 13/128\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3326 - val_loss: 0.3097\n",
      "Epoch 14/128\n",
      "242/242 [==============================] - 0s 713us/step - loss: 0.3325 - val_loss: 0.2919\n",
      "Epoch 15/128\n",
      "242/242 [==============================] - 0s 768us/step - loss: 0.3247 - val_loss: 0.2926\n",
      "Epoch 16/128\n",
      "242/242 [==============================] - 0s 723us/step - loss: 0.3207 - val_loss: 0.2867\n",
      "Epoch 17/128\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.3153 - val_loss: 0.3071\n",
      "Epoch 18/128\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.3115 - val_loss: 0.3281\n",
      "Epoch 19/128\n",
      "242/242 [==============================] - 0s 687us/step - loss: 0.3103 - val_loss: 0.2933\n",
      "Epoch 20/128\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.3121 - val_loss: 0.2835\n",
      "Epoch 21/128\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.3052 - val_loss: 0.2868\n",
      "Epoch 22/128\n",
      "242/242 [==============================] - 0s 719us/step - loss: 0.3077 - val_loss: 0.3179\n",
      "Epoch 23/128\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.3043 - val_loss: 0.3529\n",
      "Epoch 24/128\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2952 - val_loss: 0.2907\n",
      "Epoch 25/128\n",
      "242/242 [==============================] - 0s 752us/step - loss: 0.2976 - val_loss: 0.2832\n",
      "Epoch 26/128\n",
      "242/242 [==============================] - 0s 742us/step - loss: 0.2938 - val_loss: 0.2885\n",
      "Epoch 27/128\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.2925 - val_loss: 0.2840\n",
      "Epoch 28/128\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.2976 - val_loss: 0.3583\n",
      "Epoch 29/128\n",
      "242/242 [==============================] - 0s 770us/step - loss: 0.2876 - val_loss: 0.2785\n",
      "Epoch 30/128\n",
      "242/242 [==============================] - 0s 747us/step - loss: 0.2843 - val_loss: 0.2822\n",
      "Epoch 31/128\n",
      "242/242 [==============================] - 0s 690us/step - loss: 0.2890 - val_loss: 0.2813\n",
      "Epoch 32/128\n",
      "242/242 [==============================] - 0s 740us/step - loss: 0.2829 - val_loss: 0.3836\n",
      "Epoch 33/128\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.2860 - val_loss: 0.3062\n",
      "Epoch 34/128\n",
      "242/242 [==============================] - 0s 744us/step - loss: 0.2793 - val_loss: 0.2764\n",
      "Epoch 35/128\n",
      "242/242 [==============================] - 0s 701us/step - loss: 0.2798 - val_loss: 0.3398\n",
      "Epoch 36/128\n",
      "242/242 [==============================] - 0s 778us/step - loss: 0.2797 - val_loss: 0.2725\n",
      "Epoch 37/128\n",
      "242/242 [==============================] - 0s 730us/step - loss: 0.2825 - val_loss: 0.2947\n",
      "Epoch 38/128\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.2694 - val_loss: 0.2917\n",
      "Epoch 39/128\n",
      "242/242 [==============================] - 0s 694us/step - loss: 0.2706 - val_loss: 0.2712\n",
      "Epoch 40/128\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.2753 - val_loss: 0.3067\n",
      "Epoch 41/128\n",
      "242/242 [==============================] - 0s 708us/step - loss: 0.2739 - val_loss: 0.2756\n",
      "Epoch 42/128\n",
      "242/242 [==============================] - 0s 732us/step - loss: 0.2698 - val_loss: 0.2680\n",
      "Epoch 43/128\n",
      "242/242 [==============================] - 0s 702us/step - loss: 0.2733 - val_loss: 0.2686\n",
      "Epoch 44/128\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.2647 - val_loss: 0.2724\n",
      "Epoch 45/128\n",
      "242/242 [==============================] - 0s 686us/step - loss: 0.2670 - val_loss: 0.2820\n",
      "Epoch 46/128\n",
      "242/242 [==============================] - 0s 734us/step - loss: 0.2721 - val_loss: 0.2662\n",
      "Epoch 47/128\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.2636 - val_loss: 0.2845\n",
      "Epoch 48/128\n",
      "242/242 [==============================] - 0s 746us/step - loss: 0.2651 - val_loss: 0.2962\n",
      "Epoch 49/128\n",
      "242/242 [==============================] - 0s 767us/step - loss: 0.2595 - val_loss: 0.3221\n",
      "Epoch 50/128\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.2611 - val_loss: 0.2863\n",
      "Epoch 51/128\n",
      "242/242 [==============================] - 0s 697us/step - loss: 0.2577 - val_loss: 0.4528\n",
      "Epoch 52/128\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.2690 - val_loss: 0.2937\n",
      "Epoch 53/128\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.2582 - val_loss: 0.2670\n",
      "Epoch 54/128\n",
      "242/242 [==============================] - 0s 783us/step - loss: 0.2549 - val_loss: 0.3367\n",
      "Epoch 55/128\n",
      "242/242 [==============================] - 0s 704us/step - loss: 0.2523 - val_loss: 0.3190\n",
      "Epoch 56/128\n",
      "242/242 [==============================] - 0s 738us/step - loss: 0.2539 - val_loss: 0.2801\n",
      "121/121 [==============================] - 0s 512us/step - loss: 0.2810\n",
      "Epoch 1/128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlschader/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [-0.30633252 -0.30936119 -0.36884068 -0.30155578 -0.32397479 -0.32171959\n",
      " -0.32347036         nan -0.32172443 -0.33385044 -0.3000675  -0.31264895\n",
      " -0.33004927 -0.31747373 -0.30430953]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 797us/step - loss: 0.6503 - val_loss: 0.4955\n",
      "Epoch 2/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.4314 - val_loss: 0.3672\n",
      "Epoch 3/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.3926 - val_loss: 0.3393\n",
      "Epoch 4/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.3784 - val_loss: 0.4834\n",
      "Epoch 5/128\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.3743 - val_loss: 0.3284\n",
      "Epoch 6/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.3510 - val_loss: 0.3473\n",
      "Epoch 7/128\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.3492 - val_loss: 0.3080\n",
      "Epoch 8/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3399 - val_loss: 0.3123\n",
      "Epoch 9/128\n",
      "363/363 [==============================] - 0s 584us/step - loss: 0.3333 - val_loss: 0.3418\n",
      "Epoch 10/128\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.3240 - val_loss: 0.3255\n",
      "Epoch 11/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.3216 - val_loss: 0.4065\n",
      "Epoch 12/128\n",
      "363/363 [==============================] - 0s 583us/step - loss: 0.3184 - val_loss: 0.3305\n",
      "Epoch 13/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3161 - val_loss: 0.2924\n",
      "Epoch 14/128\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.3075 - val_loss: 0.2771\n",
      "Epoch 15/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.3057 - val_loss: 0.3346\n",
      "Epoch 16/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.3021 - val_loss: 0.3490\n",
      "Epoch 17/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.2994 - val_loss: 0.3063\n",
      "Epoch 18/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.2975 - val_loss: 0.2839\n",
      "Epoch 19/128\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.2988 - val_loss: 0.3121\n",
      "Epoch 20/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.2938 - val_loss: 0.3356\n",
      "Epoch 21/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.2897 - val_loss: 0.2899\n",
      "Epoch 22/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.2879 - val_loss: 0.2767\n",
      "Epoch 23/128\n",
      "363/363 [==============================] - 0s 600us/step - loss: 0.2845 - val_loss: 0.2738\n",
      "Epoch 24/128\n",
      "363/363 [==============================] - 0s 607us/step - loss: 0.2836 - val_loss: 0.3311\n",
      "Epoch 25/128\n",
      "363/363 [==============================] - 0s 594us/step - loss: 0.2839 - val_loss: 0.2631\n",
      "Epoch 26/128\n",
      "363/363 [==============================] - 0s 599us/step - loss: 0.2814 - val_loss: 0.3230\n",
      "Epoch 27/128\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.2771 - val_loss: 0.2884\n",
      "Epoch 28/128\n",
      "363/363 [==============================] - 0s 605us/step - loss: 0.2771 - val_loss: 0.2765\n",
      "Epoch 29/128\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.2721 - val_loss: 0.2668\n",
      "Epoch 30/128\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.2737 - val_loss: 0.2673\n",
      "Epoch 31/128\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.2735 - val_loss: 0.2746\n",
      "Epoch 32/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.2707 - val_loss: 0.2656\n",
      "Epoch 33/128\n",
      "363/363 [==============================] - 0s 583us/step - loss: 0.2670 - val_loss: 0.2704\n",
      "Epoch 34/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.2641 - val_loss: 0.2807\n",
      "Epoch 35/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.2638 - val_loss: 0.2606\n",
      "Epoch 36/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.2616 - val_loss: 0.2626\n",
      "Epoch 37/128\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.2606 - val_loss: 0.2866\n",
      "Epoch 38/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.2613 - val_loss: 0.2657\n",
      "Epoch 39/128\n",
      "363/363 [==============================] - 0s 594us/step - loss: 0.2534 - val_loss: 0.2927\n",
      "Epoch 40/128\n",
      "363/363 [==============================] - 0s 585us/step - loss: 0.2560 - val_loss: 0.2922\n",
      "Epoch 41/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.2553 - val_loss: 0.2661\n",
      "Epoch 42/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.2547 - val_loss: 0.2590\n",
      "Epoch 43/128\n",
      "363/363 [==============================] - 0s 582us/step - loss: 0.2499 - val_loss: 0.2680\n",
      "Epoch 44/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.2513 - val_loss: 0.3310\n",
      "Epoch 45/128\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.2483 - val_loss: 0.2970\n",
      "Epoch 46/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.2463 - val_loss: 0.2622\n",
      "Epoch 47/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.2545 - val_loss: 0.2761\n",
      "Epoch 48/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.2441 - val_loss: 0.3754\n",
      "Epoch 49/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.2475 - val_loss: 0.2821\n",
      "Epoch 50/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.2451 - val_loss: 0.2537\n",
      "Epoch 51/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.2443 - val_loss: 0.2811\n",
      "Epoch 52/128\n",
      "363/363 [==============================] - 0s 579us/step - loss: 0.2394 - val_loss: 0.2831\n",
      "Epoch 53/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.2397 - val_loss: 0.2753\n",
      "Epoch 54/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.2359 - val_loss: 0.2807\n",
      "Epoch 55/128\n",
      "363/363 [==============================] - 0s 574us/step - loss: 0.2384 - val_loss: 0.3127\n",
      "Epoch 56/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.2363 - val_loss: 0.2685\n",
      "Epoch 57/128\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.2344 - val_loss: 0.2772\n",
      "Epoch 58/128\n",
      "363/363 [==============================] - 0s 598us/step - loss: 0.2339 - val_loss: 0.2731\n",
      "Epoch 59/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.2360 - val_loss: 0.2603\n",
      "Epoch 60/128\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.2305 - val_loss: 0.2788\n",
      "{'n_nodes': 41, 'n_hidden': 8, 'lr': 0.029797979797979796}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {\n",
    "  'n_hidden': [6, 7, 8, 9],\n",
    "  'n_nodes': range(40, 61),\n",
    "  'lr': np.linspace(0.01, 0.05, 100),\n",
    "}\n",
    "\n",
    "rndcv = RandomizedSearchCV(keras_reg, param_distributions=params, n_iter=15, cv=3)\n",
    "rndcv.fit(X_train, y_train, epochs=128, validation_data=(X_val, y_val), callbacks=(keras.callbacks.EarlyStopping(patience=10)))\n",
    "print(rndcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "363/363 [==============================] - 1s 830us/step - loss: 0.6486 - val_loss: 0.4374\n",
      "Epoch 2/128\n",
      "363/363 [==============================] - 0s 590us/step - loss: 0.4429 - val_loss: 1.0706\n",
      "Epoch 3/128\n",
      "363/363 [==============================] - 0s 619us/step - loss: 0.4074 - val_loss: 0.3556\n",
      "Epoch 4/128\n",
      "363/363 [==============================] - 0s 596us/step - loss: 0.3863 - val_loss: 0.3925\n",
      "Epoch 5/128\n",
      "363/363 [==============================] - 0s 588us/step - loss: 0.3682 - val_loss: 0.3120\n",
      "Epoch 6/128\n",
      "363/363 [==============================] - 0s 580us/step - loss: 0.3522 - val_loss: 0.3263\n",
      "Epoch 7/128\n",
      "363/363 [==============================] - 0s 594us/step - loss: 0.3551 - val_loss: 0.3721\n",
      "Epoch 8/128\n",
      "363/363 [==============================] - 0s 628us/step - loss: 0.3395 - val_loss: 0.2911\n",
      "Epoch 9/128\n",
      "363/363 [==============================] - 0s 599us/step - loss: 0.3401 - val_loss: 0.2927\n",
      "Epoch 10/128\n",
      "363/363 [==============================] - 0s 595us/step - loss: 0.3264 - val_loss: 0.2950\n",
      "Epoch 11/128\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.3250 - val_loss: 0.2986\n",
      "Epoch 12/128\n",
      "363/363 [==============================] - 0s 615us/step - loss: 0.3233 - val_loss: 0.3365\n",
      "Epoch 13/128\n",
      "363/363 [==============================] - 0s 609us/step - loss: 0.3137 - val_loss: 0.2937\n",
      "Epoch 14/128\n",
      "363/363 [==============================] - 0s 607us/step - loss: 0.3116 - val_loss: 0.2864\n",
      "Epoch 15/128\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.3113 - val_loss: 0.3046\n",
      "Epoch 16/128\n",
      "363/363 [==============================] - 0s 600us/step - loss: 0.3031 - val_loss: 0.2920\n",
      "Epoch 17/128\n",
      "363/363 [==============================] - 0s 627us/step - loss: 0.3039 - val_loss: 0.3119\n",
      "Epoch 18/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.3011 - val_loss: 0.2819\n",
      "Epoch 19/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.3010 - val_loss: 0.2906\n",
      "Epoch 20/128\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.2987 - val_loss: 0.2759\n",
      "Epoch 21/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.2892 - val_loss: 0.2948\n",
      "Epoch 22/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.2906 - val_loss: 0.3183\n",
      "Epoch 23/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.2887 - val_loss: 0.2979\n",
      "Epoch 24/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.2871 - val_loss: 0.3107\n",
      "Epoch 25/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.2839 - val_loss: 0.2773\n",
      "Epoch 26/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.2850 - val_loss: 0.3000\n",
      "Epoch 27/128\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.2843 - val_loss: 0.2876\n",
      "Epoch 28/128\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.2784 - val_loss: 0.2945\n",
      "Epoch 29/128\n",
      "363/363 [==============================] - 0s 585us/step - loss: 0.2802 - val_loss: 0.2658\n",
      "Epoch 30/128\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.2799 - val_loss: 0.2681\n",
      "Epoch 31/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.2725 - val_loss: 0.2726\n",
      "Epoch 32/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.2740 - val_loss: 0.2690\n",
      "Epoch 33/128\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.2735 - val_loss: 0.2803\n",
      "Epoch 34/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.2695 - val_loss: 0.2774\n",
      "Epoch 35/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.2651 - val_loss: 0.2865\n",
      "Epoch 36/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.2687 - val_loss: 0.3084\n",
      "Epoch 37/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.2674 - val_loss: 0.2615\n",
      "Epoch 38/128\n",
      "363/363 [==============================] - 0s 578us/step - loss: 0.2657 - val_loss: 0.3098\n",
      "Epoch 39/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.2617 - val_loss: 0.2809\n",
      "Epoch 40/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.2617 - val_loss: 0.2789\n",
      "Epoch 41/128\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.2643 - val_loss: 0.2591\n",
      "Epoch 42/128\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.2676 - val_loss: 0.2609\n",
      "Epoch 43/128\n",
      "363/363 [==============================] - 0s 617us/step - loss: 0.2598 - val_loss: 0.2811\n",
      "Epoch 44/128\n",
      "363/363 [==============================] - 0s 597us/step - loss: 0.2604 - val_loss: 0.2710\n",
      "Epoch 45/128\n",
      "363/363 [==============================] - 0s 567us/step - loss: 0.2572 - val_loss: 0.2706\n",
      "Epoch 46/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.2553 - val_loss: 0.2608\n",
      "Epoch 47/128\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.2541 - val_loss: 0.2625\n",
      "Epoch 48/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.2535 - val_loss: 0.2829\n",
      "Epoch 49/128\n",
      "363/363 [==============================] - 0s 571us/step - loss: 0.2522 - val_loss: 0.2884\n",
      "Epoch 50/128\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.2544 - val_loss: 0.2679\n",
      "Epoch 51/128\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.2514 - val_loss: 0.2674\n",
      "162/162 [==============================] - 0s 375us/step - loss: 0.2770\n",
      "0.27699583768844604\n"
     ]
    }
   ],
   "source": [
    "best = build_model(**rndcv.best_params_)\n",
    "best.fit(X_train, y_train, epochs=128, validation_data=(X_val, y_val), callbacks=(keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)))\n",
    "print(best.evaluate(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 326us/step\n",
      "[3.964 1.869 1.676 ... 1.087 1.232 2.725]\n",
      "[4.081619  3.19416   1.909896  ... 1.105144  1.9045583 2.71219  ]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(y_test)\n",
    "print(pred.reshape(1, -1).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 784) (45000,)\n"
     ]
    }
   ],
   "source": [
    "# exercise\n",
    "\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess = Pipeline([\n",
    "  ('flatten', FunctionTransformer(lambda X: X.reshape(len(X), -1))),\n",
    "  ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "# (X_train_raw_full, y_train_full), (X_test_raw, y_test) = keras.datasets.mnist.load_data()\n",
    "(X_train_raw_full, y_train_full), (X_test_raw, y_test) = fashion_mnist = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = preprocess.fit_transform(X_train_raw_full)\n",
    "X_test = preprocess.fit_transform(X_test_raw)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/q7k30xcj51x3tc06z506z3n00000gn/T/ipykernel_42341/3891158908.py:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_clf = keras.wrappers.scikit_learn.KerasClassifier(build_model)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "save_dir = os.path.join(os.curdir, 'MNIST.h5')\n",
    "log_dir = os.path.join(os.curdir, 'logs')\n",
    "\n",
    "def get_run_log_dir():\n",
    "  run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "  return os.path.join(log_dir, run_id)\n",
    "\n",
    "def build_model(n_hidden=1, n_nodes=30, lr=0.005, input_shape=(28, 28), output_nodes=10, output_activation='softmax', loss=keras.losses.categorical_crossentropy, metrics=['accuracy']):\n",
    "  model = keras.models.Sequential()\n",
    "  model.add(keras.layers.Flatten(input_shape=input_shape))\n",
    "  for _ in range(n_hidden):\n",
    "    model.add(keras.layers.Dense(n_nodes, activation='selu', kernel_initializer='lecun_normal'))\n",
    "  model.add(keras.layers.Dense(output_nodes, activation=output_activation))\n",
    "  model.compile(optimizer=keras.optimizers.SGD(lr), loss=loss, metrics=metrics)\n",
    "  return model\n",
    "\n",
    "keras_clf = keras.wrappers.scikit_learn.KerasClassifier(build_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "1407/1407 [==============================] - 13s 9ms/step - loss: 0.5656 - sparse_categorical_accuracy: 0.8151 - val_loss: 0.5043 - val_sparse_categorical_accuracy: 0.8289\n",
      "Epoch 2/256\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 0.3686 - sparse_categorical_accuracy: 0.8682 - val_loss: 0.5767 - val_sparse_categorical_accuracy: 0.8135\n",
      "Epoch 3/256\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 0.3184 - sparse_categorical_accuracy: 0.8843 - val_loss: 0.5007 - val_sparse_categorical_accuracy: 0.8297\n",
      "Epoch 4/256\n",
      "1407/1407 [==============================] - 12s 8ms/step - loss: 0.2803 - sparse_categorical_accuracy: 0.8977 - val_loss: 0.3833 - val_sparse_categorical_accuracy: 0.8661\n",
      "Epoch 5/256\n",
      "1407/1407 [==============================] - 12s 9ms/step - loss: 0.2546 - sparse_categorical_accuracy: 0.9063 - val_loss: 0.6503 - val_sparse_categorical_accuracy: 0.8184\n",
      "Epoch 6/256\n",
      " 517/1407 [==========>...................] - ETA: 6s - loss: 0.2225 - sparse_categorical_accuracy: 0.9177"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# params = {\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X46sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#   'n_hidden': [x for x in range(1, 3)],\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X46sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#   'n_nodes': [x for x in range(15, 25)],\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X46sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# ])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# print(randcv.best_params_)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m model \u001b[39m=\u001b[39m build_model(\u001b[39m5\u001b[39m, \u001b[39m800\u001b[39m, \u001b[39m0.03\u001b[39m, X_test\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:], \u001b[39m10\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39msparse_categorical_crossentropy, metrics\u001b[39m=\u001b[39m[keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39msparse_categorical_accuracy])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_val, y_val), callbacks\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X46sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   keras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mTensorBoard(get_run_log_dir()),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X46sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   keras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mModelCheckpoint(save_dir, save_best_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X46sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   keras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mEarlyStopping(patience\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, restore_best_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch10/ch10.ipynb#X46sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m ])\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score\n",
    "\n",
    "# params = {\n",
    "#   'n_hidden': [x for x in range(1, 3)],\n",
    "#   'n_nodes': [x for x in range(15, 25)],\n",
    "#   'lr': np.linspace(0.025, 0.05, 100),\n",
    "#   'input_shape': X_test_scaled.shape[1:],\n",
    "# }\n",
    "\n",
    "# randcv = RandomizedSearchCV(keras_clf, param_distributions=params, cv=3, n_iter=10)\n",
    "# randcv.fit(X_train_scaled, y_train, epochs=256, validation_data=(X_val_scaled, y_val), callbacks=[\n",
    "#   keras.callbacks.TensorBoard(get_run_log_dir()),\n",
    "#   keras.callbacks.ModelCheckpoint(save_dir, save_best_only=True),\n",
    "#   keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "# ])\n",
    "# print(randcv.best_params_)\n",
    "\n",
    "\n",
    "model = build_model(5, 800, 0.01, X_test.shape[1:], 10, 'softmax', loss=keras.losses.sparse_categorical_crossentropy, metrics=[keras.metrics.sparse_categorical_accuracy])\n",
    "model.fit(X_train, y_train, epochs=256, validation_data=(X_val, y_val), callbacks=[\n",
    "  keras.callbacks.TensorBoard(get_run_log_dir()),\n",
    "  keras.callbacks.ModelCheckpoint(save_dir, save_best_only=True),\n",
    "  keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n",
      "7 7\n",
      "[[ 966    0    1    1    0    3    4    2    2    1]\n",
      " [   0 1124    4    1    0    1    2    1    2    0]\n",
      " [   5    2  999    9    2    0    3    5    7    0]\n",
      " [   0    1    3  986    1    5    0    4    4    6]\n",
      " [   0    0    8    1  951    1    5    3    1   12]\n",
      " [   2    1    0   14    2  857    5    3    6    2]\n",
      " [   7    2    1    1    4    7  934    0    2    0]\n",
      " [   0    7    9    3    2    1    0  992    3   11]\n",
      " [   2    2    2   10    7   12    4    5  926    4]\n",
      " [   3    4    1    8   14    3    1    8    2  965]]\n",
      "0.97\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded = keras.models.load_model(save_dir)\n",
    "pred = loaded.predict(X_test)\n",
    "i = 0\n",
    "plt.imshow(X_test_raw[i])\n",
    "print(np.argmax(pred[i]), y_test[i])\n",
    "print(confusion_matrix(y_test, np.argmax(pred, axis=1)))\n",
    "print(accuracy_score(y_test, np.argmax(pred, axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9120ca18d9a6352a9f63d878cd22df8740658f703b5d617e89c0216f7e3927ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
