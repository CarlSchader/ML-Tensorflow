{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) <dtype: 'float32'>\n",
      "tf.Tensor(\n",
      "[[2. 3.]\n",
      " [5. 6.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.]\n",
      " [5.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.]\n",
      " [5.]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tensor = tf.constant([\n",
    "  [1.0, 2.0, 3.0],\n",
    "  [4.0, 5.0, 6.0]\n",
    "])\n",
    "\n",
    "print(tensor.shape, tensor.dtype)\n",
    "print(tensor[:, 1:])\n",
    "print(tensor[..., 1, tf.newaxis])\n",
    "print(tensor[..., 1, None]) # identical (tf.newaxis == None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 42.  42.]\n",
      " [105. 105.]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 4.]\n",
      " [2. 5.]\n",
      " [3. 6.]], shape=(3, 2), dtype=float32)\n",
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[100.,   4.,   7.],\n",
      "       [  2.,  42.,   8.],\n",
      "       [  3.,   6., 200.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# immutable tensors\n",
    "t1 = tensor\n",
    "t2 = tf.constant([\n",
    "  [7.0, 7.0],\n",
    "  [7.0, 7.0],\n",
    "  [7.0, 7.0],\n",
    "])\n",
    "\n",
    "print(t1 @ t2) # matrix multiplication\n",
    "print(tf.transpose(t1)) # transpose\n",
    "\n",
    "# mutable tensor\n",
    "v1 = tf.Variable([\n",
    "  [1., 2., 3.],\n",
    "  [4., 5., 6.],\n",
    "  [7., 8., 9.],\n",
    "])\n",
    "\n",
    "v1.assign(tf.transpose(v1))\n",
    "v1[1, 1].assign(42.)\n",
    "v1.scatter_nd_update(indices=([0, 0], [2, 2]), updates=(100., 200.))\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Load some data before getting on plane lol\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8) (3870, 8) (5160, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.125   4.5     0.28125 2.5    ], shape=(4,), dtype=float32)\n",
      "tf.Tensor(2.3125, shape=(), dtype=float32)\n",
      "tf.Tensor(1.8515625, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def huber_loss(y_true, y_pred, delta=1.):\n",
    "  errors = tf.abs(y_pred - y_true)\n",
    "  low_error = errors < delta\n",
    "  squared_error = delta * tf.square(errors) / 2\n",
    "  linear_error = delta * (errors - 0.5)\n",
    "  return tf.where(low_error, squared_error, linear_error)\n",
    "\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "  def __init__(self, delta=1.0, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.delta = delta\n",
    "    self.count = tf.Variable(0.)\n",
    "    self.sum = tf.Variable(0.)\n",
    "  \n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    errors = huber_loss(y_true, y_pred, self.delta)\n",
    "    self.sum.assign_add(tf.reduce_sum(errors))\n",
    "    self.count.assign_add(tf.cast(tf.size(errors), tf.float32))\n",
    "  \n",
    "  def result(self):\n",
    "    return self.sum / self.count\n",
    "  \n",
    "  def get_config(self):\n",
    "    base_config = super().get_config()\n",
    "    return {**base_config, 'delta': self.delta}\n",
    "\n",
    "y_true = tf.constant([20., 30., 40., 50.])\n",
    "y_pred = tf.constant([20.5, 35., 39.25, 53.])\n",
    "\n",
    "print(huber_loss(y_true, y_pred))\n",
    "\n",
    "metric = HuberMetric()\n",
    "metric.update_state(y_true[:2], y_pred[:2])\n",
    "print(metric.result())\n",
    "metric.update_state(y_true[2:], y_pred[2:])\n",
    "print(metric.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2429 - huber_metric_4: 0.2429 - val_loss: 0.4191 - val_huber_metric_4: 0.4191 - lr: 0.0350\n",
      "Epoch 2/256\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.2017 - huber_metric_4: 0.2017 - val_loss: 0.1898 - val_huber_metric_4: 0.1898 - lr: 0.0350\n",
      "Epoch 3/256\n",
      "363/363 [==============================] - 0s 731us/step - loss: 0.1909 - huber_metric_4: 0.1909 - val_loss: 0.2250 - val_huber_metric_4: 0.2250 - lr: 0.0350\n",
      "Epoch 4/256\n",
      "363/363 [==============================] - 0s 726us/step - loss: 0.1811 - huber_metric_4: 0.1811 - val_loss: 0.2060 - val_huber_metric_4: 0.2060 - lr: 0.0350\n",
      "Epoch 5/256\n",
      "363/363 [==============================] - 0s 922us/step - loss: 0.1803 - huber_metric_4: 0.1803 - val_loss: 0.1830 - val_huber_metric_4: 0.1830 - lr: 0.0350\n",
      "Epoch 6/256\n",
      "363/363 [==============================] - 0s 725us/step - loss: 0.1779 - huber_metric_4: 0.1779 - val_loss: 0.2186 - val_huber_metric_4: 0.2186 - lr: 0.0350\n",
      "Epoch 7/256\n",
      "363/363 [==============================] - 0s 916us/step - loss: 0.1756 - huber_metric_4: 0.1756 - val_loss: 0.1779 - val_huber_metric_4: 0.1779 - lr: 0.0350\n",
      "Epoch 8/256\n",
      "363/363 [==============================] - 0s 712us/step - loss: 0.1786 - huber_metric_4: 0.1786 - val_loss: 0.2084 - val_huber_metric_4: 0.2084 - lr: 0.0350\n",
      "Epoch 9/256\n",
      "363/363 [==============================] - 0s 723us/step - loss: 0.1785 - huber_metric_4: 0.1785 - val_loss: 0.2000 - val_huber_metric_4: 0.2000 - lr: 0.0350\n",
      "Epoch 10/256\n",
      "363/363 [==============================] - 0s 896us/step - loss: 0.1744 - huber_metric_4: 0.1744 - val_loss: 0.1813 - val_huber_metric_4: 0.1813 - lr: 0.0350\n",
      "Epoch 11/256\n",
      "363/363 [==============================] - 0s 725us/step - loss: 0.1719 - huber_metric_4: 0.1719 - val_loss: 0.1939 - val_huber_metric_4: 0.1939 - lr: 0.0350\n",
      "Epoch 12/256\n",
      "363/363 [==============================] - 0s 753us/step - loss: 0.1711 - huber_metric_4: 0.1711 - val_loss: 0.1836 - val_huber_metric_4: 0.1836 - lr: 0.0350\n",
      "Epoch 13/256\n",
      "363/363 [==============================] - 0s 758us/step - loss: 0.1657 - huber_metric_4: 0.1657 - val_loss: 0.1972 - val_huber_metric_4: 0.1972 - lr: 0.0280\n",
      "Epoch 14/256\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.1702 - huber_metric_4: 0.1702 - val_loss: 0.1809 - val_huber_metric_4: 0.1809 - lr: 0.0280\n",
      "Epoch 15/256\n",
      "363/363 [==============================] - 0s 755us/step - loss: 0.1695 - huber_metric_4: 0.1695 - val_loss: 0.1758 - val_huber_metric_4: 0.1758 - lr: 0.0280\n",
      "Epoch 16/256\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.1685 - huber_metric_4: 0.1685 - val_loss: 0.1828 - val_huber_metric_4: 0.1828 - lr: 0.0280\n",
      "Epoch 17/256\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.1656 - huber_metric_4: 0.1656 - val_loss: 0.1816 - val_huber_metric_4: 0.1816 - lr: 0.0280\n",
      "Epoch 18/256\n",
      "363/363 [==============================] - 0s 719us/step - loss: 0.1670 - huber_metric_4: 0.1670 - val_loss: 0.2055 - val_huber_metric_4: 0.2055 - lr: 0.0280\n",
      "Epoch 19/256\n",
      "363/363 [==============================] - 0s 703us/step - loss: 0.1687 - huber_metric_4: 0.1687 - val_loss: 0.1827 - val_huber_metric_4: 0.1827 - lr: 0.0280\n",
      "Epoch 20/256\n",
      "363/363 [==============================] - 0s 692us/step - loss: 0.1676 - huber_metric_4: 0.1676 - val_loss: 0.1888 - val_huber_metric_4: 0.1888 - lr: 0.0280\n",
      "Epoch 21/256\n",
      "363/363 [==============================] - 0s 758us/step - loss: 0.1621 - huber_metric_4: 0.1621 - val_loss: 0.1802 - val_huber_metric_4: 0.1802 - lr: 0.0224\n",
      "Epoch 22/256\n",
      "363/363 [==============================] - 0s 708us/step - loss: 0.1582 - huber_metric_4: 0.1582 - val_loss: 0.1798 - val_huber_metric_4: 0.1798 - lr: 0.0224\n",
      "Epoch 23/256\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.1595 - huber_metric_4: 0.1595 - val_loss: 0.1857 - val_huber_metric_4: 0.1857 - lr: 0.0224\n",
      "Epoch 24/256\n",
      "363/363 [==============================] - 0s 707us/step - loss: 0.1632 - huber_metric_4: 0.1632 - val_loss: 0.2179 - val_huber_metric_4: 0.2179 - lr: 0.0224\n",
      "Epoch 25/256\n",
      "363/363 [==============================] - 0s 752us/step - loss: 0.1624 - huber_metric_4: 0.1624 - val_loss: 0.2043 - val_huber_metric_4: 0.2043 - lr: 0.0224\n",
      "Epoch 26/256\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.1608 - huber_metric_4: 0.1608 - val_loss: 0.1978 - val_huber_metric_4: 0.1978 - lr: 0.0179\n",
      "Epoch 27/256\n",
      "363/363 [==============================] - 0s 710us/step - loss: 0.1599 - huber_metric_4: 0.1599 - val_loss: 0.1706 - val_huber_metric_4: 0.1706 - lr: 0.0179\n",
      "Epoch 28/256\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.1606 - huber_metric_4: 0.1606 - val_loss: 0.1768 - val_huber_metric_4: 0.1768 - lr: 0.0179\n",
      "Epoch 29/256\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.1604 - huber_metric_4: 0.1604 - val_loss: 0.1860 - val_huber_metric_4: 0.1860 - lr: 0.0179\n",
      "Epoch 30/256\n",
      "363/363 [==============================] - 0s 724us/step - loss: 0.1627 - huber_metric_4: 0.1627 - val_loss: 0.1962 - val_huber_metric_4: 0.1962 - lr: 0.0179\n",
      "Epoch 31/256\n",
      "363/363 [==============================] - 0s 706us/step - loss: 0.1566 - huber_metric_4: 0.1566 - val_loss: 0.2097 - val_huber_metric_4: 0.2097 - lr: 0.0179\n",
      "Epoch 32/256\n",
      "363/363 [==============================] - 0s 732us/step - loss: 0.1595 - huber_metric_4: 0.1595 - val_loss: 0.1915 - val_huber_metric_4: 0.1915 - lr: 0.0179\n",
      "Epoch 33/256\n",
      "363/363 [==============================] - 0s 772us/step - loss: 0.1555 - huber_metric_4: 0.1555 - val_loss: 0.1889 - val_huber_metric_4: 0.1889 - lr: 0.0143\n",
      "Epoch 34/256\n",
      "363/363 [==============================] - 0s 715us/step - loss: 0.1583 - huber_metric_4: 0.1583 - val_loss: 0.2038 - val_huber_metric_4: 0.2038 - lr: 0.0143\n",
      "Epoch 35/256\n",
      "363/363 [==============================] - 0s 698us/step - loss: 0.1568 - huber_metric_4: 0.1568 - val_loss: 0.1877 - val_huber_metric_4: 0.1877 - lr: 0.0143\n",
      "Epoch 36/256\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.1624 - huber_metric_4: 0.1624 - val_loss: 0.1864 - val_huber_metric_4: 0.1864 - lr: 0.0143\n",
      "Epoch 37/256\n",
      "363/363 [==============================] - 0s 721us/step - loss: 0.1577 - huber_metric_4: 0.1577 - val_loss: 0.1885 - val_huber_metric_4: 0.1885 - lr: 0.0143\n",
      "Epoch 38/256\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.1551 - huber_metric_4: 0.1551 - val_loss: 0.1940 - val_huber_metric_4: 0.1940 - lr: 0.0115\n",
      "Epoch 39/256\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.1559 - huber_metric_4: 0.1559 - val_loss: 0.1946 - val_huber_metric_4: 0.1946 - lr: 0.0115\n",
      "Epoch 40/256\n",
      "363/363 [==============================] - 0s 702us/step - loss: 0.1596 - huber_metric_4: 0.1596 - val_loss: 0.1849 - val_huber_metric_4: 0.1849 - lr: 0.0115\n",
      "Epoch 41/256\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.1540 - huber_metric_4: 0.1540 - val_loss: 0.1846 - val_huber_metric_4: 0.1846 - lr: 0.0115\n",
      "Epoch 42/256\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.1564 - huber_metric_4: 0.1564 - val_loss: 0.1855 - val_huber_metric_4: 0.1855 - lr: 0.0115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2929c7940>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(30, activation='elu', kernel_initializer='he_normal'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(0.035, 0.9, True), loss=huber_loss, metrics=[HuberMetric()])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=256, validation_data=(X_val, y_val), callbacks=[\n",
    "  # keras.callbacks.TensorBoard(get_run_log_dir()),\n",
    "  # keras.callbacks.ModelCheckpoint(save_dir, save_best_only=True),\n",
    "  keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
    "  keras.callbacks.ReduceLROnPlateau(factor=0.8,patience=5),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 419us/step\n",
      "0.4242804723101253\n",
      "0.788 [0.783056]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print(mean_absolute_error(y_test, pred))\n",
    "print(y_test[12], pred[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer\n",
    "class CustomDense(keras.layers.Layer):\n",
    "  def __init__(self, units, activation=None, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.units = units\n",
    "    self.activation = keras.activations.get(activation)\n",
    "  \n",
    "  def build(self, batch_input_shape):\n",
    "    self.kernel = self.add_weight(\n",
    "      name='kernel', shape=(batch_input_shape[-1], self.units),\n",
    "      initializer='he_normal',\n",
    "    )\n",
    "    self.bias = self.add_weight(name='bias', shape=[self.units], initializer='zeros')\n",
    "    super().build(batch_input_shape) # must be at the end\n",
    "  \n",
    "  def call(self, X):\n",
    "    return self.activation((X @ self.kernel) + self.bias)\n",
    "  \n",
    "  def compute_output_shape(self, batch_input_shape):\n",
    "    return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "  \n",
    "  def get_config(self):\n",
    "    base = super().get_config()\n",
    "    return {**base, 'units': self.units, 'activation': keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.2722 - huber_metric_13: 0.2722 - val_loss: 0.8555 - val_huber_metric_13: 0.8555 - lr: 0.0350\n",
      "Epoch 2/256\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.2214 - huber_metric_13: 0.2214 - val_loss: 0.2781 - val_huber_metric_13: 0.2781 - lr: 0.0350\n",
      "Epoch 3/256\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.2062 - huber_metric_13: 0.2062 - val_loss: 0.2577 - val_huber_metric_13: 0.2577 - lr: 0.0350\n",
      "Epoch 4/256\n",
      "363/363 [==============================] - 0s 761us/step - loss: 0.1991 - huber_metric_13: 0.1991 - val_loss: 0.2216 - val_huber_metric_13: 0.2216 - lr: 0.0350\n",
      "Epoch 5/256\n",
      "363/363 [==============================] - 0s 756us/step - loss: 0.1951 - huber_metric_13: 0.1951 - val_loss: 0.2051 - val_huber_metric_13: 0.2051 - lr: 0.0350\n",
      "Epoch 6/256\n",
      "363/363 [==============================] - 0s 756us/step - loss: 0.1873 - huber_metric_13: 0.1873 - val_loss: 0.2102 - val_huber_metric_13: 0.2102 - lr: 0.0350\n",
      "Epoch 7/256\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.1844 - huber_metric_13: 0.1844 - val_loss: 0.2042 - val_huber_metric_13: 0.2042 - lr: 0.0350\n",
      "Epoch 8/256\n",
      "363/363 [==============================] - 0s 856us/step - loss: 0.1824 - huber_metric_13: 0.1824 - val_loss: 0.2254 - val_huber_metric_13: 0.2254 - lr: 0.0350\n",
      "Epoch 9/256\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.1772 - huber_metric_13: 0.1772 - val_loss: 0.1794 - val_huber_metric_13: 0.1794 - lr: 0.0350\n",
      "Epoch 10/256\n",
      "363/363 [==============================] - 0s 870us/step - loss: 0.1791 - huber_metric_13: 0.1791 - val_loss: 0.2173 - val_huber_metric_13: 0.2173 - lr: 0.0350\n",
      "Epoch 11/256\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.1799 - huber_metric_13: 0.1799 - val_loss: 0.2073 - val_huber_metric_13: 0.2073 - lr: 0.0350\n",
      "Epoch 12/256\n",
      "363/363 [==============================] - 0s 756us/step - loss: 0.1742 - huber_metric_13: 0.1742 - val_loss: 0.1899 - val_huber_metric_13: 0.1899 - lr: 0.0350\n",
      "Epoch 13/256\n",
      "363/363 [==============================] - 0s 768us/step - loss: 0.1741 - huber_metric_13: 0.1741 - val_loss: 0.2184 - val_huber_metric_13: 0.2184 - lr: 0.0350\n",
      "Epoch 14/256\n",
      "363/363 [==============================] - 0s 756us/step - loss: 0.1735 - huber_metric_13: 0.1735 - val_loss: 0.1869 - val_huber_metric_13: 0.1869 - lr: 0.0350\n",
      "Epoch 15/256\n",
      "363/363 [==============================] - 0s 754us/step - loss: 0.1728 - huber_metric_13: 0.1728 - val_loss: 0.2520 - val_huber_metric_13: 0.2520 - lr: 0.0280\n",
      "Epoch 16/256\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.1715 - huber_metric_13: 0.1715 - val_loss: 0.1910 - val_huber_metric_13: 0.1910 - lr: 0.0280\n",
      "Epoch 17/256\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.1705 - huber_metric_13: 0.1705 - val_loss: 0.1869 - val_huber_metric_13: 0.1869 - lr: 0.0280\n",
      "Epoch 18/256\n",
      "363/363 [==============================] - 0s 747us/step - loss: 0.1671 - huber_metric_13: 0.1671 - val_loss: 0.1935 - val_huber_metric_13: 0.1935 - lr: 0.0280\n",
      "Epoch 19/256\n",
      "363/363 [==============================] - 0s 755us/step - loss: 0.1682 - huber_metric_13: 0.1682 - val_loss: 0.1875 - val_huber_metric_13: 0.1875 - lr: 0.0280\n",
      "Epoch 20/256\n",
      "363/363 [==============================] - 0s 769us/step - loss: 0.1656 - huber_metric_13: 0.1656 - val_loss: 0.2053 - val_huber_metric_13: 0.2053 - lr: 0.0224\n",
      "Epoch 21/256\n",
      "363/363 [==============================] - 0s 744us/step - loss: 0.1628 - huber_metric_13: 0.1628 - val_loss: 0.1881 - val_huber_metric_13: 0.1881 - lr: 0.0224\n",
      "Epoch 22/256\n",
      "363/363 [==============================] - 0s 754us/step - loss: 0.1657 - huber_metric_13: 0.1657 - val_loss: 0.1799 - val_huber_metric_13: 0.1799 - lr: 0.0224\n",
      "Epoch 23/256\n",
      "363/363 [==============================] - 0s 748us/step - loss: 0.1654 - huber_metric_13: 0.1654 - val_loss: 0.1803 - val_huber_metric_13: 0.1803 - lr: 0.0224\n",
      "Epoch 24/256\n",
      "363/363 [==============================] - 0s 750us/step - loss: 0.1609 - huber_metric_13: 0.1609 - val_loss: 0.1984 - val_huber_metric_13: 0.1984 - lr: 0.0224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cee75bd0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = 2\n",
    "units = 100\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=X_train.shape[1:]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(hidden):\n",
    "  model.add(CustomDense(units, activation='elu'))\n",
    "  model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(0.035, 0.9, True), loss=huber_loss, metrics=[HuberMetric()])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=256, validation_data=(X_val, y_val), callbacks=[\n",
    "  # keras.callbacks.TensorBoard(get_run_log_dir()),\n",
    "  # keras.callbacks.ModelCheckpoint(save_dir, save_best_only=True),\n",
    "  keras.callbacks.EarlyStopping(patience=15, restore_best_weights=True),\n",
    "  keras.callbacks.ReduceLROnPlateau(factor=0.8,patience=5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 429us/step\n",
      "0.40852880441154066\n",
      "2.405 [2.5110328]\n",
      "1.975 [1.6943629]\n",
      "3.151 [2.9167361]\n",
      "1.875 [1.8150289]\n",
      "1.893 [2.1726897]\n",
      "1.265 [1.3826244]\n",
      "1.5 [2.0641282]\n",
      "1.335 [1.2278967]\n",
      "0.875 [0.9539348]\n",
      "0.863 [0.88381755]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(mean_absolute_error(y_test, pred))\n",
    "for i in range(10):\n",
    "  print(y_test[i], pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Epoch 1/256\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.6465 - mean_squared_error: 0.5910 - val_loss: 2.3106 - val_mean_squared_error: 2.0545 - lr: 0.0350\n",
      "Epoch 2/256\n",
      "363/363 [==============================] - 0s 730us/step - loss: 0.4983 - mean_squared_error: 0.4820 - val_loss: 0.5849 - val_mean_squared_error: 0.5480 - lr: 0.0350\n",
      "Epoch 3/256\n",
      "363/363 [==============================] - 0s 786us/step - loss: 0.4806 - mean_squared_error: 0.4664 - val_loss: 0.6033 - val_mean_squared_error: 0.5387 - lr: 0.0350\n",
      "Epoch 4/256\n",
      "363/363 [==============================] - 0s 739us/step - loss: 0.4532 - mean_squared_error: 0.4403 - val_loss: 0.7066 - val_mean_squared_error: 0.6133 - lr: 0.0350\n",
      "Epoch 5/256\n",
      "363/363 [==============================] - 0s 797us/step - loss: 0.4588 - mean_squared_error: 0.4471 - val_loss: 0.7630 - val_mean_squared_error: 0.5687 - lr: 0.0350\n",
      "Epoch 6/256\n",
      "363/363 [==============================] - 0s 885us/step - loss: 0.4506 - mean_squared_error: 0.4389 - val_loss: 0.7686 - val_mean_squared_error: 0.6232 - lr: 0.0350\n",
      "Epoch 7/256\n",
      "363/363 [==============================] - 0s 803us/step - loss: 0.4502 - mean_squared_error: 0.4398 - val_loss: 0.8042 - val_mean_squared_error: 0.6427 - lr: 0.0350\n",
      "Epoch 8/256\n",
      "363/363 [==============================] - 0s 725us/step - loss: 0.4476 - mean_squared_error: 0.4385 - val_loss: 0.6539 - val_mean_squared_error: 0.6212 - lr: 0.0350\n",
      "Epoch 9/256\n",
      "363/363 [==============================] - 0s 920us/step - loss: 0.4451 - mean_squared_error: 0.4365 - val_loss: 0.5755 - val_mean_squared_error: 0.4842 - lr: 0.0350\n",
      "Epoch 10/256\n",
      "363/363 [==============================] - 0s 709us/step - loss: 0.4326 - mean_squared_error: 0.4237 - val_loss: 0.6010 - val_mean_squared_error: 0.5476 - lr: 0.0350\n",
      "Epoch 11/256\n",
      "363/363 [==============================] - 0s 745us/step - loss: 0.4438 - mean_squared_error: 0.4347 - val_loss: 0.6799 - val_mean_squared_error: 0.5041 - lr: 0.0350\n",
      "Epoch 12/256\n",
      "363/363 [==============================] - 0s 868us/step - loss: 0.4455 - mean_squared_error: 0.4363 - val_loss: 0.6363 - val_mean_squared_error: 0.5599 - lr: 0.0350\n",
      "Epoch 13/256\n",
      "363/363 [==============================] - 0s 778us/step - loss: 0.4405 - mean_squared_error: 0.4314 - val_loss: 0.6204 - val_mean_squared_error: 0.5680 - lr: 0.0350\n",
      "Epoch 14/256\n",
      "363/363 [==============================] - 0s 862us/step - loss: 0.4398 - mean_squared_error: 0.4315 - val_loss: 0.7402 - val_mean_squared_error: 0.4778 - lr: 0.0350\n",
      "Epoch 15/256\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.4411 - mean_squared_error: 0.4330 - val_loss: 0.5527 - val_mean_squared_error: 0.5347 - lr: 0.0350\n",
      "Epoch 16/256\n",
      "363/363 [==============================] - 0s 706us/step - loss: 0.4430 - mean_squared_error: 0.4345 - val_loss: 0.6856 - val_mean_squared_error: 0.6649 - lr: 0.0350\n",
      "Epoch 17/256\n",
      "363/363 [==============================] - 0s 716us/step - loss: 0.4396 - mean_squared_error: 0.4306 - val_loss: 1.1768 - val_mean_squared_error: 0.5801 - lr: 0.0350\n",
      "Epoch 18/256\n",
      "363/363 [==============================] - 0s 858us/step - loss: 0.4391 - mean_squared_error: 0.4305 - val_loss: 0.6542 - val_mean_squared_error: 0.6376 - lr: 0.0350\n",
      "Epoch 19/256\n",
      "363/363 [==============================] - 0s 695us/step - loss: 0.4385 - mean_squared_error: 0.4305 - val_loss: 0.7084 - val_mean_squared_error: 0.5121 - lr: 0.0350\n",
      "Epoch 20/256\n",
      "363/363 [==============================] - 0s 722us/step - loss: 0.4319 - mean_squared_error: 0.4232 - val_loss: 0.7534 - val_mean_squared_error: 0.6458 - lr: 0.0350\n",
      "Epoch 21/256\n",
      "363/363 [==============================] - 0s 758us/step - loss: 0.4268 - mean_squared_error: 0.4187 - val_loss: 0.5587 - val_mean_squared_error: 0.5197 - lr: 0.0350\n",
      "Epoch 22/256\n",
      "363/363 [==============================] - 0s 695us/step - loss: 0.4308 - mean_squared_error: 0.4221 - val_loss: 0.7328 - val_mean_squared_error: 0.6982 - lr: 0.0350\n",
      "Epoch 23/256\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.4342 - mean_squared_error: 0.4221 - val_loss: 0.6798 - val_mean_squared_error: 0.5221 - lr: 0.0350\n",
      "Epoch 24/256\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.4378 - mean_squared_error: 0.4297 - val_loss: 0.6977 - val_mean_squared_error: 0.5224 - lr: 0.0350\n",
      "Epoch 25/256\n",
      "363/363 [==============================] - 0s 697us/step - loss: 0.4293 - mean_squared_error: 0.4213 - val_loss: 0.6664 - val_mean_squared_error: 0.5395 - lr: 0.0350\n",
      "Epoch 26/256\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.4310 - mean_squared_error: 0.4231 - val_loss: 0.7585 - val_mean_squared_error: 0.7076 - lr: 0.0280\n",
      "Epoch 27/256\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.4284 - mean_squared_error: 0.4215 - val_loss: 0.5396 - val_mean_squared_error: 0.4743 - lr: 0.0280\n",
      "Epoch 28/256\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.4254 - mean_squared_error: 0.4181 - val_loss: 0.6004 - val_mean_squared_error: 0.5612 - lr: 0.0280\n",
      "Epoch 29/256\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.4212 - mean_squared_error: 0.4137 - val_loss: 0.8469 - val_mean_squared_error: 0.7989 - lr: 0.0280\n",
      "Epoch 30/256\n",
      "363/363 [==============================] - 0s 710us/step - loss: 0.4323 - mean_squared_error: 0.4250 - val_loss: 0.7295 - val_mean_squared_error: 0.4589 - lr: 0.0280\n",
      "Epoch 31/256\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.4309 - mean_squared_error: 0.4232 - val_loss: 0.6325 - val_mean_squared_error: 0.5976 - lr: 0.0280\n",
      "Epoch 32/256\n",
      "363/363 [==============================] - 0s 690us/step - loss: 0.4236 - mean_squared_error: 0.4162 - val_loss: 1.0809 - val_mean_squared_error: 0.7644 - lr: 0.0280\n",
      "Epoch 33/256\n",
      "363/363 [==============================] - 0s 689us/step - loss: 0.4342 - mean_squared_error: 0.4257 - val_loss: 0.6082 - val_mean_squared_error: 0.5830 - lr: 0.0280\n",
      "Epoch 34/256\n",
      "363/363 [==============================] - 0s 698us/step - loss: 0.4249 - mean_squared_error: 0.4176 - val_loss: 0.6734 - val_mean_squared_error: 0.5349 - lr: 0.0280\n",
      "Epoch 35/256\n",
      "363/363 [==============================] - 0s 691us/step - loss: 0.4227 - mean_squared_error: 0.4143 - val_loss: 0.6798 - val_mean_squared_error: 0.6482 - lr: 0.0280\n",
      "Epoch 36/256\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.4181 - mean_squared_error: 0.4099 - val_loss: 0.5629 - val_mean_squared_error: 0.4927 - lr: 0.0280\n",
      "Epoch 37/256\n",
      "363/363 [==============================] - 0s 695us/step - loss: 0.4257 - mean_squared_error: 0.4184 - val_loss: 0.6613 - val_mean_squared_error: 0.5289 - lr: 0.0280\n",
      "Epoch 38/256\n",
      "363/363 [==============================] - 0s 686us/step - loss: 0.4238 - mean_squared_error: 0.4178 - val_loss: 0.5397 - val_mean_squared_error: 0.4932 - lr: 0.0224\n",
      "Epoch 39/256\n",
      "363/363 [==============================] - 0s 715us/step - loss: 0.4132 - mean_squared_error: 0.4070 - val_loss: 0.5171 - val_mean_squared_error: 0.4935 - lr: 0.0224\n",
      "Epoch 40/256\n",
      "363/363 [==============================] - 0s 687us/step - loss: 0.4185 - mean_squared_error: 0.4127 - val_loss: 0.5159 - val_mean_squared_error: 0.5014 - lr: 0.0224\n",
      "Epoch 41/256\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.4152 - mean_squared_error: 0.4081 - val_loss: 0.6820 - val_mean_squared_error: 0.4812 - lr: 0.0224\n",
      "Epoch 42/256\n",
      "363/363 [==============================] - 0s 699us/step - loss: 0.4117 - mean_squared_error: 0.4054 - val_loss: 0.6586 - val_mean_squared_error: 0.5108 - lr: 0.0224\n",
      "Epoch 43/256\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.4165 - mean_squared_error: 0.4093 - val_loss: 0.7523 - val_mean_squared_error: 0.6358 - lr: 0.0224\n",
      "Epoch 44/256\n",
      "363/363 [==============================] - 0s 695us/step - loss: 0.4167 - mean_squared_error: 0.4100 - val_loss: 0.6664 - val_mean_squared_error: 0.6390 - lr: 0.0224\n",
      "Epoch 45/256\n",
      "363/363 [==============================] - 0s 688us/step - loss: 0.4205 - mean_squared_error: 0.4141 - val_loss: 0.6737 - val_mean_squared_error: 0.5400 - lr: 0.0224\n",
      "Epoch 46/256\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.4183 - mean_squared_error: 0.4121 - val_loss: 0.6028 - val_mean_squared_error: 0.5182 - lr: 0.0224\n",
      "Epoch 47/256\n",
      "363/363 [==============================] - 0s 755us/step - loss: 0.4188 - mean_squared_error: 0.4124 - val_loss: 0.6313 - val_mean_squared_error: 0.4320 - lr: 0.0224\n",
      "Epoch 48/256\n",
      "363/363 [==============================] - 0s 714us/step - loss: 0.4188 - mean_squared_error: 0.4127 - val_loss: 0.5887 - val_mean_squared_error: 0.5716 - lr: 0.0224\n",
      "Epoch 49/256\n",
      "363/363 [==============================] - 0s 684us/step - loss: 0.4159 - mean_squared_error: 0.4093 - val_loss: 0.6478 - val_mean_squared_error: 0.4591 - lr: 0.0224\n",
      "Epoch 50/256\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.4227 - mean_squared_error: 0.4159 - val_loss: 0.5988 - val_mean_squared_error: 0.5723 - lr: 0.0224\n",
      "Epoch 51/256\n",
      "363/363 [==============================] - 0s 690us/step - loss: 0.4083 - mean_squared_error: 0.4018 - val_loss: 0.6783 - val_mean_squared_error: 0.6631 - lr: 0.0179\n",
      "Epoch 52/256\n",
      "363/363 [==============================] - 0s 693us/step - loss: 0.3997 - mean_squared_error: 0.3941 - val_loss: 0.5269 - val_mean_squared_error: 0.4487 - lr: 0.0179\n",
      "Epoch 53/256\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.4152 - mean_squared_error: 0.4089 - val_loss: 0.6675 - val_mean_squared_error: 0.5997 - lr: 0.0179\n",
      "Epoch 54/256\n",
      "363/363 [==============================] - 0s 701us/step - loss: 0.4178 - mean_squared_error: 0.4122 - val_loss: 0.6332 - val_mean_squared_error: 0.6195 - lr: 0.0179\n",
      "Epoch 55/256\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.4007 - mean_squared_error: 0.3950 - val_loss: 0.5933 - val_mean_squared_error: 0.5772 - lr: 0.0179\n",
      "Epoch 56/256\n",
      "363/363 [==============================] - 0s 696us/step - loss: 0.4170 - mean_squared_error: 0.4106 - val_loss: 0.5484 - val_mean_squared_error: 0.4966 - lr: 0.0179\n",
      "Epoch 57/256\n",
      "363/363 [==============================] - 0s 712us/step - loss: 0.4076 - mean_squared_error: 0.4021 - val_loss: 0.7362 - val_mean_squared_error: 0.5462 - lr: 0.0179\n",
      "Epoch 58/256\n",
      "363/363 [==============================] - 0s 682us/step - loss: 0.4094 - mean_squared_error: 0.4030 - val_loss: 0.5458 - val_mean_squared_error: 0.4898 - lr: 0.0179\n",
      "Epoch 59/256\n",
      "363/363 [==============================] - 0s 687us/step - loss: 0.4047 - mean_squared_error: 0.3982 - val_loss: 0.5815 - val_mean_squared_error: 0.5480 - lr: 0.0179\n",
      "Epoch 60/256\n",
      "363/363 [==============================] - 0s 694us/step - loss: 0.4083 - mean_squared_error: 0.4024 - val_loss: 0.5834 - val_mean_squared_error: 0.5544 - lr: 0.0179\n",
      "162/162 [==============================] - 0s 403us/step\n",
      "0.49030395756424305\n"
     ]
    }
   ],
   "source": [
    "class CustomModelWithCustomLoss(keras.models.Model):\n",
    "  def __init__(self, input_units, n_hidden=5, units=30, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.hidden = [keras.layers.Dense(units, activation='selu', kernel_initializer='lecun_normal') for _ in range(n_hidden)]\n",
    "    self.input_batch = keras.layers.BatchNormalization()\n",
    "    self.batch = [keras.layers.BatchNormalization() for _ in range(n_hidden)]\n",
    "    self.out = keras.layers.Dense(1)\n",
    "    print(input_units)\n",
    "    self.reconstruction = keras.layers.Dense(input_units)\n",
    "  \n",
    "  def call(self, inputs):\n",
    "    Z = inputs\n",
    "    Z = self.input_batch(Z)\n",
    "    scaled_input = Z\n",
    "    for i in range(len(self.hidden)):\n",
    "      Z = self.hidden[i](Z)\n",
    "      Z = self.batch[i](Z)\n",
    "    recon = self.reconstruction(Z)\n",
    "    self.add_loss(tf.reduce_mean(tf.square(recon - scaled_input)))\n",
    "\n",
    "    return self.out(Z)\n",
    "\n",
    "customModel = CustomModelWithCustomLoss(X_train.shape[-1], n_hidden=3, units=10)\n",
    "\n",
    "customModel.compile(optimizer=keras.optimizers.SGD(0.035, 0.9, True), loss=keras.losses.mean_squared_error, metrics=[keras.metrics.mean_squared_error])\n",
    "\n",
    "customModel.fit(X_train, y_train, epochs=256, validation_data=(X_val, y_val), callbacks=[\n",
    "  keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
    "  keras.callbacks.ReduceLROnPlateau(factor=0.8,patience=10),\n",
    "])\n",
    "\n",
    "pred = customModel.predict(X_test)\n",
    "print(mean_absolute_error(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 389us/step\n",
      "0.44318227836202273\n",
      "2.405 [2.5989137]\n",
      "1.975 [1.5719242]\n",
      "3.151 [3.117066]\n",
      "1.875 [2.016601]\n",
      "1.893 [1.997782]\n",
      "1.265 [1.4025056]\n",
      "1.5 [1.9752915]\n",
      "1.335 [1.2818793]\n",
      "0.875 [0.9925835]\n",
      "0.863 [1.0024114]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(mean_absolute_error(y_test, pred))\n",
    "for i in range(10):\n",
    "  print(y_test[i], pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(748.0, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=20.0>, <tf.Tensor: shape=(), dtype=float32, numpy=324.0>]\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "  return (x**2) + (3*(y**3))\n",
    "\n",
    "w1, w2 = tf.Variable(10.), tf.Variable(6.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = f(w1, w2)\n",
    "  print(z)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(748.0, shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   \u001b[39mprint\u001b[39m(z)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m gradients1 \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39mgradient(z, [w1, w2])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m gradients2 \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(z, [w1, w2])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(gradients)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1055\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39m\"\"\"Computes the gradient using operations recorded in context of this tape.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \n\u001b[1;32m   1017\u001b[0m \u001b[39mNote: Unless you set `persistent=True` a GradientTape can only be used to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39m   called with an unknown value.\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mA non-persistent GradientTape can only be used to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mcompute one set of gradients (or jacobians)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1057\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recording:\n\u001b[1;32m   1058\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)"
     ]
    }
   ],
   "source": [
    "w1, w2 = tf.Variable(10.), tf.Variable(6.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = f(w1, w2)\n",
    "  print(z)\n",
    "\n",
    "gradients1 = tape.gradient(z, [w1, w2])\n",
    "# should throw runtime \n",
    "gradients2 = tape.gradient(z, [w1, w2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(748.0, shape=(), dtype=float32)\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=20.0>, <tf.Tensor: shape=(), dtype=float32, numpy=324.0>]\n",
      "[<tf.Tensor: shape=(), dtype=float32, numpy=20.0>, <tf.Tensor: shape=(), dtype=float32, numpy=324.0>]\n"
     ]
    }
   ],
   "source": [
    "w1, w2 = tf.Variable(10.), tf.Variable(6.)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  z = f(w1, w2)\n",
    "  print(z)\n",
    "\n",
    "gradients1 = tape.gradient(z, [w1, w2])\n",
    "# should throw runtime \n",
    "gradients2 = tape.gradient(z, [w1, w2])\n",
    "\n",
    "print(gradients1)\n",
    "print(gradients2)\n",
    "\n",
    "del tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "11610/11610 - mean: 41183.3438 - mean_absolute_error: 19.34435\n",
      "Epoch 2/12\n",
      "11610/11610 - mean: 20595.3906 - mean_absolute_error: 0.9142\n",
      "Epoch 3/12\n",
      "11610/11610 - mean: 13732.5566 - mean_absolute_error: 0.9188\n",
      "Epoch 4/12\n",
      "11610/11610 - mean: 10301.2842 - mean_absolute_error: 0.9411\n",
      "Epoch 5/12\n",
      "11610/11610 - mean: 8242.3613 - mean_absolute_error: 0.9175\n",
      "Epoch 6/12\n",
      "11610/11610 - mean: 6869.6895 - mean_absolute_error: 0.9223\n",
      "Epoch 7/12\n",
      "11610/11610 - mean: 5889.1738 - mean_absolute_error: 0.9165\n",
      "Epoch 8/12\n",
      "11610/11610 - mean: 5153.7622 - mean_absolute_error: 0.9142\n",
      "Epoch 9/12\n",
      "11610/11610 - mean: 4582.1865 - mean_absolute_error: 0.9263\n",
      "Epoch 10/12\n",
      "11610/11610 - mean: 4125.2012 - mean_absolute_error: 0.9168\n",
      "Epoch 11/12\n",
      "11610/11610 - mean: 3751.1396 - mean_absolute_error: 0.9040\n",
      "Epoch 12/12\n",
      "11610/11610 - mean: 3439.3057 - mean_absolute_error: 0.9155\n"
     ]
    }
   ],
   "source": [
    "# Custom training loop time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# start with a model\n",
    "# (doesn't need to be compiled since we are defining our own training loop)\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "  keras.layers.BatchNormalization(),\n",
    "  keras.layers.Dense(\n",
    "    30,\n",
    "    activation='elu',\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_regularizer=l2_reg,\n",
    "  ),\n",
    "  keras.layers.BatchNormalization(),\n",
    "  keras.layers.Dense(\n",
    "    30,\n",
    "    activation='elu',\n",
    "    kernel_initializer='he_normal',\n",
    "    kernel_regularizer=l2_reg,\n",
    "  ),\n",
    "  keras.layers.BatchNormalization(),\n",
    "  keras.layers.Dense(1, kernel_regularizer=l2_reg),\n",
    "])\n",
    "\n",
    "# function to randomly sample a batch for training\n",
    "def random_batch(X, y, batch_size=32):\n",
    "  idx = np.random.randint(len(X), size=batch_size)\n",
    "  return X[idx], y[idx]\n",
    "\n",
    "# what to print during training epoch\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "  metrics = ' - '.join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss] + (metrics or [])])\n",
    "  end = '' if iteration < total else '\\n'\n",
    "  print('\\r{}/{} - '.format(iteration, total) + metrics, end=end)\n",
    "\n",
    "# Actual loop\n",
    "n_epochs = 12\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "# optimizer = keras.optimizers.SGD(learning_rate=0.035, momentum=0.9, nesterov=True)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  print('Epoch {}/{}'.format(epoch, n_epochs))\n",
    "  for step in range(1, n_steps + 1):\n",
    "    X_batch, y_batch = random_batch(X_train, y_train, batch_size=batch_size)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "      y_pred = model(X_batch)\n",
    "      main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "      loss = tf.add_n([main_loss] + model.losses)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    mean_loss(loss)\n",
    "\n",
    "    for metric in metrics:\n",
    "      metric(y_batch, y_pred)\n",
    "    print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "  print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "  for metric in metrics:\n",
    "    metric.reset_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 384us/step\n",
      "0.9132545991188019\n",
      "2.405 [2.0760326]\n",
      "1.975 [2.0759225]\n",
      "3.151 [2.0759225]\n",
      "1.875 [2.0759225]\n",
      "1.893 [2.0759225]\n",
      "1.265 [2.0759225]\n",
      "1.5 [2.0759225]\n",
      "1.335 [2.0759225]\n",
      "0.875 [2.0849552]\n",
      "0.863 [2.0759225]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(mean_absolute_error(y_test, pred))\n",
    "for i in range(10):\n",
    "  print(y_test[i], pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1000, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def cube(x):\n",
    "  return x**3\n",
    "\n",
    "tf_cube = tf.function(cube)\n",
    "\n",
    "print(tf_cube(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "  \n",
    "  def build(self, batch_input_shape):\n",
    "    shape = batch_input_shape[-1:]\n",
    "    self.alpha = self.add_weight(name='alpha', shape=shape, initializer='ones')\n",
    "    self.beta = self.add_weight(name='beta', shape=shape, initializer='zeros')\n",
    "    super().build(batch_input_shape)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    mean, var = tf.nn.moments(inputs, axes=0, keepdims=True)\n",
    "    std = tf.sqrt(var)\n",
    "    return tf.multiply(self.alpha, ((inputs - mean) / (std + 1e-6))) + self.beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.5629 - sparse_categorical_crossentropy: 0.5629 - val_loss: 0.4466 - val_sparse_categorical_crossentropy: 0.4466 - lr: 0.0350\n",
      "Epoch 2/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.3927 - sparse_categorical_crossentropy: 0.3927 - val_loss: 0.3768 - val_sparse_categorical_crossentropy: 0.3768 - lr: 0.0350\n",
      "Epoch 3/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.3478 - sparse_categorical_crossentropy: 0.3478 - val_loss: 0.3575 - val_sparse_categorical_crossentropy: 0.3575 - lr: 0.0350\n",
      "Epoch 4/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.3185 - sparse_categorical_crossentropy: 0.3185 - val_loss: 0.3816 - val_sparse_categorical_crossentropy: 0.3816 - lr: 0.0350\n",
      "Epoch 5/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.2961 - sparse_categorical_crossentropy: 0.2961 - val_loss: 0.3392 - val_sparse_categorical_crossentropy: 0.3392 - lr: 0.0350\n",
      "Epoch 6/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.2773 - sparse_categorical_crossentropy: 0.2773 - val_loss: 0.3392 - val_sparse_categorical_crossentropy: 0.3392 - lr: 0.0350\n",
      "Epoch 7/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.2609 - sparse_categorical_crossentropy: 0.2609 - val_loss: 0.3426 - val_sparse_categorical_crossentropy: 0.3426 - lr: 0.0350\n",
      "Epoch 8/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.2505 - sparse_categorical_crossentropy: 0.2505 - val_loss: 0.3328 - val_sparse_categorical_crossentropy: 0.3328 - lr: 0.0350\n",
      "Epoch 9/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.2372 - sparse_categorical_crossentropy: 0.2372 - val_loss: 0.3269 - val_sparse_categorical_crossentropy: 0.3269 - lr: 0.0350\n",
      "Epoch 10/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.2234 - sparse_categorical_crossentropy: 0.2234 - val_loss: 0.3486 - val_sparse_categorical_crossentropy: 0.3486 - lr: 0.0350\n",
      "Epoch 11/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.2197 - sparse_categorical_crossentropy: 0.2197 - val_loss: 0.3525 - val_sparse_categorical_crossentropy: 0.3525 - lr: 0.0350\n",
      "Epoch 12/256\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.2065 - sparse_categorical_crossentropy: 0.2065 - val_loss: 0.3490 - val_sparse_categorical_crossentropy: 0.3490 - lr: 0.0350\n",
      "Epoch 13/256\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.1963 - sparse_categorical_crossentropy: 0.1963 - val_loss: 0.3584 - val_sparse_categorical_crossentropy: 0.3584 - lr: 0.0350\n",
      "Epoch 14/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.1922 - sparse_categorical_crossentropy: 0.1922 - val_loss: 0.3613 - val_sparse_categorical_crossentropy: 0.3613 - lr: 0.0350\n",
      "Epoch 15/256\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.1690 - sparse_categorical_crossentropy: 0.1690 - val_loss: 0.3419 - val_sparse_categorical_crossentropy: 0.3419 - lr: 0.0280\n",
      "Epoch 16/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.1563 - sparse_categorical_crossentropy: 0.1563 - val_loss: 0.3680 - val_sparse_categorical_crossentropy: 0.3680 - lr: 0.0280\n",
      "Epoch 17/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.1544 - sparse_categorical_crossentropy: 0.1544 - val_loss: 0.3795 - val_sparse_categorical_crossentropy: 0.3795 - lr: 0.0280\n",
      "Epoch 18/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.1500 - sparse_categorical_crossentropy: 0.1500 - val_loss: 0.3769 - val_sparse_categorical_crossentropy: 0.3769 - lr: 0.0280\n",
      "Epoch 19/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.1438 - sparse_categorical_crossentropy: 0.1438 - val_loss: 0.3720 - val_sparse_categorical_crossentropy: 0.3720 - lr: 0.0280\n",
      "Epoch 20/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.1232 - sparse_categorical_crossentropy: 0.1232 - val_loss: 0.3884 - val_sparse_categorical_crossentropy: 0.3884 - lr: 0.0224\n",
      "Epoch 21/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.1158 - sparse_categorical_crossentropy: 0.1158 - val_loss: 0.3910 - val_sparse_categorical_crossentropy: 0.3910 - lr: 0.0224\n",
      "Epoch 22/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.1123 - sparse_categorical_crossentropy: 0.1123 - val_loss: 0.4010 - val_sparse_categorical_crossentropy: 0.4010 - lr: 0.0224\n",
      "Epoch 23/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.1116 - sparse_categorical_crossentropy: 0.1116 - val_loss: 0.4105 - val_sparse_categorical_crossentropy: 0.4105 - lr: 0.0224\n",
      "Epoch 24/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.1053 - sparse_categorical_crossentropy: 0.1053 - val_loss: 0.4167 - val_sparse_categorical_crossentropy: 0.4167 - lr: 0.0224\n",
      "Epoch 25/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.0910 - sparse_categorical_crossentropy: 0.0910 - val_loss: 0.4237 - val_sparse_categorical_crossentropy: 0.4237 - lr: 0.0179\n",
      "Epoch 26/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.0894 - sparse_categorical_crossentropy: 0.0894 - val_loss: 0.4263 - val_sparse_categorical_crossentropy: 0.4263 - lr: 0.0179\n",
      "Epoch 27/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.0841 - sparse_categorical_crossentropy: 0.0841 - val_loss: 0.4356 - val_sparse_categorical_crossentropy: 0.4356 - lr: 0.0179\n",
      "Epoch 28/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.0836 - sparse_categorical_crossentropy: 0.0836 - val_loss: 0.4370 - val_sparse_categorical_crossentropy: 0.4370 - lr: 0.0179\n",
      "Epoch 29/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.0816 - sparse_categorical_crossentropy: 0.0816 - val_loss: 0.4443 - val_sparse_categorical_crossentropy: 0.4443 - lr: 0.0179\n",
      "Epoch 30/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.0683 - sparse_categorical_crossentropy: 0.0683 - val_loss: 0.4683 - val_sparse_categorical_crossentropy: 0.4683 - lr: 0.0143\n",
      "Epoch 31/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.0677 - sparse_categorical_crossentropy: 0.0677 - val_loss: 0.4647 - val_sparse_categorical_crossentropy: 0.4647 - lr: 0.0143\n",
      "Epoch 32/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.0604 - sparse_categorical_crossentropy: 0.0604 - val_loss: 0.4911 - val_sparse_categorical_crossentropy: 0.4911 - lr: 0.0143\n",
      "Epoch 33/256\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.0655 - sparse_categorical_crossentropy: 0.0655 - val_loss: 0.4722 - val_sparse_categorical_crossentropy: 0.4722 - lr: 0.0143\n",
      "Epoch 34/256\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.0607 - sparse_categorical_crossentropy: 0.0607 - val_loss: 0.4982 - val_sparse_categorical_crossentropy: 0.4982 - lr: 0.0143\n",
      "Epoch 35/256\n",
      "1383/1407 [============================>.] - ETA: 0s - loss: 0.0532 - sparse_categorical_crossentropy: 0.0532"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mSequential([\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m   keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mFlatten(input_shape\u001b[39m=\u001b[39mX_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:]),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m   LayerNorm(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   keras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m10\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD(\u001b[39m0.035\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, nesterov\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), loss\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39msparse_categorical_crossentropy, metrics\u001b[39m=\u001b[39m[keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39msparse_categorical_crossentropy])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_val, y_val), callbacks\u001b[39m=\u001b[39;49m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m   keras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mEarlyStopping(patience\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, restore_best_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m   keras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mReduceLROnPlateau(factor\u001b[39m=\u001b[39;49m\u001b[39m0.8\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/carlschader/programming/ml-tensorflow/ch12/ch12.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m ))\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/programming/ml-tensorflow/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "  keras.layers.Flatten(input_shape=X_train.shape[1:]),\n",
    "  LayerNorm(),\n",
    "  keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "  LayerNorm(),\n",
    "  keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "  LayerNorm(),\n",
    "  keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "  LayerNorm(),\n",
    "  keras.layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(0.035, momentum=0.9, nesterov=True), loss=keras.losses.sparse_categorical_crossentropy, metrics=[keras.metrics.sparse_categorical_crossentropy])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=256, validation_data=(X_val, y_val), callbacks=(\n",
    "  keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True),\n",
    "  keras.callbacks.ReduceLROnPlateau(factor=0.8, patience=5),\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step\n",
      "[[830   1  19  22   5   4 110   0   9   0]\n",
      " [  7 976   1  10   2   0   3   0   1   0]\n",
      " [ 19   2 833  15  83   0  47   0   1   0]\n",
      " [ 25   6  14 900  41   1   9   0   4   0]\n",
      " [  4   1  85  27 845   0  34   0   4   0]\n",
      " [  0   0   0   1   0 958   1  27   0  13]\n",
      " [125   1  90  31  92   0 652   0   9   0]\n",
      " [  0   0   0   0   0   9   0 969   1  21]\n",
      " [  5   0   4   4   4   3   5   3 972   0]\n",
      " [  0   0   0   0   0  10   1  46   0 943]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, np.argmax(pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "  keras.layers.Flatten(input_shape=X_train.shape[1:]),\n",
    "  LayerNorm(),\n",
    "  keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "  LayerNorm(),\n",
    "  keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "  LayerNorm(),\n",
    "  keras.layers.Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 1 mean loss: 0.04829869419336319, 100.0%75248933145%%%\n",
      "validation loss: 0.4333, validation accuracy: 0.8931\n",
      " epoch: 2 mean loss: 0.040302641689777374, 100.0%5248933145%%%%\n",
      "validation loss: 0.4460, validation accuracy: 0.8945\n",
      " epoch: 3 mean loss: 0.0411611869931221, 100.0%775248933145%%%\n",
      "validation loss: 0.4432, validation accuracy: 0.8949\n",
      " epoch: 4 mean loss: 0.03848792612552643, 100.0%75248933145%%%%\n",
      "validation loss: 0.4567, validation accuracy: 0.8937\n",
      " epoch: 5 mean loss: 0.03656065836548805, 100.0%75248933145%%%\n",
      "validation loss: 0.4629, validation accuracy: 0.8960\n",
      " epoch: 6 mean loss: 0.031425293534994125, 100.0%248933145%%%%%\n",
      "validation loss: 0.4843, validation accuracy: 0.8934\n",
      " epoch: 7 mean loss: 0.034133028239011765, 100.0%5248933145%%%\n",
      "validation loss: 0.4930, validation accuracy: 0.8942\n",
      " epoch: 8 mean loss: 0.03184564411640167, 100.0%75248933145%%%%\n",
      "validation loss: 0.4901, validation accuracy: 0.8923\n",
      " epoch: 9 mean loss: 0.029081635177135468, 100.0%248933145%%%%\n",
      "validation loss: 0.4879, validation accuracy: 0.8929\n",
      " epoch: 10 mean loss: 0.028976941481232643, 100.0%75248933145%%\n",
      "validation loss: 0.5014, validation accuracy: 0.8933\n",
      " epoch: 11 mean loss: 0.027812287211418152, 100.0%5248933145%%%\n",
      "validation loss: 0.5071, validation accuracy: 0.8934\n",
      " epoch: 12 mean loss: 0.02695852890610695, 100.0%775248933145%%%\n",
      "validation loss: 0.5109, validation accuracy: 0.8946\n"
     ]
    }
   ],
   "source": [
    "def random_batch(X, y, batch_size):\n",
    "  idx = np.random.randint(len(X), size=batch_size)\n",
    "  return X[idx], y[idx]\n",
    "\n",
    "epochs = 12\n",
    "batch_size = 64\n",
    "n_steps = len(X_train) // batch_size\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "metric = keras.metrics.sparse_categorical_accuracy\n",
    "mean = keras.metrics.Mean()\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.00875, momentum=0.9, nesterov=True)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "  for step in range(1, n_steps + 1):\n",
    "    X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "      pred = model(X_batch)\n",
    "      loss = tf.reduce_mean(loss_fn(y_batch, pred))\n",
    "    mean(loss)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    end = ''\n",
    "    if step == n_steps:\n",
    "      end = '\\n'\n",
    "    print('\\r epoch: {} mean loss: {}, {}%'.format(epoch, mean.result(), step / n_steps * 100), end=end)\n",
    "  print('validation loss: {:.4f}, validation accuracy: {:.4f}'.format(tf.reduce_mean(loss_fn(y_val, model(X_val))), tf.reduce_mean(metric(y_val, model(X_val)))))\n",
    "  mean.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 783us/step\n",
      "[[822   3  14  25   9   1 116   0  10   0]\n",
      " [  6 975   1  12   2   1   2   0   1   0]\n",
      " [ 24   1 787  13  95   1  77   1   1   0]\n",
      " [ 24   8  11 888  32   3  28   0   6   0]\n",
      " [  3   3  78  35 829   1  48   0   3   0]\n",
      " [  1   0   0   1   0 958   2  19   3  16]\n",
      " [119   2  75  30  67   2 693   0  12   0]\n",
      " [  1   0   0   0   0  23   0 933   1  42]\n",
      " [  5   1   2   5   4   3  10   4 966   0]\n",
      " [  1   0   0   0   0  12   1  28   0 958]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, np.argmax(pred, axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Aug 30 2022, 04:58:14) [Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9120ca18d9a6352a9f63d878cd22df8740658f703b5d617e89c0216f7e3927ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
